{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIVr6K8kPrL6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**\n"
      ],
      "metadata": {
        "id": "A5PEJpnkShIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##for .txt only\n",
        "\n",
        "# with open(\"/content/The-Book-of-Five-Rings-by-Musashi-Miyamoto.pdf\",\"r\",encoding=\"utf-8\") as f:\n",
        "#   d = f.read()\n",
        "\n",
        "# print(\"Total number of characters\",len(d))\n",
        "# print(\"Total number of words\",len(d.split()))\n",
        "# print(d[:100])"
      ],
      "metadata": {
        "id": "yI_i7MfnSmB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLPbf5EJTyoQ",
        "outputId": "132b9dae-eb16-4a4a-dcc9-485ab82dc3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Open the PDF file\n",
        "# with open(\"/content/The-Book-of-Five-Rings-by-Musashi-Miyamoto.pdf\", \"rb\") as f:\n",
        "#     reader = PyPDF2.PdfReader(f)\n",
        "#     text = \"\"\n",
        "#     for page in reader.pages:\n",
        "#         text += page.extract_text() or \"\"  # Extract text from each page\n",
        "\n",
        "# print(\"Total number of characters:\", len(text))\n",
        "# print(\"Total number of words:\", len(text.split()))\n",
        "# print(text[:100])  # Print the first 100 characters\n"
      ],
      "metadata": {
        "id": "ie6m_HskTIdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/The-Book-of-Five-Rings-by-Musashi-Miyamoto.pdf\",\"rb\") as f:\n",
        "  reader = PyPDF2.PdfReader(f)\n",
        "  text = \"\"\n",
        "  for page in reader.pages:\n",
        "    # print(page.extract_text())\n",
        "    text += page.extract_text()\n",
        "\n",
        "\n",
        "print(\"Total num of characters:\",len(text))\n",
        "print(\"Total words:\",len(text.split()))\n",
        "print(text[100:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "OWwqf2geT7Hu",
        "outputId": "30acc060-a83c-4b50-b6f3-08d0c850e04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/The-Book-of-Five-Rings-by-Musashi-Miyamoto.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ace0b2454bd6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/The-Book-of-Five-Rings-by-Musashi-Miyamoto.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(page.extract_text())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/The-Book-of-Five-Rings-by-Musashi-Miyamoto.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we have loaded text now we need to split the text and do some preprocessing\n"
      ],
      "metadata": {
        "id": "ezqJKCi9eHjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "my = \"Prabhakar sharma is a good boy. From sharda university\"\n",
        "res = re.split(r'(\\s)',my)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedpxyHnUex0",
        "outputId": "08233531-c0a9-4503-d732-009e43f5c0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Prabhakar', ' ', 'sharma', ' ', 'is', ' ', 'a', ' ', 'good', ' ', 'boy.', ' ', 'From', ' ', 'sharda', ' ', 'university']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we got our individual words ,whitespace, punctuations etc, but there is a problem we can see the text is splitted on basis of whitespace so comma,fullstop is with words"
      ],
      "metadata": {
        "id": "6SpsgxpXey5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modidy reg exp to split on the basis of whitespace,comma,fullstop etc"
      ],
      "metadata": {
        "id": "HmCV2P-BfQbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = re.split(r'([,.]|\\s)',my)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKIxIBdWeaDw",
        "outputId": "207df7f2-3996-46c5-8171-af7907f03387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Prabhakar', ' ', 'sharma', ' ', 'is', ' ', 'a', ' ', 'good', ' ', 'boy', '.', '', ' ', 'From', ' ', 'sharda', ' ', 'university']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are getting everything seperate cool just need to remove whitespace because it's also counted as token"
      ],
      "metadata": {
        "id": "jeazbVNrf6TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = [i for i in res if i.strip()]\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkvyrww7fP3a",
        "outputId": "47a8941e-8aa8-42b5-f765-6c6605f1181c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Prabhakar',\n",
              " 'sharma',\n",
              " 'is',\n",
              " 'a',\n",
              " 'good',\n",
              " 'boy',\n",
              " '.',\n",
              " 'From',\n",
              " 'sharda',\n",
              " 'university']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove or not whitespace** It depends on use case to remove or not whitespace case 1) remove whitespace when not necessary- it'll help in saving computation resource   case 2) Not remove whitespace- some applications require not removal of whitesapce like if we are training model to generate python code to identation and spacing is important so we can't train model without spacing"
      ],
      "metadata": {
        "id": "YbYKZJrpjG51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need more robust regrex function to seprate ?., etc not just fullstop and comma"
      ],
      "metadata": {
        "id": "kuUIiGcGkbnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text  = \"Hello guys. Am i audible? yes! i'm --\"\n",
        "res2 = re.split(r'([,.?;:_!\"()\\'] | --|\\s)',text)\n",
        "res2 = [i.strip() for i in res2 if i.strip()]\n",
        "res2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtU731DojGlO",
        "outputId": "f4b57111-d0a1-4b27-cb50-cf80b32a0285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'guys', '.', 'Am', 'i', 'audible', '?', 'yes', '!', \"i'm\", '--']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use this to apply at our book data"
      ],
      "metadata": {
        "id": "An6dlHCqlr86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = re.split(r'([,.?;:_!\"()\\']|--|\\s)', text)\n",
        "preprocessed_data = [i.strip() for i in preprocessed_data if i.strip()]\n",
        "preprocessed_data[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pWH7UbxhWjQ",
        "outputId": "b42ea241-c17b-45c9-865d-444bf48f2804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Book',\n",
              " 'of',\n",
              " 'Five',\n",
              " 'Rings',\n",
              " 'Miyamoto',\n",
              " ',',\n",
              " 'Musashi',\n",
              " 'Published',\n",
              " ':',\n",
              " '1644',\n",
              " 'Categories',\n",
              " '(',\n",
              " 's',\n",
              " ')',\n",
              " ':',\n",
              " 'Non-Fiction',\n",
              " ',',\n",
              " 'Philosophy',\n",
              " 'Source']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 convert token into Token ids"
      ],
      "metadata": {
        "id": "qN0NOqSImJ8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's create vocabulary of preprocessed data"
      ],
      "metadata": {
        "id": "KNyoxCxLmkpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = sorted(set(preprocessed_data))\n",
        "print((preprocessed_data[:20]))\n",
        "print(\"Vocab Size :\",len(preprocessed_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3l_yiVehnXp",
        "outputId": "4f50f061-9d92-43e4-cc9a-39d61a5d0b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', \"'\", '(', ')', '+artha', ',', '-', '-Nonexisting', '-allthevices', '-allthings', '-allthree', '-this', '.', '//www', '1', '10ofthemerchant', '11Moreover', '12Chapter', '13']\n",
            "Vocab Size : 4629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign token id"
      ],
      "metadata": {
        "id": "tVnWkhQ8qF1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab ={token:id for id,token in enumerate(preprocessed_data)}\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g8FGs21h240",
        "outputId": "69d12f2c-1095-4e0e-a87d-0ba06b604328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, '+artha': 5, ',': 6, '-': 7, '-Nonexisting': 8, '-allthevices': 9, '-allthings': 10, '-allthree': 11, '-this': 12, '.': 13, '//www': 14, '1': 15, '10ofthemerchant': 16, '11Moreover': 17, '12Chapter': 18, '13': 19, '13wisdom': 20, '14Grip': 21, '1532': 22, '1584–June': 23, '15theseat': 24, '1644': 25, '1645': 26, '16hemay': 27, '17spring': 28, '1886': 29, '18Continuous': 30, '19': 31, '1928': 32, '1960s': 33, '19You': 34, '1About': 35, '1THE': 36, '2': 37, '20aside': 38, '20th': 39, '21To': 40, '22Oral': 41, '23Chapter': 42, '24Stand': 43, '25Alternatively': 44, '26check': 45, '27Knowing': 46, '28afortified': 47, '29Inlarge-scale': 48, '2Introduction': 49, '2THE': 50, '3': 51, '3-5': 52, '30things': 53, '31The': 54, '32The': 55, '33': 56, '34Ofcourse': 57, '35Chapter': 58, '36inferior': 59, '37Without': 60, '380BC': 61, '38you': 62, '39condition': 63, '3Chapter': 64, '3THE': 65, '4': 66, '40Speed': 67, '41Inthis': 68, '42Chapter': 69, '43SHINMEN': 70, '44Loved': 71, '45Beyond': 72, '46of': 73, '47Masters': 74, '4849': 75, '4THE': 76, '4practice': 77, '5': 78, '5THE': 79, '5houses': 80, '6': 81, '6The': 82, '7': 83, '7should': 84, '8': 85, '8AsIwill': 86, '9': 87, '9The': 88, ':': 89, ';': 90, '?': 91, 'Abandon': 92, 'Abdomen': 93, 'Abilities': 94, 'About': 95, 'Above': 96, 'Absorb': 97, 'According': 98, 'Accordingly': 99, 'Adopt': 100, 'Advantage': 101, 'After': 102, 'Akiyama': 103, 'Allthefive': 104, 'Allthings': 105, 'Allthis': 106, 'Also': 107, 'Although': 108, 'Always': 109, 'Anelevated': 110, 'Anindividual': 111, 'Another': 112, 'Antoon': 113, 'Any': 114, 'Anyway': 115, 'Apart': 116, 'Aportion': 117, 'Apply': 118, 'Approach': 119, 'Approaches': 120, 'Arima': 121, 'Array': 122, 'Art': 123, 'ArtofWar': 124, 'Arts': 125, 'As': 126, 'AsIsaid': 127, 'Asfortheother': 128, 'Asia': 129, 'Asifitwere': 130, 'Asifwith': 131, 'Astheenemy': 132, 'Asyou': 133, 'Athenians': 134, 'Atnight': 135, 'Attack': 136, 'Attaining': 137, 'Attitude': 138, 'Attitude-No': 139, 'Attitudes': 140, 'BCbySun': 141, 'BOOK': 142, 'Balance': 143, 'Battle': 144, 'Be-': 145, 'Bearing': 146, 'Because': 147, 'Become': 148, 'Beintent': 149, 'Bene-': 150, 'Benefit': 151, 'Beneither': 152, 'Bennosuke': 153, 'Besides': 154, 'Beyond': 155, 'Blows': 156, 'Body': 157, 'Book': 158, 'Books': 159, 'Both': 160, 'Bows': 161, 'Brace': 162, 'Buddha': 163, 'Buddhism': 164, 'Buddhist': 165, 'But': 166, 'Buteven': 167, 'Butwe': 168, 'Butyou': 169, 'By': 170, 'ByWind': 171, 'Byobserving': 172, 'Bythe': 173, 'Bytheir': 174, 'Byusing': 175, 'Byvictory': 176, 'Byvoid': 177, 'Böse': 178, 'Call': 179, 'Cannot': 180, 'Carpenter': 181, 'Carrying': 182, 'Categories': 183, 'Cause': 184, 'Ceremony': 185, 'Change': 186, 'Characters': 187, 'Chase': 188, 'China': 189, 'Chinese': 190, 'Ching': 191, 'Collapse': 192, 'Coming': 193, 'Commander': 194, 'Commanders': 195, 'Communication': 196, 'Comparing': 197, 'Composed': 198, 'Conception': 199, 'Confront': 200, 'Confusion': 201, 'Continuous': 202, 'Corners': 203, 'Crossing': 204, 'Crush': 205, 'Cthulhu': 206, 'Cut': 207, 'Cutting': 208, 'De': 209, 'Deliberately': 210, 'Depending': 211, 'Depths': 212, 'Design': 213, 'Develop': 214, 'Direct': 215, 'Discern': 216, 'Discipline': 217, 'Distinguish': 218, 'Do': 219, 'Dojo': 220, 'Donot': 221, 'Donotlet': 222, 'Donotletyour': 223, 'Donottrytocutstrongly': 224, 'Down': 225, 'Draw': 226, 'During': 227, 'Dōraku': 228, 'Each': 229, 'East': 230, 'Ei': 231, 'Einstein': 232, 'Emulsion': 233, 'Enact': 234, 'Enemies': 235, 'Enemy': 236, 'English': 237, 'Entanglement': 238, 'Even': 239, 'Everything': 240, 'Evil': 241, 'Examine': 242, 'Existing': 243, 'Extra-Long': 244, 'Eyes': 245, 'FIRE': 246, 'Face': 247, 'Fast': 248, 'February': 249, 'Feet': 250, 'Feint': 251, 'Fifthly': 252, 'Fire': 253, 'Fires': 254, 'First': 255, 'Five': 256, 'Fixed': 257, 'Fixed-': 258, 'Fixing': 259, 'Fixyour': 260, 'Floor': 261, 'Flowing': 262, 'Footballers': 263, 'Footwork': 264, 'For': 265, 'Ford': 266, 'Forestall': 267, 'Forexample': 268, 'Forthis': 269, 'Four': 270, 'Fourthly': 271, 'Friedrich': 272, 'Fright': 273, 'Frighten': 274, 'From': 275, 'Fujiwara': 276, 'Furthermore': 277, 'GROUND': 278, 'Gautama': 279, 'Gaze': 280, 'Generally': 281, 'German': 282, 'Get': 283, 'Glue': 284, 'Go': 285, 'Good': 286, 'Gotama': 287, 'Great': 288, 'Ground': 289, 'Gut': 290, 'Hands': 291, 'He': 292, 'Head': 293, 'Heart': 294, 'Hecarries': 295, 'Heends': 296, 'Heidegger': 297, 'Height': 298, 'Hemakes': 299, 'Hendrik': 300, 'Here': 301, 'Hermann': 302, 'Heshould': 303, 'Hesse': 304, 'Hetaught': 305, 'Hewho': 306, 'Hewill': 307, 'Hilt': 308, 'His': 309, 'Hit': 310, 'Hitachi': 311, 'Hold': 312, 'Holding': 313, 'Holybooks': 314, 'Horses': 315, 'Houses': 316, 'Howard': 317, 'However': 318, 'Hyōhō': 319, 'I': 320, 'Ialways': 321, 'Ibelieve': 322, 'Icannot': 323, 'Ichi': 324, 'Ichi-ryū': 325, 'Iclearly': 326, 'Icould': 327, 'Idescribe': 328, 'Ididnotusethelaw': 329, 'Idislike': 330, 'Idonot': 331, 'Idonotdeal': 332, 'Iexpect': 333, 'If': 334, 'Ifagen-': 335, 'Ifasyou': 336, 'Ifheattains': 337, 'Ifhisbody': 338, 'Ifirst': 339, 'Ifthecorners': 340, 'Iftheenemy': 341, 'Iftheforeman': 342, 'Ifthere': 343, 'Ifthesituation': 344, 'Ifthewind': 345, 'Ifthis': 346, 'Ifweconsider': 347, 'Ifwecrush': 348, 'Ifweknow': 349, 'Ifwelook': 350, 'Ifwewatch': 351, 'Ifyou': 352, 'Igradually': 353, 'Ihad': 354, 'Ihave': 355, 'Iintend': 356, 'IlPrincipe': 357, 'Ilooked': 358, 'Imamichi': 359, 'Imean': 360, 'Immature': 361, 'Immediately': 362, 'In': 363, 'InBeyond': 364, 'InChina': 365, 'InOne': 366, 'InPlato': 367, 'Inac-': 368, 'Inallforms': 369, 'Inallskills': 370, 'Inancient': 371, 'Inany': 372, 'Inasmuch': 373, 'Inbattles': 374, 'Inbuildings': 375, 'Incontests': 376, 'Indian': 377, 'Induels': 378, 'Inhisbook': 379, 'Inhouses': 380, 'Injure': 381, 'Inlarge-scale': 382, 'InmyIchi': 383, 'Inmydoctrine': 384, 'Inmystrategy': 385, 'Inolden': 386, 'Inorder': 387, 'Inshort': 388, 'Insingle': 389, 'Instrategy': 390, 'Interior': 391, 'IntheWay': 392, 'Inthebook': 393, 'Intheconstruction': 394, 'Inthefifth': 395, 'Inthefirst': 396, 'Inthehands': 397, 'Inthese': 398, 'Inthesecond': 399, 'Inthethird': 400, 'Inthevoid': 401, 'Intheworld': 402, 'Inthis': 403, 'Ipractice': 404, 'Ireached': 405, 'Ishow': 406, 'Isitflourishing': 407, 'Istruck': 408, 'Istudied': 409, 'It': 410, 'Itake': 411, 'Iteach': 412, 'Ithas': 413, 'Ithink': 414, 'Itis': 415, 'Itisalso': 416, 'Itisbad': 417, 'Itisbetter': 418, 'Itisdifficult': 419, 'Itisencumbering': 420, 'Itisespecially': 421, 'Itisessential': 422, 'Itisimportant': 423, 'Itisimpossible': 424, 'Itisnecessary': 425, 'Itisnot': 426, 'Itisnow': 427, 'Itisoften': 428, 'Itisoneofthemost': 429, 'Itispossible': 430, 'Itisthefirst': 431, 'Itistheonly': 432, 'Itisvery': 433, 'Itiswritten': 434, 'Itmeans': 435, 'Itshows': 436, 'Itsinfluence': 437, 'Ittakes': 438, 'Itwas': 439, 'Itwill': 440, 'Iwas': 441, 'Iwato': 442, 'Iwent': 443, 'Iwill': 444, 'Japan': 445, 'Japanese': 446, 'Jenseits': 447, 'Just': 448, 'Kakuzo': 449, 'Kama': 450, 'Kantori': 451, 'Ken': 452, 'Kihei': 453, 'Know': 454, 'Knowing': 455, 'Knows': 456, 'Kwannon': 457, 'Kāma': 458, 'Lacquer': 459, 'Language': 460, 'Laozi': 461, 'Learn': 462, 'Leaves': 463, 'Left': 464, 'Let': 465, 'Letit': 466, 'Like': 467, 'Long': 468, 'Looking': 469, 'Lorentz': 470, 'Loss': 471, 'Lovecraft': 472, 'Lower': 473, 'MUSASHI': 474, 'Machiavelli': 475, 'Machiavellian': 476, 'Magonojo': 477, 'Maintain': 478, 'Make': 479, 'Many': 480, 'Masters': 481, 'May': 482, 'Meet': 483, 'Men': 484, 'Methods': 485, 'Middle': 486, 'Mind': 487, 'Mingle': 488, 'Miyamoto': 489, 'Monkey': 490, 'More': 491, 'Mountain-Sea': 492, 'Move': 493, 'Musashi': 494, 'My': 495, 'MyWay': 496, 'Myfirst': 497, 'Myheart': 498, 'Name': 499, 'Neck': 500, 'Neither': 501, 'Neo-Confucianism': 502, 'Next': 503, 'Ni': 504, 'NiTen': 505, 'Niccolò': 506, 'Nietzsche': 507, 'Niten': 508, 'Niten-': 509, 'Nito': 510, 'No': 511, 'No-Attitude': 512, 'NoConcep-': 513, 'NoConception': 514, 'NoGenshin': 515, 'NoKami': 516, 'NoSen': 517, 'Non-Fiction': 518, 'Note': 519, 'OF': 520, 'Ob-': 521, 'Observing': 522, 'Obviously': 523, 'Of': 524, 'Ofcourse': 525, 'Okakura': 526, 'On': 527, 'Once': 528, 'One': 529, 'Onthebattle-': 530, 'Or': 531, 'Oral': 532, 'Originally': 533, 'Oryou': 534, 'Other': 535, 'Outline': 536, 'Ox': 537, 'P': 538, 'Parry': 539, 'Parvati': 540, 'Pass': 541, 'Pay': 542, 'Penetrate': 543, 'Penetrating': 544, 'People': 545, 'Perceive': 546, 'Perceiving': 547, 'Perception': 548, 'Perhaps': 549, 'Phillips': 550, 'Philosophie': 551, 'Philosophy': 552, 'Pillow': 553, 'Place': 554, 'Placing': 555, 'Planning': 556, 'Plato': 557, 'Pliability': 558, 'Polish': 559, 'Practice': 560, 'Prelude': 561, 'Prince': 562, 'Principalities': 563, 'Principatibus': 564, 'Published': 565, 'Rat': 566, 'Reading': 567, 'Really': 568, 'Recently': 569, 'Recorded': 570, 'Red': 571, 'Relativity': 572, 'Release': 573, 'Renew': 574, 'Republic': 575, 'Research': 576, 'Return': 577, 'Right': 578, 'Rings': 579, 'Rock': 580, 'Ryu': 581, 'S': 582, 'Sanskrit': 583, 'Schools': 584, 'Scold': 585, 'Second': 586, 'Sein': 587, 'Sen': 588, 'Seven': 589, 'Shade': 590, 'Shadow': 591, 'Shastra': 592, 'Shinmen': 593, 'Shinto': 594, 'Shiva': 595, 'Shoho': 596, 'Shorter': 597, 'Shout': 598, 'Shouts': 599, 'Siddhartha': 600, 'Side': 601, 'Sight': 602, 'Similar': 603, 'Similarly': 604, 'Since': 605, 'Slash': 606, 'Slashing': 607, 'Sleepiness': 608, 'Slow': 609, 'Smacking': 610, 'Small': 611, 'So': 612, 'So-called': 613, 'Soak': 614, 'Some': 615, 'Someone': 616, 'Sometimes': 617, 'Source': 618, 'Spear': 619, 'Spears': 620, 'Speed': 621, 'Spirit': 622, 'Spiritual': 623, 'Spoke': 624, 'Springing': 625, 'Stab': 626, 'Stance': 627, 'Step': 628, 'Stickiness': 629, 'Still': 630, 'Stones': 631, 'Straight': 632, 'Strategy': 633, 'Stretch': 634, 'Strictly': 635, 'Strike': 636, 'Strive': 637, 'Strong': 638, 'Students': 639, 'Study': 640, 'Such': 641, 'Sun': 642, 'Surely': 643, 'Surface': 644, 'Sutra': 645, 'Sweep': 646, 'Sword': 647, 'Swords': 648, 'THE': 649, 'TUT': 650, 'Tadashima': 651, 'Tai': 652, 'TaiNo': 653, 'Take': 654, 'Takezō': 655, 'Tales': 656, 'Tao': 657, 'Taoism': 658, 'Taoist': 659, 'Te': 660, 'TeChing': 661, 'Tea': 662, 'Teach': 663, 'Teaching': 664, 'Teaism': 665, 'Teruro': 666, 'That': 667, 'The': 668, 'Their': 669, 'Then': 670, 'Theory': 671, 'There': 672, 'Therefore': 673, 'These': 674, 'They': 675, 'Things': 676, 'Third': 677, 'Thirdly': 678, 'This': 679, 'Those': 680, 'Three': 681, 'Through': 682, 'Throw': 683, 'Thrust': 684, 'Thus': 685, 'Timber': 686, 'Timbers': 687, 'Time': 688, 'Times': 689, 'Timing': 690, 'To': 691, 'Toaim': 692, 'ToallWays': 693, 'Toattain': 694, 'Tobecome': 695, 'Tocross': 696, 'Tocutand': 697, 'Today': 698, 'Todefeat': 699, 'Tohold': 700, 'Toknow': 701, 'Tolearn': 702, 'Tomaster': 703, 'Torelease': 704, 'Torenew': 705, 'Tostab': 706, 'Totread': 707, 'Tounderstand': 708, 'Towield': 709, 'Tradition': 710, 'Traditionally': 711, 'Train': 712, 'Tread': 713, 'Treading': 714, 'Troops': 715, 'Turn': 716, 'Tut': 717, 'Tut-TUT': 718, 'Twelfth': 719, 'Two': 720, 'Tzu': 721, 'Unpracticed': 722, 'Until': 723, 'Upper': 724, 'Use': 725, 'Using': 726, 'Usually': 727, 'VOID': 728, 'Vatsyayana': 729, 'Very': 730, 'Victory': 731, 'Void': 732, 'Vorspiel': 733, 'WATER': 734, 'WIND': 735, 'Waiting': 736, 'War': 737, 'Warriors': 738, 'Water': 739, 'Way': 740, 'Ways': 741, 'We': 742, 'Weapons': 743, 'Wedge': 744, 'Wedonot': 745, 'Welive': 746, 'Wemust': 747, 'Weshout': 748, 'Western': 749, 'What': 750, 'Whatever': 751, 'When': 752, 'Whenever': 753, 'Whether': 754, 'Wield': 755, 'Wikipedia': 756, 'Wilhelm': 757, 'Wind': 758, 'Wisdom': 759, 'With': 760, 'Without': 761, 'Written': 762, 'Yin-Yang': 763, 'You': 764, 'Your': 765, 'Zarathustra': 766, 'Zeit': 767, 'Zhuangzi': 768, 'Zukunft': 769, '[allusion': 770, 'a': 771, 'aWay': 772, 'aban-': 773, 'abdo-': 774, 'abdomen': 775, 'abilities': 776, 'ability': 777, 'able': 778, 'abored': 779, 'about': 780, 'above': 781, 'abow': 782, 'absorb': 783, 'ac-': 784, 'accelerating': 785, 'acceptance': 786, 'accompany': 787, 'accomplished': 788, 'accomplishments': 789, 'according': 790, 'accordingly': 791, 'account': 792, 'accumulate': 793, 'accurately': 794, 'accustomed': 795, 'achapter': 796, 'achieve': 797, 'acquainted': 798, 'across': 799, 'actaccording': 800, 'actcontrarily': 801, 'actions': 802, 'actors': 803, 'actually': 804, 'acut': 805, 'adead': 806, 'adhere': 807, 'adifferent': 808, 'adistance': 809, 'adistanced': 810, 'adjusting': 811, 'adoctrine': 812, 'adopt': 813, 'adopts': 814, 'advance': 815, 'advantage': 816, 'advantageous': 817, 'advantages': 818, 'advice': 819, 'af-': 820, 'afact': 821, 'afanorshort': 822, 'afast': 823, 'afault': 824, 'afavourable': 825, 'afavourite': 826, 'afeeling': 827, 'afeint': 828, 'afew': 829, 'affected': 830, 'afinish': 831, 'afolding': 832, 'after': 833, 'afterwards': 834, 'again': 835, 'against': 836, 'agap': 837, 'age': 838, 'agesixty': 839, 'agetospeak': 840, 'agood': 841, 'agreat': 842, 'agricultural': 843, 'ahigh': 844, 'ahuge': 845, 'ahundred': 846, 'aided': 847, 'aim': 848, 'ajumpy': 849, 'alarge': 850, 'aliking': 851, 'aline': 852, 'alittle': 853, 'all': 854, 'allat': 855, 'alleged': 856, 'allfour': 857, 'allfrighten': 858, 'allitscontents': 859, 'allmanner': 860, 'allots': 861, 'allow': 862, 'allowing': 863, 'allthe': 864, 'allwarriors': 865, 'ally': 866, 'alone': 867, 'along': 868, 'along-time': 869, 'aloss': 870, 'alow': 871, 'alplaces': 872, 'also': 873, 'altering': 874, 'alternately': 875, 'alternative': 876, 'although': 877, 'always': 878, 'aman': 879, 'amaster': 880, 'amawarrior': 881, 'amistaken': 882, 'among': 883, 'an': 884, 'anartbut': 885, 'anat-': 886, 'anattack': 887, 'anaxe': 888, 'ance': 889, 'ancient': 890, 'and': 891, 'anen-': 892, 'anenemy': 893, 'anentangled': 894, 'anhonest': 895, 'anin-': 896, 'anopponent': 897, 'another': 898, 'anoutline': 899, 'anox': 900, 'anticipating': 901, 'anunderstanding': 902, 'any': 903, 'anything': 904, 'aone': 905, 'ap-': 906, 'apart': 907, 'apatient': 908, 'aperson': 909, 'apillow': 910, 'aplane': 911, 'apopular': 912, 'appear': 913, 'appearance': 914, 'applies': 915, 'apply': 916, 'applying': 917, 'appreciate': 918, 'approach': 919, 'approached': 920, 'approaches': 921, 'appropriate': 922, 'apreced-': 923, 'aprisoner': 924, 'aquick': 925, 'arat': 926, 'arate': 927, 'arather': 928, 'archers': 929, 'archery': 930, 'architecture': 931, 'are': 932, 'area': 933, 'areable': 934, 'areabout': 935, 'arealso': 936, 'arealways': 937, 'areatadisadvantage': 938, 'arebeaten': 939, 'arecalled': 940, 'arecarried': 941, 'arechiefly': 942, 'areconcerned': 943, 'arecontend-': 944, 'arecorrect': 945, 'arecrossed': 946, 'aredecisive': 947, 'aredifficult': 948, 'aredivided': 949, 'areeasy': 950, 'areeffectively': 951, 'areengaged': 952, 'areessen-': 953, 'arefighting': 954, 'arefluid': 955, 'arefol-': 956, 'areforsituations': 957, 'arefour': 958, 'arehard-pressed': 959, 'arein': 960, 'areinahouse': 961, 'areinconfrontation': 962, 'areintent': 963, 'areknow': 964, 'areknown': 965, 'arelooking': 966, 'aremany': 967, 'aremeet': 968, 'arenoattitudes': 969, 'arenomethods': 970, 'arenone': 971, 'arenot': 972, 'arenotlimited': 973, 'arenotpart': 974, 'arenottobemoved': 975, 'arenowadays': 976, 'areobstructions': 977, 'areonhorseback': 978, 'areoverthrown': 979, 'arerelying': 980, 'aresaid': 981, 'areseen': 982, 'areserved': 983, 'areseveral': 984, 'areshown': 985, 'areside-tracks': 986, 'aresolute': 987, 'aresotechnical': 988, 'arestriving': 989, 'arestruggling': 990, 'arethe': 991, 'arethefour': 992, 'arethegive': 993, 'arethehigher': 994, 'aretheidle': 995, 'arethespecialties': 996, 'arethethree': 997, 'arethetrue': 998, 'arethetwo': 999, 'arethis': 1000, 'arethoroughly': 1001, 'aretraining': 1002, 'aretwo': 1003, 'areusually': 1004, 'arevari-': 1005, 'arevarious': 1006, 'areweapons': 1007, 'arewritten': 1008, 'argues': 1009, 'arises': 1010, 'arm': 1011, 'armed': 1012, 'arms': 1013, 'army': 1014, 'arock': 1015, 'around': 1016, 'array': 1017, 'arrows': 1018, 'art': 1019, 'artand': 1020, 'articles': 1021, 'artisan': 1022, 'artisans': 1023, 'artistic': 1024, 'artists': 1025, 'arts': 1026, 'aryarts': 1027, 'as': 1028, 'as-': 1029, 'asCon-': 1030, 'asIusu-': 1031, 'asKama': 1032, 'asLegalism': 1033, 'asZen': 1034, 'asabeginner': 1035, 'asabeneficial': 1036, 'asabove': 1037, 'asadoctor': 1038, 'asagood': 1039, 'asahorse': 1040, 'asahundred': 1041, 'asanaffirmative': 1042, 'asapejorative': 1043, 'asapoet': 1044, 'asasource': 1045, 'asawarrior': 1046, 'asbe-': 1047, 'ascalmly': 1048, 'asearoute': 1049, 'asecond': 1050, 'asexcellent': 1051, 'asfast': 1052, 'asfighting': 1053, 'asfive': 1054, 'asgentlemen': 1055, 'ashadow': 1056, 'ashand': 1057, 'asheattacks': 1058, 'asheattempts': 1059, 'ashedraws': 1060, 'ashefeels': 1061, 'ashere-': 1062, 'asherelaxes': 1063, 'ashisskill': 1064, 'ashort': 1065, 'ashorter': 1066, 'ashow': 1067, 'aside': 1068, 'aside-track': 1069, 'asideways': 1070, 'asifonawinding': 1071, 'asifrideable': 1072, 'asifthey': 1073, 'asifthrusting': 1074, 'asiftocutand': 1075, 'asiftothrust': 1076, 'asiftying': 1077, 'asifusing': 1078, 'asinfights': 1079, 'asinnormal': 1080, 'asitcame': 1081, 'asitismirrored': 1082, 'asitispossible': 1083, 'asking': 1084, 'askodachi': 1085, 'asloudly': 1086, 'asmall': 1087, 'asmany': 1088, 'asmaster': 1089, 'asmen': 1090, 'asnotknowing': 1091, 'asociety': 1092, 'asoneman': 1093, 'aspart': 1094, 'aspecific': 1095, 'aspect': 1096, 'aspects': 1097, 'aspirit': 1098, 'aspos-': 1099, 'aspossible': 1100, 'asquickly': 1101, 'assault': 1102, 'assemble': 1103, 'assoon': 1104, 'asspears': 1105, 'asspeed': 1106, 'asstrategists': 1107, 'asstrong': 1108, 'assume': 1109, 'assure': 1110, 'astance': 1111, 'astenthousand': 1112, 'asthe': 1113, 'astheWay': 1114, 'asthebasis': 1115, 'asthedefin-': 1116, 'astheenemy': 1117, 'asthefive': 1118, 'asthefoundation': 1119, 'asthelong-sword': 1120, 'asthenormal': 1121, 'astheoccasion': 1122, 'asthespear': 1123, 'asthesword': 1124, 'asthey': 1125, 'asticky': 1126, 'astouching': 1127, 'astraight': 1128, 'astrategist': 1129, 'astrong': 1130, 'asuitable': 1131, 'asvarious': 1132, 'aswarriors': 1133, 'asweak': 1134, 'aswell': 1135, 'aswith': 1136, 'asword': 1137, 'asyou': 1138, 'asyour': 1139, 'at': 1140, 'at-': 1141, 'ata': 1142, 'ataford': 1143, 'atatime': 1144, 'atclose': 1145, 'atcommunicating': 1146, 'atechnique': 1147, 'atee-dum': 1148, 'atfirst': 1149, 'athim': 1150, 'athis': 1151, 'athisface': 1152, 'athou-': 1153, 'athousand': 1154, 'athrust': 1155, 'atiming': 1156, 'ato-do': 1157, 'atother': 1158, 'atrooper': 1159, 'atrue': 1160, 'atstrategy': 1161, 'attack': 1162, 'attack-': 1163, 'attacking': 1164, 'attacks': 1165, 'attain': 1166, 'attaining': 1167, 'attainment': 1168, 'attempt': 1169, 'attendants': 1170, 'attention': 1171, 'atthe': 1172, 'atthebest': 1173, 'atthecommencement': 1174, 'atthecorners': 1175, 'attheenemy': 1176, 'attheface': 1177, 'atthegate': 1178, 'attheheart': 1179, 'atthehips': 1180, 'attheir': 1181, 'atthem': 1182, 'atthesame': 1183, 'attheside': 1184, 'atthestart': 1185, 'atthesyllable': 1186, 'attheworld': 1187, 'atthings': 1188, 'atti-': 1189, 'attitude': 1190, 'attitudes': 1191, 'atwill': 1192, 'at…': 1193, 'audiences': 1194, 'author': 1195, 'autumns': 1196, 'available': 1197, 'awaiting': 1198, 'awall': 1199, 'awarrior': 1200, 'away': 1201, 'awide-': 1202, 'awild': 1203, 'awkward': 1204, 'aword': 1205, 'back': 1206, 'bad': 1207, 'bamboo': 1208, 'based': 1209, 'battle': 1210, 'battles': 1211, 'be': 1212, 'be-': 1213, 'beable': 1214, 'beamaster': 1215, 'bear': 1216, 'bearing': 1217, 'beat': 1218, 'beaten': 1219, 'beating': 1220, 'bebeaten': 1221, 'became': 1222, 'becarried': 1223, 'because': 1224, 'beclearly': 1225, 'becoarse': 1226, 'become': 1227, 'becomes': 1228, 'becoming': 1229, 'becompletely': 1230, 'bedeceived': 1231, 'bedecided': 1232, 'bedefeated': 1233, 'bedependent': 1234, 'bedetermined': 1235, 'bediffering': 1236, 'bedifficult': 1237, 'been': 1238, 'beex-': 1239, 'beexplained': 1240, 'befamiliar': 1241, 'befixed': 1242, 'before': 1243, 'begetting': 1244, 'beginners': 1245, 'begood': 1246, 'behardy': 1247, 'behavior': 1248, 'behind': 1249, 'beinfluenced': 1250, 'being': 1251, 'beinsuch': 1252, 'beintent': 1253, 'beknocked': 1254, 'belarge': 1255, 'below': 1256, 'belt': 1257, 'bemastered': 1258, 'beneficial': 1259, 'benefit': 1260, 'benohelp': 1261, 'benosuch': 1262, 'beoverthrown': 1263, 'beparaded': 1264, 'ber': 1265, 'bers': 1266, 'beseen': 1267, 'beso': 1268, 'best': 1269, 'best-known': 1270, 'besturdy': 1271, 'betaken': 1272, 'bethat': 1273, 'bethevoid': 1274, 'bethinking': 1275, 'between': 1276, 'beun-': 1277, 'beused': 1278, 'bewielded': 1279, 'bewildered': 1280, 'bewilderment': 1281, 'bewon': 1282, 'bewrinkled': 1283, 'beyond': 1284, 'bigiseasy': 1285, 'bigorsmall': 1286, 'bigthings': 1287, 'blade': 1288, 'blades': 1289, 'blind': 1290, 'blue': 1291, 'boards': 1292, 'bodies': 1293, 'body': 1294, 'book': 1295, 'books': 1296, 'born': 1297, 'both': 1298, 'bow': 1299, 'bows': 1300, 'box': 1301, 'boy': 1302, 'branches': 1303, 'break': 1304, 'breast': 1305, 'breathing': 1306, 'bring': 1307, 'bringing': 1308, 'broad': 1309, 'broaden': 1310, 'broadly': 1311, 'brought': 1312, 'building': 1313, 'bull': 1314, 'busi-': 1315, 'busy': 1316, 'but': 1317, 'butChinese': 1318, 'butIhave': 1319, 'butallow': 1320, 'butalso': 1321, 'butbygood': 1322, 'butdonottryitathird': 1323, 'butfeign': 1324, 'buthecannot': 1325, 'buthisspirit': 1326, 'buthitting': 1327, 'butifthe': 1328, 'butifwecompare': 1329, 'butincombat': 1330, 'butit': 1331, 'butitcanbegrasped': 1332, 'butitisanobstacle': 1333, 'butitisbad': 1334, 'butneither': 1335, 'butnotpub-': 1336, 'butobjectively': 1337, 'butonce': 1338, 'butpriests': 1339, 'butrather': 1340, 'butsmacking': 1341, 'butsothat': 1342, 'butswords': 1343, 'buttheenemy': 1344, 'buttheir': 1345, 'butthese': 1346, 'butthey': 1347, 'butthis': 1348, 'buttocling': 1349, 'buttodo': 1350, 'butwith': 1351, 'butyou': 1352, 'by': 1353, 'byHermann': 1354, 'byLovecraft': 1355, 'byOkakura': 1356, 'byPlato': 1357, 'byany': 1358, 'byapplying': 1359, 'byastrong': 1360, 'byattacking': 1361, 'bychanging': 1362, 'bycutting': 1363, 'bydashing': 1364, 'byday': 1365, 'byforestalling': 1366, 'byfrightening': 1367, 'bygiving': 1368, 'byhitting': 1369, 'byimagining': 1370, 'byinsignificant': 1371, 'byknowing': 1372, 'bymaking': 1373, 'bymeans': 1374, 'bymerely': 1375, 'bynotaccord-': 1376, 'byob-': 1377, 'byone': 1378, 'byover-': 1379, 'byphilosopher-kings': 1380, 'bypushing': 1381, 'byquickly': 1382, 'byrelaxing': 1383, 'bysetting': 1384, 'byshouting': 1385, 'bysticking': 1386, 'bytaking': 1387, 'bytheFlorentine': 1388, 'bytheGerman': 1389, 'bytheIndian': 1390, 'bythedevice': 1391, 'bytheenemy': 1392, 'bythelaw': 1393, 'bytheopponent': 1394, 'bythereactions': 1395, 'bythespan': 1396, 'bythis': 1397, 'bythrusting': 1398, 'bytraining': 1399, 'byusing': 1400, 'bywhat': 1401, 'byyour': 1402, 'c': 1403, 'calendar': 1404, 'called': 1405, 'calligraphers': 1406, 'calm': 1407, 'calmly': 1408, 'calmness': 1409, 'came': 1410, 'campaign': 1411, 'can': 1412, 'can-': 1413, 'canadvance': 1414, 'canalso': 1415, 'canalways': 1416, 'canbeat': 1417, 'canbecarried': 1418, 'canbecome': 1419, 'canbeeasily': 1420, 'canbepassed': 1421, 'canbeseen': 1422, 'canbeused': 1423, 'cancause': 1424, 'candothis': 1425, 'caneasily': 1426, 'canhepolish': 1427, 'canknow': 1428, 'canmanage': 1429, 'canmove': 1430, 'cannot': 1431, 'canoften': 1432, 'canperform': 1433, 'canseetheenemy': 1434, 'canseewhy': 1435, 'cansing': 1436, 'cansuddenly': 1437, 'cantake': 1438, 'canunder-': 1439, 'canunrestrictedly': 1440, 'canwalk': 1441, 'canwield': 1442, 'canwin': 1443, 'canwork': 1444, 'capability': 1445, 'captain': 1446, 'carefully': 1447, 'careless': 1448, 'carpenter': 1449, 'carpentry': 1450, 'carriage': 1451, 'carries': 1452, 'carry': 1453, 'carrying': 1454, 'carve': 1455, 'carvings': 1456, 'castles': 1457, 'catching': 1458, 'caught': 1459, 'cause': 1460, 'caused': 1461, 'causes': 1462, 'causing': 1463, 'cautious': 1464, 'ceased': 1465, 'ceaselessly': 1466, 'ceilings': 1467, 'cen-': 1468, 'central': 1469, 'centre': 1470, 'cern': 1471, 'certain': 1472, 'certainly': 1473, 'certainty': 1474, 'chan-': 1475, 'chance': 1476, 'change': 1477, 'changes': 1478, 'changing': 1479, 'chant': 1480, 'characters': 1481, 'chase': 1482, 'chasing': 1483, 'check': 1484, 'choice': 1485, 'chopping': 1486, 'chronicles': 1487, 'cifically': 1488, 'cing': 1489, 'ciple': 1490, 'circulat-': 1491, 'circumstance': 1492, 'cision': 1493, 'claim': 1494, 'clarity': 1495, 'clash': 1496, 'clear': 1497, 'clearly': 1498, 'clench': 1499, 'climbed': 1500, 'clination': 1501, 'clining': 1502, 'close': 1503, 'closed': 1504, 'closely': 1505, 'clouded': 1506, 'clouds': 1507, 'coarsely': 1508, 'collapse': 1509, 'collapses': 1510, 'colour': 1511, 'colouring': 1512, 'columns': 1513, 'com': 1514, 'com/a-book-of-five-rings-by-miyamoto-musashi/': 1515, 'combat': 1516, 'come': 1517, 'comes': 1518, 'command': 1519, 'commander': 1520, 'commanders': 1521, 'commercial': 1522, 'companion': 1523, 'compared': 1524, 'comparison': 1525, 'complete': 1526, 'composed': 1527, 'con-': 1528, 'concealed': 1529, 'concentrate': 1530, 'concentrating': 1531, 'concept': 1532, 'conception': 1533, 'concepts': 1534, 'concerned': 1535, 'concerning': 1536, 'condition': 1537, 'conditions': 1538, 'confines': 1539, 'confused': 1540, 'confuses': 1541, 'confusion': 1542, 'conquer': 1543, 'conscientiously': 1544, 'consider': 1545, 'consideration': 1546, 'considerations': 1547, 'considered': 1548, 'consists': 1549, 'constantly': 1550, 'construction': 1551, 'contests': 1552, 'continue': 1553, 'continuously': 1554, 'control': 1555, 'convenient': 1556, 'conversant': 1557, 'copy': 1558, 'cordance': 1559, 'cording': 1560, 'corners': 1561, 'correct': 1562, 'correctly': 1563, 'counter-cut': 1564, 'country': 1565, 'course': 1566, 'cower': 1567, 'craft': 1568, 'cringing': 1569, 'critical': 1570, 'critics': 1571, 'critique': 1572, 'cross': 1573, 'crosses': 1574, 'crossing': 1575, 'crow': 1576, 'crowd': 1577, 'crush': 1578, 'crushed': 1579, 'crushing': 1580, 'cultivate': 1581, 'culture': 1582, 'cupied': 1583, 'cuss': 1584, 'cusses': 1585, 'cut': 1586, 'cutanenemy': 1587, 'cutasstrongly': 1588, 'cutdown': 1589, 'cutdownwards': 1590, 'cutfirst': 1591, 'cuthim': 1592, 'cuthisarms': 1593, 'cuthishead': 1594, 'cuthisupper': 1595, 'cutitbecomes': 1596, 'cutleftand': 1597, 'cutonyour': 1598, 'cuts': 1599, 'cutsever-': 1600, 'cutsideways': 1601, 'cutstrongly': 1602, 'cuttheen-': 1603, 'cuttheenemies': 1604, 'cuttheenemy': 1605, 'cutthem': 1606, 'cutting': 1607, 'cutunreasonably': 1608, 'cutwith': 1609, 'cu…': 1610, 'daily': 1611, 'dancing': 1612, 'dash': 1613, 'day': 1614, 'de-': 1615, 'deal': 1616, 'deals': 1617, 'death': 1618, 'deavour': 1619, 'decadent': 1620, 'decide': 1621, 'decides': 1622, 'decisions': 1623, 'decisively': 1624, 'decorative': 1625, 'decoratively': 1626, 'ded': 1627, 'deep': 1628, 'deeply': 1629, 'defeat': 1630, 'defeated': 1631, 'defects': 1632, 'defensive': 1633, 'definitely': 1634, 'deflect': 1635, 'deliberate': 1636, 'demand': 1637, 'demands': 1638, 'demoralising': 1639, 'denly': 1640, 'departing': 1641, 'depending': 1642, 'depends': 1643, 'deploys': 1644, 'depths': 1645, 'der': 1646, 'deranged': 1647, 'derangement': 1648, 'derstand': 1649, 'derstanding': 1650, 'derstood': 1651, 'des-': 1652, 'describe': 1653, 'detail': 1654, 'detailed': 1655, 'develop': 1656, 'devoted': 1657, 'dexterity': 1658, 'dextrously': 1659, 'di-': 1660, 'dialogue': 1661, 'dialogues': 1662, 'dicate': 1663, 'die': 1664, 'dies': 1665, 'differ': 1666, 'difference': 1667, 'different': 1668, 'difficult': 1669, 'difficulty': 1670, 'dili-': 1671, 'diligently': 1672, 'direction': 1673, 'directions': 1674, 'directly': 1675, 'dis-': 1676, 'discern-': 1677, 'discharges': 1678, 'discipline': 1679, 'discord': 1680, 'discovering': 1681, 'discreetly': 1682, 'discusses': 1683, 'discussing': 1684, 'dishonestly': 1685, 'dislike': 1686, 'dislikes': 1687, 'disorder': 1688, 'disordered': 1689, 'disposition': 1690, 'distance': 1691, 'distinguish': 1692, 'disturb-': 1693, 'ditional': 1694, 'diverge': 1695, 'diverges': 1696, 'divisions': 1697, 'do': 1698, 'doIlike': 1699, 'doctrines': 1700, 'does': 1701, 'doing': 1702, 'dointhestreet': 1703, 'dom': 1704, 'don': 1705, 'donot': 1706, 'donotallow': 1707, 'donotappreciate': 1708, 'donotbeconscious': 1709, 'donotbemisled': 1710, 'donotbend': 1711, 'donotcallmasters': 1712, 'donotfixtheir': 1713, 'donotgive': 1714, 'donotknow': 1715, 'donotlearn': 1716, 'donotlet': 1717, 'donotlethim': 1718, 'donotletyour': 1719, 'donotlook': 1720, 'donotseparate': 1721, 'donotthink': 1722, 'donotunconditionally': 1723, 'donotunderstand': 1724, 'donotvary': 1725, 'doorkeeper': 1726, 'doors': 1727, 'doso': 1728, 'dosuch': 1729, 'dothis': 1730, 'doubt': 1731, 'dowespeak': 1732, 'down': 1733, 'downfall': 1734, 'downloaded': 1735, 'downwards': 1736, 'dred': 1737, 'drive': 1738, 'drop': 1739, 'drum': 1740, 'drunk': 1741, 'duel': 1742, 'dueling': 1743, 'duels': 1744, 'dum': 1745, 'during': 1746, 'dying': 1747, 'each': 1748, 'earn-': 1749, 'ease': 1750, 'easily': 1751, 'easy': 1752, 'edge': 1753, 'ef-': 1754, 'effectively': 1755, 'einer': 1756, 'either': 1757, 'embarks': 1758, 'emerge': 1759, 'emphasises': 1760, 'employ': 1761, 'emy': 1762, 'en-': 1763, 'enact': 1764, 'encounter': 1765, 'encounters': 1766, 'encourage': 1767, 'endeavour': 1768, 'endurance': 1769, 'enemies': 1770, 'enemy': 1771, 'energy': 1772, 'engaged': 1773, 'enough': 1774, 'entail': 1775, 'entangled': 1776, 'entanglement': 1777, 'enters': 1778, 'entire': 1779, 'entity': 1780, 'entsince': 1781, 'environment': 1782, 'equipment': 1783, 'er': 1784, 'erect': 1785, 'errbyusing': 1786, 'escape': 1787, 'especially': 1788, 'essence': 1789, 'estly': 1790, 'evades': 1791, 'evading': 1792, 'evasive': 1793, 'even': 1794, 'evening': 1795, 'event': 1796, 'eventu-': 1797, 'every': 1798, 'everyday': 1799, 'everything': 1800, 'everywhere': 1801, 'evil': 1802, 'examine': 1803, 'examining': 1804, 'example': 1805, 'exist': 1806, 'existence': 1807, 'expand': 1808, 'expands': 1809, 'expect': 1810, 'experience': 1811, 'explain': 1812, 'explained': 1813, 'explains': 1814, 'explanation': 1815, 'express': 1816, 'expressed': 1817, 'extending': 1818, 'extinguished': 1819, 'extra': 1820, 'extra-long': 1821, 'extraordinary': 1822, 'eye': 1823, 'eyeontheenemy': 1824, 'eyes': 1825, 'face': 1826, 'fact': 1827, 'fail': 1828, 'failed': 1829, 'failing': 1830, 'failtotake': 1831, 'failyetagain': 1832, 'fall': 1833, 'fallen': 1834, 'falling': 1835, 'fallofcapital': 1836, 'false': 1837, 'fame': 1838, 'familiar': 1839, 'family': 1840, 'famous': 1841, 'fan': 1842, 'fanorashort': 1843, 'far': 1844, 'far-reaching': 1845, 'faraway': 1846, 'farmer': 1847, 'farmers': 1848, 'fast': 1849, 'fearlessly': 1850, 'features': 1851, 'fected': 1852, 'fectively': 1853, 'feel': 1854, 'feeling': 1855, 'feelintent': 1856, 'feet': 1857, 'feint': 1858, 'fencing': 1859, 'few': 1860, 'fictional': 1861, 'field': 1862, 'fields': 1863, 'fierce': 1864, 'fifth': 1865, 'fifty': 1866, 'fight': 1867, 'fight-': 1868, 'fighting': 1869, 'fights': 1870, 'file': 1871, 'fin-': 1872, 'find': 1873, 'fine': 1874, 'fined': 1875, 'finger': 1876, 'fingers': 1877, 'fingertips': 1878, 'finished': 1879, 'fire': 1880, 'fires': 1881, 'firewood': 1882, 'firmly': 1883, 'first': 1884, 'fist': 1885, 'fiting': 1886, 'five': 1887, 'fixed-steps': 1888, 'fixedness': 1889, 'fixing': 1890, 'fixtheeyes': 1891, 'fixtheir': 1892, 'fixyour': 1893, 'flaring': 1894, 'floating': 1895, 'floating-foot': 1896, 'floorboards': 1897, 'flourishes': 1898, 'flourishing': 1899, 'flowers': 1900, 'foiling': 1901, 'folk': 1902, 'follow': 1903, 'followed': 1904, 'following': 1905, 'foot': 1906, 'footholds': 1907, 'for': 1908, 'forTaoism': 1909, 'forbattle': 1910, 'forbreath': 1911, 'force': 1912, 'forces': 1913, 'forcombat': 1914, 'forcommanders': 1915, 'ford': 1916, 'fore': 1917, 'forearms': 1918, 'forefinger': 1919, 'forehead': 1920, 'foreigners': 1921, 'foreman': 1922, 'forestall': 1923, 'forestalling': 1924, 'forever': 1925, 'foreveryday': 1926, 'forexample': 1927, 'forextra-long': 1928, 'forgarrisoning': 1929, 'forget': 1930, 'forheight': 1931, 'forhispresence': 1932, 'forhisweak': 1933, 'forifyou': 1934, 'forinformation': 1935, 'forkilling': 1936, 'forknowing': 1937, 'forlarge': 1938, 'form': 1939, 'forman': 1940, 'formation': 1941, 'foroneman': 1942, 'forprofit': 1943, 'forsale': 1944, 'forsingle': 1945, 'forsuperior': 1946, 'forsurvival': 1947, 'forsword-testing': 1948, 'fortaking': 1949, 'fortenmil-': 1950, 'fortenthousand': 1951, 'forthe': 1952, 'fortheenemy': 1953, 'fortheinner': 1954, 'fortheir': 1955, 'forthelong': 1956, 'fortheman': 1957, 'fortheprinciple': 1958, 'forthepupil': 1959, 'fortherevealed': 1960, 'fortheschool': 1961, 'forthese': 1962, 'forthey': 1963, 'forthis': 1964, 'forthrightness': 1965, 'fortifications': 1966, 'forty': 1967, 'forustoattack': 1968, 'forwhat': 1969, 'forwin-': 1970, 'foryou': 1971, 'foster': 1972, 'fought': 1973, 'found': 1974, 'founder': 1975, 'four': 1976, 'fourth': 1977, 'free': 1978, 'freely': 1979, 'friends': 1980, 'frighten': 1981, 'frightened': 1982, 'frisk': 1983, 'from': 1984, 'fronts': 1985, 'fucianism': 1986, 'fucius': 1987, 'fullarmour': 1988, 'fullest': 1989, 'fully': 1990, 'further': 1991, 'gain': 1992, 'gained': 1993, 'gap': 1994, 'gardeners': 1995, 'gate': 1996, 'gates': 1997, 'gaze': 1998, 'gazing': 1999, 'generalisation': 2000, 'generally': 2001, 'generically': 2002, 'gentleman': 2003, 'gently': 2004, 'gers': 2005, 'get': 2006, 'getadecisive': 2007, 'getout': 2008, 'getoutoftime': 2009, 'gets': 2010, 'getting': 2011, 'ging': 2012, 'girders': 2013, 'give': 2014, 'gives': 2015, 'giving': 2016, 'go': 2017, 'goagainst': 2018, 'gods': 2019, 'gointo': 2020, 'goinwith': 2021, 'gonokoe': 2022, 'good': 2023, 'gothehilt': 2024, 'gotogether': 2025, 'gotomeet': 2026, 'gototheextent': 2027, 'gotten': 2028, 'govern': 2029, 'governing': 2030, 'grade': 2031, 'gradually': 2032, 'grasp': 2033, 'grasped': 2034, 'great': 2035, 'gredients': 2036, 'grip': 2037, 'ground': 2038, 'growing': 2039, 'guard': 2040, 'guarded': 2041, 'guide': 2042, 'guidlines': 2043, 'gun': 2044, 'guns': 2045, 'gunshot': 2046, 'habit': 2047, 'had': 2048, 'hairline': 2049, 'halberd': 2050, 'halberdiers': 2051, 'halberds': 2052, 'hand': 2053, 'hand-grip': 2054, 'handed': 2055, 'handle': 2056, 'hands': 2057, 'hanging': 2058, 'hap-': 2059, 'happens': 2060, 'hard': 2061, 'hardship': 2062, 'has': 2063, 'hasaclear': 2064, 'hasastrong': 2065, 'hasbeen': 2066, 'hasexistence': 2067, 'hasfew': 2068, 'hashad': 2069, 'hasnobegin-': 2070, 'hasnoequal': 2071, 'hasnonatural': 2072, 'hassince': 2073, 'hasthese': 2074, 'have': 2075, 'having': 2076, 'he': 2077, 'head': 2078, 'heand': 2079, 'hearing': 2080, 'heart': 2081, 'heattacks': 2082, 'heattempts': 2083, 'heavy': 2084, 'hebreaks': 2085, 'hecanbeawarrior': 2086, 'hedis-': 2087, 'hedoes': 2088, 'heels': 2089, 'hehasmany': 2090, 'height': 2091, 'heisconfused': 2092, 'heisdead': 2093, 'heisstill': 2094, 'heisunsettled': 2095, 'hejumps': 2096, 'held': 2097, 'hemay': 2098, 'henothave': 2099, 'hepasses': 2100, 'hepossesses': 2101, 'here': 2102, 'heregards': 2103, 'hesees': 2104, 'heshould': 2105, 'hewas': 2106, 'hewho': 2107, 'hewill': 2108, 'higher': 2109, 'him': 2110, 'himself': 2111, 'hips': 2112, 'his': 2113, 'hisattack': 2114, 'hisattacking': 2115, 'hisback': 2116, 'hiscon-': 2117, 'hiscut': 2118, 'hisdefence': 2119, 'hiseyes': 2120, 'hisface': 2121, 'hisgoals': 2122, 'hishands': 2123, 'hisin-': 2124, 'hisjump': 2125, 'hislifetime': 2126, 'hisliving': 2127, 'hislong': 2128, 'hismen': 2129, 'hismind': 2130, 'hismovements': 2131, 'hisown': 2132, 'hisplans': 2133, 'hisposition': 2134, 'hisquality': 2135, 'hisrenunciation': 2136, 'hisresources': 2137, 'hisright': 2138, 'hisschool': 2139, 'hisspeedy': 2140, 'hisspir-': 2141, 'hisspirit': 2142, 'hisstrategy': 2143, 'hisstrength': 2144, 'hissword': 2145, 'histechnique': 2146, 'histhoughts': 2147, 'histhrust': 2148, 'history': 2149, 'hisuseless': 2150, 'hiswife': 2151, 'hiswork': 2152, 'hit': 2153, 'hitfirst': 2154, 'hitfrom': 2155, 'hithim': 2156, 'hithishands': 2157, 'hittheenemy': 2158, 'hitting': 2159, 'hittoohard': 2160, 'hitwith': 2161, 'hold': 2162, 'holding': 2163, 'holybooks': 2164, 'homage': 2165, 'hon-': 2166, 'hope': 2167, 'horizontally': 2168, 'horses': 2169, 'hour': 2170, 'house': 2171, 'houses': 2172, 'how': 2173, 'however': 2174, 'http': 2175, 'hundreds': 2176, 'hurriedly': 2177, 'ideas': 2178, 'ies': 2179, 'if': 2180, 'ifalittle': 2181, 'ifaman': 2182, 'ifhe': 2183, 'ifhisrhythm': 2184, 'ifhisspirit': 2185, 'ifitisnotbased': 2186, 'iftheen-': 2187, 'iftheenemy': 2188, 'ifthere': 2189, 'ifthey': 2190, 'iftiming': 2191, 'ifwe': 2192, 'ifwearebusy': 2193, 'ifweareusing': 2194, 'ifwethink': 2195, 'ifwhen': 2196, 'ifyou': 2197, 'ifyour': 2198, 'ignorance': 2199, 'illusionary': 2200, 'illustrating': 2201, 'im-': 2202, 'implies': 2203, 'important': 2204, 'importantly': 2205, 'in': 2206, 'in-': 2207, 'in1513': 2208, 'in1906': 2209, 'in1919': 2210, 'in1922': 2211, 'in1951': 2212, 'inBuddhism': 2213, 'inChinese': 2214, 'inGerman': 2215, 'inIndia': 2216, 'inKyushu': 2217, 'inSanskrit': 2218, 'inThe': 2219, 'inWeird': 2220, 'ina': 2221, 'inaccordance': 2222, 'inacompetitive': 2223, 'inaconfined': 2224, 'inacontrolled': 2225, 'inactual': 2226, 'inaday': 2227, 'inadocumentary': 2228, 'inaduel': 2229, 'inafresh': 2230, 'inahouse': 2231, 'inallthe': 2232, 'inanad-': 2233, 'inand': 2234, 'inanunsuspecting': 2235, 'inapproximately': 2236, 'inareasonable': 2237, 'inaseparate': 2238, 'inasimple': 2239, 'inastrong': 2240, 'inattacking': 2241, 'inawide': 2242, 'inbattle': 2243, 'inboth': 2244, 'inch': 2245, 'inches': 2246, 'inclination': 2247, 'inclined': 2248, 'included': 2249, 'including': 2250, 'incombat': 2251, 'incon-': 2252, 'inconfusing': 2253, 'incrossing': 2254, 'indancing': 2255, 'indecisive': 2256, 'independent': 2257, 'indetail': 2258, 'indicating': 2259, 'indistinguishable': 2260, 'individual': 2261, 'individuals': 2262, 'indoor': 2263, 'induced': 2264, 'indueling': 2265, 'ineither': 2266, 'inev-': 2267, 'inevery': 2268, 'ineveryday': 2269, 'ineverything': 2270, 'inexchange': 2271, 'infa-': 2272, 'infect': 2273, 'inferior': 2274, 'infighting': 2275, 'infights': 2276, 'inflight': 2277, 'influence': 2278, 'influenced': 2279, 'influential': 2280, 'infront': 2281, 'ing': 2282, 'ingand': 2283, 'ingattacked': 2284, 'ingboth': 2285, 'ingsignificance': 2286, 'ingsothat': 2287, 'ingtheeyeballs': 2288, 'ingthesoundness': 2289, 'ingup': 2290, 'ingwith': 2291, 'ingyour': 2292, 'inharbour': 2293, 'inharmony': 2294, 'inhis': 2295, 'inhisharmony': 2296, 'inhisthriving': 2297, 'inhistool': 2298, 'initiative': 2299, 'initsdepths': 2300, 'injure': 2301, 'inlarge': 2302, 'inlarge-scale': 2303, 'inlater': 2304, 'inletting': 2305, 'inman': 2306, 'inmind': 2307, 'inmyWay': 2308, 'inner': 2309, 'inone': 2310, 'inoneaction': 2311, 'inonehand': 2312, 'inonequarter': 2313, 'inorder': 2314, 'inourland': 2315, 'inpenetrating': 2316, 'inplace': 2317, 'inpure': 2318, 'inquestion': 2319, 'inquickly': 2320, 'inrhythm': 2321, 'insections': 2322, 'inseveral': 2323, 'inside': 2324, 'insieges': 2325, 'insingle': 2326, 'inspired': 2327, 'inspirit': 2328, 'instances': 2329, 'instant': 2330, 'instantly': 2331, 'instead': 2332, 'instill': 2333, 'instrategy': 2334, 'instrongly': 2335, 'instructing': 2336, 'instruction': 2337, 'instruments': 2338, 'insufficiently': 2339, 'insweeps': 2340, 'insword': 2341, 'intaking': 2342, 'intechniques': 2343, 'inten-': 2344, 'intend': 2345, 'intent': 2346, 'intention': 2347, 'intentionally': 2348, 'intentions': 2349, 'interior': 2350, 'interms': 2351, 'interpret': 2352, 'interpretations': 2353, 'interpreted': 2354, 'inthat': 2355, 'inthe': 2356, 'inthe1910s': 2357, 'intheGround': 2358, 'intheIchi': 2359, 'intheSanskrit': 2360, 'intheU': 2361, 'intheVoid': 2362, 'intheWay': 2363, 'intheabove': 2364, 'inthecause': 2365, 'inthedepths': 2366, 'inthedirection': 2367, 'intheearly': 2368, 'intheenemy': 2369, 'inthefight': 2370, 'inthefive': 2371, 'intheinterior': 2372, 'intheir': 2373, 'intheleast': 2374, 'inthemidst': 2375, 'inthenew': 2376, 'inthephysical': 2377, 'intheriseand': 2378, 'inthesame': 2379, 'inthesecond': 2380, 'inthesense': 2381, 'inthesmall': 2382, 'inthespirit': 2383, 'inthesummer': 2384, 'inthesun': 2385, 'inthetwentieth': 2386, 'intheuseofhistools': 2387, 'inthewake': 2388, 'inthewhole': 2389, 'intheworld': 2390, 'inthis': 2391, 'intime': 2392, 'intiming': 2393, 'into': 2394, 'introduce': 2395, 'introduced': 2396, 'introduces': 2397, 'intuitive': 2398, 'intuitively': 2399, 'invincible': 2400, 'involved': 2401, 'involving': 2402, 'inwhich': 2403, 'inwith': 2404, 'inwords': 2405, 'inwriting': 2406, 'inyour': 2407, 'ions': 2408, 'irritate': 2409, 'is': 2410, 'isa': 2411, 'isaChinese': 2412, 'isaSocratic': 2413, 'isaWay': 2414, 'isaamaster': 2415, 'isabit': 2416, 'isabook': 2417, 'isabout': 2418, 'isacase': 2419, 'isaccessibile': 2420, 'isaconstant': 2421, 'isadifferent': 2422, 'isaffected': 2423, 'isagainst': 2424, 'isagitated': 2425, 'isahawk': 2426, 'isaliving': 2427, 'isalso': 2428, 'isanallegorical': 2429, 'isanancient': 2430, 'isanencumbrance': 2431, 'isanimportant': 2432, 'isanobstruction': 2433, 'isapheasant': 2434, 'isaplan': 2435, 'isapolitical': 2436, 'isaprinciple': 2437, 'isasaneedle': 2438, 'isasmuch': 2439, 'isaspiritual': 2440, 'isasthread': 2441, 'isathing': 2442, 'isatruth': 2443, 'isattributed': 2444, 'isbad': 2445, 'isbased': 2446, 'isbe-': 2447, 'isbecoming': 2448, 'isbyway': 2449, 'iscalled': 2450, 'iscalm': 2451, 'iscertain': 2452, 'iscommonly': 2453, 'iscompletely': 2454, 'isdanger': 2455, 'isde-': 2456, 'isdecisive': 2457, 'isdefens-': 2458, 'isdefensive': 2459, 'isdevoted': 2460, 'isdexterity': 2461, 'isdifferent': 2462, 'isdifficult': 2463, 'isdisorganised': 2464, 'isdone': 2465, 'iseasy': 2466, 'isencountered': 2467, 'isespecially': 2468, 'isessential': 2469, 'isexplained': 2470, 'isfirm': 2471, 'isfortheextra-long': 2472, 'isfundamental': 2473, 'isgood': 2474, 'ishappier': 2475, 'ishaving': 2476, 'ishigh': 2477, 'ishow': 2478, 'ishurrying': 2479, 'isimmoveable': 2480, 'isimportant': 2481, 'isinaconfined': 2482, 'isinalong': 2483, 'isinclined': 2484, 'isinconsistent': 2485, 'isinferior': 2486, 'isinspirit': 2487, 'isintent': 2488, 'isintheRight': 2489, 'isknow': 2490, 'isknown': 2491, 'island': 2492, 'islarge': 2493, 'islax': 2494, 'isless': 2495, 'islike': 2496, 'islittle': 2497, 'islow-pitched': 2498, 'ismade': 2499, 'ismeant': 2500, 'isneither': 2501, 'isno': 2502, 'isnocase': 2503, 'isnoenemy': 2504, 'isnoescape': 2505, 'isnofast': 2506, 'isnoinner': 2507, 'isnoneed': 2508, 'isnopossible': 2509, 'isnoreal': 2510, 'isnosuch': 2511, 'isnot': 2512, 'isnotexpecting': 2513, 'isnothing': 2514, 'isnothingness': 2515, 'isnothitting': 2516, 'isnotintheleast': 2517, 'isnotmerely': 2518, 'isnotparrying': 2519, 'isnotpart': 2520, 'isnotslack': 2521, 'isnotthe': 2522, 'isnottheessence': 2523, 'isnottheslightest': 2524, 'isnotthetrue': 2525, 'isnottolethim': 2526, 'isnotwarped': 2527, 'isnowarrior': 2528, 'isofnoaccount': 2529, 'isoften': 2530, 'isone': 2531, 'isoneofH': 2532, 'isperhaps': 2533, 'isrecorded': 2534, 'isrelaxed': 2535, 'isresolute': 2536, 'isresolved': 2537, 'isrevealed': 2538, 'isshown': 2539, 'isshut': 2540, 'issimilar': 2541, 'issmall': 2542, 'isstill': 2543, 'isstraight': 2544, 'isstrong': 2545, 'issword-fighting': 2546, 'istactically': 2547, 'isthat': 2548, 'isthe': 2549, 'istheFire': 2550, 'istheMiddle': 2551, 'istheWater': 2552, 'istheWay': 2553, 'isthebasis': 2554, 'isthecase': 2555, 'isthecraft': 2556, 'istheessence': 2557, 'isthefirst': 2558, 'istheheart': 2559, 'isthein-': 2560, 'istheinability': 2561, 'isthemain': 2562, 'isthemeaning': 2563, 'isthemost': 2564, 'isthenight': 2565, 'istheoldest': 2566, 'isthesame': 2567, 'isthespirit': 2568, 'isthesure': 2569, 'isthetwofold': 2570, 'isthevoid': 2571, 'isthis': 2572, 'isthought': 2573, 'isthrough': 2574, 'istiming': 2575, 'istoannounce': 2576, 'istoattack': 2577, 'istobecome': 2578, 'istocarry': 2579, 'istochase': 2580, 'istocuttheenemy': 2581, 'istofallupon': 2582, 'istoforestall': 2583, 'istogetinquickly': 2584, 'istogoinwith': 2585, 'istogovern': 2586, 'istomaster': 2587, 'istoo': 2588, 'istostick': 2589, 'istostrike': 2590, 'istosuppress': 2591, 'istotime': 2592, 'istotrain': 2593, 'istowin': 2594, 'istradition': 2595, 'istranquil': 2596, 'istruly': 2597, 'isundesirable': 2598, 'isunobstructed': 2599, 'isused': 2600, 'isusewhen': 2601, 'isvery': 2602, 'isvictory': 2603, 'isvirtue': 2604, 'isweak': 2605, 'iswhat': 2606, 'iswhen': 2607, 'iswhere': 2608, 'iswith': 2609, 'isworn': 2610, 'iswritten': 2611, 'isyour': 2612, 'it': 2613, 'itably': 2614, 'itallhislife': 2615, 'itapplies': 2616, 'itate': 2617, 'itbecomes': 2618, 'itbegnarled': 2619, 'itcanbeseen': 2620, 'itdown': 2621, 'iteasily': 2622, 'item': 2623, 'itencourages': 2624, 'itfreely': 2625, 'ithasaninterior': 2626, 'ithasbeen': 2627, 'ithaslong': 2628, 'itincludes': 2629, 'itis': 2630, 'itisand': 2631, 'itisbad': 2632, 'itisbeneficial': 2633, 'itisdifficult': 2634, 'itiseasy': 2635, 'itishardly': 2636, 'itisimportant': 2637, 'itisnecessary': 2638, 'itisnevertheless': 2639, 'itisnotthetrue': 2640, 'itisnotunreasonable': 2641, 'itisoften': 2642, 'itisone': 2643, 'itissaid': 2644, 'itisslashing': 2645, 'itissometimes': 2646, 'itisthe': 2647, 'itistrue': 2648, 'itisundecided': 2649, 'itisunsatisfactory': 2650, 'itive': 2651, 'itmay': 2652, 'itmust': 2653, 'itnot': 2654, 'itquickly': 2655, 'itrequires': 2656, 'its': 2657, 'itsbad': 2658, 'itself': 2659, 'itsufficiently': 2660, 'itwas': 2661, 'itwhatever': 2662, 'itwill': 2663, 'ive': 2664, 'jective': 2665, 'join': 2666, 'joints': 2667, 'joists': 2668, 'journey': 2669, 'judge': 2670, 'judgement': 2671, 'judging': 2672, 'jump': 2673, 'jumping': 2674, 'jumping-foot': 2675, 'just': 2676, 'justice': 2677, 'justification': 2678, 'jutting': 2679, 'ju…': 2680, 'katana': 2681, 'keep': 2682, 'killanenemy': 2683, 'killing': 2684, 'kind': 2685, 'kinds': 2686, 'kneel': 2687, 'knocking': 2688, 'knotted': 2689, 'know': 2690, 'know-': 2691, 'knowing': 2692, 'knowledge': 2693, 'known': 2694, 'knows': 2695, 'lack': 2696, 'lags': 2697, 'language': 2698, 'languages': 2699, 'lanterns': 2700, 'large': 2701, 'large-': 2702, 'large-scale': 2703, 'largely': 2704, 'last': 2705, 'late': 2706, 'later': 2707, 'layers': 2708, 'laythefloor': 2709, 'lead': 2710, 'learn': 2711, 'learned': 2712, 'learning': 2713, 'leaves]': 2714, 'left': 2715, 'left-right': 2716, 'leftand': 2717, 'leftbyade-': 2718, 'lefthand': 2719, 'leftshoulder': 2720, 'leftside': 2721, 'legendary': 2722, 'legmovements': 2723, 'legs': 2724, 'length': 2725, 'less': 2726, 'lessens': 2727, 'lesser': 2728, 'lessons': 2729, 'letacontest': 2730, 'lethim': 2731, 'letthis': 2732, 'letting': 2733, 'level': 2734, 'lies': 2735, 'life': 2736, 'lifeand': 2737, 'lifeitisun-': 2738, 'lifeofthewarrior': 2739, 'lifeone': 2740, 'lifetime': 2741, 'lifewith': 2742, 'lifeyou': 2743, 'liftitstraight': 2744, 'lightly': 2745, 'like': 2746, 'likes': 2747, 'liking': 2748, 'limit': 2749, 'limitations': 2750, 'line': 2751, 'linked': 2752, 'lintels': 2753, 'lion': 2754, 'lished': 2755, 'list': 2756, 'listed': 2757, 'literature': 2758, 'little': 2759, 'live': 2760, 'lived': 2761, 'livelihood': 2762, 'loading': 2763, 'long': 2764, 'longsword': 2765, 'longswordsmen': 2766, 'look': 2767, 'loosely': 2768, 'lord': 2769, 'lose': 2770, 'loses': 2771, 'loss': 2772, 'lower': 2773, 'lowing': 2774, 'lowlier': 2775, 'lyrical': 2776, 'm': 2777, 'made': 2778, 'main': 2779, 'mains': 2780, 'maintain': 2781, 'major': 2782, 'make': 2783, 'maker': 2784, 'makers': 2785, 'makes': 2786, 'making': 2787, 'mal': 2788, 'man': 2789, 'man-cutting': 2790, 'manage': 2791, 'mankind': 2792, 'manner': 2793, 'many': 2794, 'mapped': 2795, 'marksmen': 2796, 'marsh': 2797, 'martial': 2798, 'master': 2799, 'mastered': 2800, 'masters': 2801, 'mastery': 2802, 'mathematics': 2803, 'matter': 2804, 'matters': 2805, 'may': 2806, 'mean': 2807, 'meaning': 2808, 'means': 2809, 'meant': 2810, 'measure': 2811, 'measures': 2812, 'meet': 2813, 'meets': 2814, 'megive': 2815, 'melody': 2816, 'memorise': 2817, 'men': 2818, 'ments': 2819, 'merchant': 2820, 'merchants': 2821, 'merciful': 2822, 'merely': 2823, 'met': 2824, 'method': 2825, 'methods': 2826, 'metre': 2827, 'middle': 2828, 'miles': 2829, 'milit-': 2830, 'military': 2831, 'mind': 2832, 'mingling': 2833, 'miraculous': 2834, 'misaligned': 2835, 'miscellaneous': 2836, 'miss': 2837, 'mistake': 2838, 'mistakenly': 2839, 'model': 2840, 'modern': 2841, 'modulation': 2842, 'moment': 2843, 'month': 2844, 'morale': 2845, 'morality': 2846, 'more': 2847, 'morning': 2848, 'most': 2849, 'mountain': 2850, 'mountain-sea': 2851, 'mountains': 2852, 'mov-': 2853, 'move': 2854, 'move-': 2855, 'moved': 2856, 'movement': 2857, 'movements': 2858, 'moves': 2859, 'moving': 2860, 'much': 2861, 'much-debated': 2862, 'muddy': 2863, 'music': 2864, 'musicians': 2865, 'must': 2866, 'mutually': 2867, 'my': 2868, 'myWay': 2869, 'mybody': 2870, 'mybrush': 2871, 'myhand': 2872, 'myheart': 2873, 'myself': 2874, 'mystrategy': 2875, 'mytechnique': 2876, 'myyouth': 2877, 'name': 2878, 'nant': 2879, 'narratives': 2880, 'narrator': 2881, 'narrow': 2882, 'narrowly': 2883, 'natural': 2884, 'naturally': 2885, 'nature': 2886, 'near': 2887, 'nearly': 2888, 'necessarily': 2889, 'necessary': 2890, 'neck': 2891, 'need': 2892, 'neglect': 2893, 'negligent': 2894, 'neither': 2895, 'ness': 2896, 'never': 2897, 'news': 2898, 'night': 2899, 'nimble': 2900, 'nine': 2901, 'ning': 2902, 'ninth': 2903, 'nise': 2904, 'no': 2905, 'noRikyu': 2906, 'noattention': 2907, 'nobility': 2908, 'noconcern': 2909, 'nodefects': 2910, 'noend': 2911, 'noevil': 2912, 'none': 2913, 'noone': 2914, 'nor': 2915, 'nor-': 2916, 'norbooks': 2917, 'norinterior': 2918, 'norlooking': 2919, 'normal': 2920, 'nortwisted': 2921, 'nose': 2922, 'nostrils': 2923, 'not': 2924, 'notable': 2925, 'notallow': 2926, 'notallowing': 2927, 'notapply': 2928, 'notappreciate': 2929, 'notaspire': 2930, 'notattaining': 2931, 'notbe': 2932, 'notbeable': 2933, 'notbeany': 2934, 'notbeeven': 2935, 'notbeinfluenced': 2936, 'notbepossible': 2937, 'notbesonegligent': 2938, 'notchange': 2939, 'notcut': 2940, 'notdepart': 2941, 'notdeviate': 2942, 'notdue': 2943, 'noteacher': 2944, 'notes': 2945, 'notexist': 2946, 'notextend': 2947, 'notfixtheeyes': 2948, 'nothing': 2949, 'nothingness': 2950, 'notinclude': 2951, 'noting': 2952, 'notjustread': 2953, 'notlet-': 2954, 'notlimited': 2955, 'notmean': 2956, 'notmeant': 2957, 'notmore': 2958, 'notmoving': 2959, 'notnamed': 2960, 'notnecessarily': 2961, 'notonce': 2962, 'notonly': 2963, 'notplanning': 2964, 'notreach': 2965, 'notrecover': 2966, 'notseparate': 2967, 'notsimply': 2968, 'notstretching': 2969, 'notthestrength': 2970, 'notthink': 2971, 'nottobedistracted': 2972, 'nottoforget': 2973, 'nottoo': 2974, 'notusethemethod': 2975, 'notwithdrawing': 2976, 'novel': 2977, 'now': 2978, 'nowadays': 2979, 'num-': 2980, 'numbers': 2981, 'numerous': 2982, 'obey': 2983, 'obeying': 2984, 'objectively': 2985, 'observe': 2986, 'observing': 2987, 'obstacles': 2988, 'obtains': 2989, 'occupied': 2990, 'occurs': 2991, 'of': 2992, 'of13chapters': 2993, 'of1926': 2994, 'ofBuddha': 2995, 'ofChinese': 2996, 'ofChristian': 2997, 'ofCon-': 2998, 'ofConfucius': 2999, 'ofCthulhu': 3000, 'ofDasein': 3001, 'ofForms': 3002, 'ofGround': 3003, 'ofHarima': 3004, 'ofHigo': 3005, 'ofIchi': 3006, 'ofJapanese': 3007, 'ofKama': 3008, 'ofKanei': 3009, 'ofSocrates': 3010, 'ofStrategy': 3011, 'ofTea': 3012, 'ofTeawas': 3013, 'ofWaka': 3014, 'ofWar': 3015, 'ofWay': 3016, 'ofadvancing': 3017, 'ofagroup': 3018, 'ofallofthem': 3019, 'ofallthis': 3020, 'ofanIndian': 3021, 'ofanarrator': 3022, 'ofany': 3023, 'ofarchery': 3024, 'ofarobber': 3025, 'ofasahigh': 3026, 'ofasmere': 3027, 'ofattacking': 3028, 'ofattacks': 3029, 'ofattitudes': 3030, 'ofawarrior': 3031, 'ofaweak': 3032, 'ofbalance': 3033, 'ofbattle': 3034, 'ofbattles': 3035, 'ofbe-': 3036, 'ofbeing': 3037, 'ofblack': 3038, 'ofblows': 3039, 'ofbouncing': 3040, 'ofbroad': 3041, 'ofcarry-': 3042, 'ofclose': 3043, 'ofclosing': 3044, 'ofcompetitive': 3045, 'ofcomplete': 3046, 'ofconcentrating': 3047, 'ofconstantly': 3048, 'ofcourse': 3049, 'ofcritical': 3050, 'ofcun-': 3051, 'ofcutting': 3052, 'ofdance': 3053, 'ofdas-in-dem-Welt-sein': 3054, 'ofdeath': 3055, 'ofdefeating': 3056, 'ofdeflecting': 3057, 'ofdexterity': 3058, 'ofdifferent': 3059, 'ofdoing': 3060, 'ofdueling': 3061, 'ofduty': 3062, 'ofequal': 3063, 'ofexistence': 3064, 'off': 3065, 'offacut': 3066, 'offerred': 3067, 'offhishold': 3068, 'offighting': 3069, 'offireis': 3070, 'offishes': 3071, 'offorestalling': 3072, 'offtheir': 3073, 'ofgood': 3074, 'ofgrief': 3075, 'ofhand': 3076, 'ofhealing': 3077, 'ofheaven': 3078, 'ofhis': 3079, 'ofhisattack': 3080, 'ofhisbody': 3081, 'ofhisforce': 3082, 'ofhisforeman': 3083, 'ofhismen': 3084, 'ofhisprevious': 3085, 'ofhisspirit': 3086, 'ofhissword': 3087, 'ofhisweapons': 3088, 'ofhitting': 3089, 'ofholding': 3090, 'ofhouses': 3091, 'ofimportance': 3092, 'ofinfinity': 3093, 'ofinspira-': 3094, 'ofitsdoctrine': 3095, 'ofitsreceptacle': 3096, 'ofjumping': 3097, 'ofjustice': 3098, 'ofknowing': 3099, 'ofknowledge': 3100, 'oflarge': 3101, 'oflaws': 3102, 'oflearning': 3103, 'ofleaving': 3104, 'oflesser': 3105, 'oflife': 3106, 'oflifeand': 3107, 'oflong': 3108, 'ofmaking': 3109, 'ofmany': 3110, 'ofmen': 3111, 'ofmorality': 3112, 'ofmy': 3113, 'ofmyIchi': 3114, 'ofmypupils': 3115, 'ofmystrategy': 3116, 'ofnature': 3117, 'ofnormal': 3118, 'ofnot': 3119, 'ofnotallowing': 3120, 'ofnotbeing': 3121, 'ofone': 3122, 'ofother': 3123, 'ofothers': 3124, 'ofourshout': 3125, 'ofourvictory': 3126, 'ofpalaces': 3127, 'ofparrying': 3128, 'ofpen': 3129, 'ofpeople': 3130, 'ofphilosophy': 3131, 'ofpoetry': 3132, 'ofpoor': 3133, 'ofpositioning': 3134, 'ofprac-': 3135, 'ofpulling': 3136, 'ofrelativity': 3137, 'ofsalvation': 3138, 'ofschools': 3139, 'ofseeing': 3140, 'ofsingle': 3141, 'ofsliding': 3142, 'ofslightly': 3143, 'ofsmacking': 3144, 'ofsmall': 3145, 'ofspears': 3146, 'ofspirit': 3147, 'ofstabbing': 3148, 'ofstickiness': 3149, 'ofstrategists': 3150, 'ofstrategy': 3151, 'ofstrength': 3152, 'ofstrong': 3153, 'ofstrongly': 3154, 'ofsuccess': 3155, 'ofsuperior': 3156, 'ofsword': 3157, 'ofsword-': 3158, 'ofsword-fencing': 3159, 'oftaking': 3160, 'often': 3161, 'oftexts': 3162, 'ofthe': 3163, 'oftheFuture': 3164, 'oftheIchi': 3165, 'oftheKashima': 3166, 'oftheNiTen': 3167, 'oftheNiTo': 3168, 'oftheNiToIchi': 3169, 'oftheVoid': 3170, 'oftheWay': 3171, 'oftheWays': 3172, 'oftheartisan': 3173, 'ofthebattlefield': 3174, 'ofthebody': 3175, 'ofthebow': 3176, 'ofthecar-': 3177, 'ofthecarpenter': 3178, 'ofthecommander': 3179, 'ofthecountry': 3180, 'oftheday': 3181, 'ofthedoctrines': 3182, 'oftheen-': 3183, 'oftheenemy': 3184, 'oftheessences': 3185, 'ofthefarmer': 3186, 'ofthefinest': 3187, 'oftheflower': 3188, 'oftheforeman': 3189, 'ofthegod': 3190, 'ofthehouse': 3191, 'oftheinformation': 3192, 'oftheir': 3193, 'ofthelength': 3194, 'ofthelong': 3195, 'ofthemer-': 3196, 'ofthemerchant': 3197, 'ofthemost': 3198, 'ofthemountains': 3199, 'oftheneck': 3200, 'oftheoldest': 3201, 'ofthephilosoph-': 3202, 'oftheplace': 3203, 'oftheprovince': 3204, 'oftherear': 3205, 'ofthese': 3206, 'oftheshort': 3207, 'ofthesituation': 3208, 'ofthesword': 3209, 'ofthetenth': 3210, 'ofthetiming': 3211, 'ofthetrue': 3212, 'ofthevirtues': 3213, 'ofthevoid': 3214, 'ofthewarrior': 3215, 'ofthewhole': 3216, 'ofthework': 3217, 'oftheworld': 3218, 'ofthewrist': 3219, 'ofthing': 3220, 'ofthings': 3221, 'ofthinking': 3222, 'ofthirteen': 3223, 'ofthis': 3224, 'ofthree': 3225, 'oftime': 3226, 'oftowers': 3227, 'oftread-': 3228, 'oftwo': 3229, 'ofusing': 3230, 'ofvictory': 3231, 'ofview': 3232, 'ofwar': 3233, 'ofwarfare': 3234, 'ofwarriors': 3235, 'ofweaponry': 3236, 'ofwhat': 3237, 'ofwhich': 3238, 'ofwielding': 3239, 'ofwinning': 3240, 'ofworking': 3241, 'ofyesterday': 3242, 'ofyou': 3243, 'ofyour': 3244, 'old': 3245, 'olden': 3246, 'oldtraditions': 3247, 'oldwar': 3248, 'on': 3249, 'on-': 3250, 'onEastern': 3251, 'onTea': 3252, 'onabroad': 3253, 'onalarge': 3254, 'onalarger': 3255, 'onaleather': 3256, 'onamoor': 3257, 'onanattack': 3258, 'onaplacid': 3259, 'once': 3260, 'oncutting': 3261, 'ondetails': 3262, 'one': 3263, 'onehand': 3264, 'oneineach': 3265, 'oneman': 3266, 'oneofthemost': 3267, 'oneself': 3268, 'onewith': 3269, 'onhuman': 3270, 'onintheworld': 3271, 'only': 3272, 'onmarshland': 3273, 'onmartial': 3274, 'onmilitary': 3275, 'onmyWay': 3276, 'onmymind': 3277, 'onmypast': 3278, 'onnine': 3279, 'onquickly': 3280, 'onsex': 3281, 'onslightly': 3282, 'onstrategy': 3283, 'onstrength': 3284, 'onswampy': 3285, 'ontechniques': 3286, 'onthe': 3287, 'ontheattitudes': 3288, 'ontheball': 3289, 'onthebattlefield': 3290, 'onthedetails': 3291, 'ontheenemy': 3292, 'onthefeet': 3293, 'onthefield': 3294, 'onthehands': 3295, 'ontheideas': 3296, 'ontheperiphery': 3297, 'ontheplace': 3298, 'onthesame': 3299, 'onthese': 3300, 'onthesubject': 3301, 'onthesurface': 3302, 'onthis': 3303, 'ontofight': 3304, 'ontothemoment': 3305, 'onun-': 3306, 'onwater': 3307, 'onwhat': 3308, 'onyour': 3309, 'open': 3310, 'openly': 3311, 'openwork': 3312, 'opin-': 3313, 'opinions': 3314, 'opponent': 3315, 'opportunities': 3316, 'opportunity': 3317, 'or': 3318, 'or-': 3319, 'orRight': 3320, 'orahundred': 3321, 'oratail-': 3322, 'orbig': 3323, 'orbythreatening': 3324, 'orcut': 3325, 'orcutting': 3326, 'order': 3327, 'ordrive': 3328, 'orenjoining': 3329, 'orflourish': 3330, 'orforce': 3331, 'orforsome': 3332, 'orguns': 3333, 'orhalberd': 3334, 'orhit': 3335, 'orif': 3336, 'orifhehasfallen': 3337, 'orifyou': 3338, 'orim-': 3339, 'orina': 3340, 'orincommon': 3341, 'orknotted': 3342, 'orlegs': 3343, 'ormake': 3344, 'ornarrow': 3345, 'ornot': 3346, 'ornotching': 3347, 'ornotthat': 3348, 'orother': 3349, 'oroutofshame': 3350, 'orretreating': 3351, 'orsexual': 3352, 'orshort': 3353, 'orside': 3354, 'orslow': 3355, 'orslowly': 3356, 'orsmacking': 3357, 'orsmall': 3358, 'orspear': 3359, 'orstring': 3360, 'orswamp': 3361, 'orten': 3362, 'ortenthousand': 3363, 'orthat': 3364, 'ortheir': 3365, 'ortheorder': 3366, 'ortheteachings': 3367, 'orthirty': 3368, 'ortooneside': 3369, 'ortothesides': 3370, 'ortouch': 3371, 'ortouching': 3372, 'ortwenty-nine': 3373, 'ortwo': 3374, 'orvoice': 3375, 'orwaning': 3376, 'orward': 3377, 'orwarding': 3378, 'orweakly': 3379, 'orwealth': 3380, 'orwhen': 3381, 'oryour': 3382, 'other': 3383, 'others': 3384, 'otherwise': 3385, 'ouaregoing': 3386, 'oughly': 3387, 'ought': 3388, 'our': 3389, 'ourefforts': 3390, 'ourlong': 3391, 'ourself': 3392, 'ourselves': 3393, 'ourtroops': 3394, 'ous': 3395, 'out': 3396, 'outasweat-': 3397, 'outline': 3398, 'outofawall': 3399, 'outofdoors': 3400, 'outside': 3401, 'outstanding': 3402, 'outtheenemy': 3403, 'outthegate': 3404, 'over': 3405, 'over-familiar': 3406, 'overcoming': 3407, 'overhead': 3408, 'overwhelming': 3409, 'own': 3410, 'ox': 3411, 'painters': 3412, 'paper': 3413, 'par-': 3414, 'parry': 3415, 'parrying': 3416, 'part': 3417, 'partic-': 3418, 'particular': 3419, 'particularly': 3420, 'pass': 3421, 'passed': 3422, 'passing': 3423, 'past': 3424, 'path': 3425, 'paying': 3426, 'peasants': 3427, 'penet-': 3428, 'penetrating': 3429, 'pens': 3430, 'penters': 3431, 'people': 3432, 'perceived': 3433, 'perception': 3434, 'perform': 3435, 'performance': 3436, 'performers': 3437, 'perhaps': 3438, 'personal': 3439, 'persons': 3440, 'philosopher': 3441, 'philosophers': 3442, 'philosophy': 3443, 'physics': 3444, 'piece': 3445, 'pieces': 3446, 'pillars': 3447, 'pillow': 3448, 'pine': 3449, 'pinhim': 3450, 'pipe': 3451, 'place': 3452, 'places': 3453, 'plan': 3454, 'planed': 3455, 'play': 3456, 'pleasure': 3457, 'pledges': 3458, 'plies': 3459, 'poets': 3460, 'point': 3461, 'pointless': 3462, 'points': 3463, 'polemical': 3464, 'polish': 3465, 'political': 3466, 'pon-': 3467, 'ponder': 3468, 'poor': 3469, 'portant': 3470, 'posi-': 3471, 'position': 3472, 'positions': 3473, 'possess': 3474, 'possible': 3475, 'pot-lids': 3476, 'powder': 3477, 'power': 3478, 'powerful': 3479, 'practical': 3480, 'practice': 3481, 'practices': 3482, 'practitioners': 3483, 'praised': 3484, 'pray': 3485, 'pre-': 3486, 'preciate': 3487, 'preconceived': 3488, 'predicted': 3489, 'predominant': 3490, 'preference': 3491, 'preferentially': 3492, 'premises': 3493, 'preoc-': 3494, 'present': 3495, 'present-day': 3496, 'preserving': 3497, 'pressed': 3498, 'previously': 3499, 'primarily': 3500, 'primary': 3501, 'principle': 3502, 'principles': 3503, 'prisoner': 3504, 'proach': 3505, 'professions': 3506, 'proficient': 3507, 'profit': 3508, 'progress': 3509, 'propagated': 3510, 'properly': 3511, 'province': 3512, 'public': 3513, 'published': 3514, 'pull': 3515, 'purpose': 3516, 'purposes': 3517, 'pursue': 3518, 'push': 3519, 'puts': 3520, 'putstrength': 3521, 'putyourself': 3522, 'quarters': 3523, 'quick': 3524, 'quicker': 3525, 'quickly': 3526, 'raise': 3527, 'raising': 3528, 'rather': 3529, 'rating': 3530, 're-': 3531, 're-assume': 3532, 'reach': 3533, 'reached': 3534, 'reaches': 3535, 'read': 3536, 'readily': 3537, 'reading': 3538, 'realize': 3539, 'realized': 3540, 'really': 3541, 'realm': 3542, 'rear': 3543, 'reason': 3544, 'reasonable': 3545, 'receive': 3546, 'received': 3547, 'recent': 3548, 'recklessly': 3549, 'recog-': 3550, 'recognised': 3551, 'recor-': 3552, 'recorded': 3553, 'recover': 3554, 'recovery': 3555, 'referred': 3556, 'regarding': 3557, 'regulations': 3558, 'relation': 3559, 'relative': 3560, 'relax': 3561, 'relaxed': 3562, 'relaxes': 3563, 'religion': 3564, 'rely': 3565, 'relying': 3566, 'remain': 3567, 'remaining': 3568, 'remains': 3569, 'remembered': 3570, 'remove': 3571, 'renews': 3572, 'repeat': 3573, 'repeatedly': 3574, 'repetitive': 3575, 'repetitively': 3576, 'representative': 3577, 'republished': 3578, 'research': 3579, 'reserve': 3580, 'resolution': 3581, 'resolve': 3582, 'resource': 3583, 'resources': 3584, 'responses': 3585, 'responsible': 3586, 'result': 3587, 'resulting': 3588, 'results': 3589, 'retreating': 3590, 'retreats': 3591, 'return': 3592, 'revealed': 3593, 'rhythm': 3594, 'rice': 3595, 'ride': 3596, 'rideable': 3597, 'ridge': 3598, 'riding': 3599, 'right': 3600, 'right-left': 3601, 'rights': 3602, 'rise': 3603, 'rising': 3604, 'river': 3605, 'road': 3606, 'roads': 3607, 'rock': 3608, 'roll': 3609, 'room': 3610, 'row': 3611, 'ruin': 3612, 'ruled': 3613, 'rules': 3614, 'run': 3615, 'runners': 3616, 'running': 3617, 'rying': 3618, 'ryū': 3619, 's': 3620, 'sacrifice': 3621, 'said': 3622, 'sail': 3623, 'saileven': 3624, 'same': 3625, 'samurai': 3626, 'sand': 3627, 'satire': 3628, 'say': 3629, 'saying': 3630, 'scaffolding': 3631, 'scale': 3632, 'scholar': 3633, 'school': 3634, 'schools': 3635, 'scolding': 3636, 'scoop-': 3637, 'scooping': 3638, 'score': 3639, 'scribed': 3640, 'sea': 3641, 'seaatacrossing': 3642, 'search': 3643, 'searching': 3644, 'seas': 3645, 'season': 3646, 'second': 3647, 'secret': 3648, 'sections': 3649, 'see': 3650, 'seehim': 3651, 'seehis': 3652, 'seehisintention': 3653, 'seeinadvance': 3654, 'seeing': 3655, 'seeitineverything': 3656, 'seem': 3657, 'seemingly': 3658, 'seen': 3659, 'seethat': 3660, 'seetheen-': 3661, 'seetheenemy': 3662, 'seetheweight': 3663, 'seethrough': 3664, 'seevarious': 3665, 'seizing': 3666, 'self': 3667, 'sell': 3668, 'selves': 3669, 'sen': 3670, 'sense': 3671, 'sensual': 3672, 'sep-': 3673, 'servant': 3674, 'serve': 3675, 'set': 3676, 'setinthis': 3677, 'setsail': 3678, 'setting': 3679, 'settled': 3680, 'settling': 3681, 'setyour': 3682, 'several': 3683, 'sexual': 3684, 'shade': 3685, 'shapes': 3686, 'sharpen': 3687, 'sharpens': 3688, 'sharply': 3689, 'shelves': 3690, 'ship': 3691, 'shooting': 3692, 'short': 3693, 'shorter': 3694, 'should': 3695, 'shoulder': 3696, 'shoulders': 3697, 'shout': 3698, 'shouting': 3699, 'shouts': 3700, 'show': 3701, 'showing': 3702, 'shown': 3703, 'shows': 3704, 'shrines': 3705, 'sible': 3706, 'siddha': 3707, 'side': 3708, 'sides': 3709, 'siduously': 3710, 'sight': 3711, 'silent': 3712, 'similar': 3713, 'simplicity': 3714, 'simply': 3715, 'simultaneously': 3716, 'single': 3717, 'sists': 3718, 'situation': 3719, 'situations': 3720, 'sixteen': 3721, 'sixty': 3722, 'size': 3723, 'skilful': 3724, 'skill': 3725, 'skilled': 3726, 'skillful': 3727, 'skills': 3728, 'slack': 3729, 'slacken': 3730, 'slash': 3731, 'sliding': 3732, 'slightest': 3733, 'slightly': 3734, 'slow': 3735, 'slowly': 3736, 'slowness': 3737, 'smacking': 3738, 'small': 3739, 'small-scale': 3740, 'snipping': 3741, 'so': 3742, 'soIhave': 3743, 'soahun-': 3744, 'soak': 3745, 'soathousand': 3746, 'socan': 3747, 'sohismovements': 3748, 'soitiswith': 3749, 'soldiers': 3750, 'solely': 3751, 'some': 3752, 'someone': 3753, 'something': 3754, 'sometimes': 3755, 'somewhat': 3756, 'somust': 3757, 'somymethod': 3758, 'soon': 3759, 'sotend': 3760, 'sothat': 3761, 'sothe': 3762, 'sotheir': 3763, 'sothere': 3764, 'sothis': 3765, 'soul': 3766, 'soyou': 3767, 'soyour': 3768, 'space': 3769, 'spaces': 3770, 'spe-': 3771, 'speak': 3772, 'speaking': 3773, 'spear': 3774, 'spearmen': 3775, 'spears': 3776, 'specialise': 3777, 'spends': 3778, 'spent': 3779, 'spirit': 3780, 'spirited': 3781, 'spiritual': 3782, 'spoke': 3783, 'spread': 3784, 'spring': 3785, 'springing': 3786, 'springs': 3787, 'square-on': 3788, 'stab': 3789, 'stabbing': 3790, 'stag-': 3791, 'stance': 3792, 'stand': 3793, 'standard': 3794, 'start': 3795, 'starts': 3796, 'stay': 3797, 'steady': 3798, 'step': 3799, 'steps': 3800, 'stick': 3801, 'stickiness': 3802, 'still': 3803, 'stony': 3804, 'stop': 3805, 'stor-': 3806, 'story': 3807, 'straight': 3808, 'straightaway': 3809, 'strategies': 3810, 'strategist': 3811, 'strategists': 3812, 'strategy': 3813, 'strategys': 3814, 'strength': 3815, 'stretch': 3816, 'stretched': 3817, 'stretching': 3818, 'strike': 3819, 'striking': 3820, 'strive': 3821, 'strong': 3822, 'stronger': 3823, 'strongest': 3824, 'strongly': 3825, 'student': 3826, 'studied': 3827, 'study': 3828, 'studying': 3829, 'style': 3830, 'subjects': 3831, 'subordinates': 3832, 'subtitled': 3833, 'succeed': 3834, 'successful': 3835, 'such': 3836, 'sud-': 3837, 'suddenly': 3838, 'suffice': 3839, 'sufficient': 3840, 'sufficiently': 3841, 'suggested': 3842, 'suitable': 3843, 'superficially': 3844, 'suppress': 3845, 'suppressing': 3846, 'sure': 3847, 'surely': 3848, 'surface': 3849, 'surprise': 3850, 'swampy': 3851, 'sweep': 3852, 'sword': 3853, 'sword-fen-': 3854, 'sword-fencers': 3855, 'sword-fencing': 3856, 'sword-fighting': 3857, 'swordpoint': 3858, 'swords': 3859, 'swordsmanship': 3860, 'swordsmen': 3861, 'syllable': 3862, 'sūtra': 3863, 'tables': 3864, 'tachi': 3865, 'tack': 3866, 'tacking': 3867, 'tactics': 3868, 'tain': 3869, 'tained': 3870, 'take': 3871, 'takes': 3872, 'taking': 3873, 'talking': 3874, 'tangled': 3875, 'taste': 3876, 'taught': 3877, 'tea': 3878, 'tea-': 3879, 'teach': 3880, 'teacher': 3881, 'teaching': 3882, 'technique': 3883, 'techniques': 3884, 'tee-': 3885, 'tee-dum': 3886, 'tempering': 3887, 'temples': 3888, 'ten': 3889, 'ten-thousand-a-side': 3890, 'tend': 3891, 'tendays': 3892, 'tendencies': 3893, 'tenmen': 3894, 'tenor': 3895, 'tens': 3896, 'tense': 3897, 'tenseness': 3898, 'tent': 3899, 'tenth': 3900, 'tenthou-': 3901, 'tenthousand': 3902, 'terchanging': 3903, 'term': 3904, 'terrify': 3905, 'testing': 3906, 'tests': 3907, 'text': 3908, 'than': 3909, 'that': 3910, 'the': 3911, 'the6thcentury': 3912, 'theBuddha': 3913, 'theFire': 3914, 'theJapanese': 3915, 'theKamiza': 3916, 'theLeft': 3917, 'theLower': 3918, 'theMiddle': 3919, 'theTao': 3920, 'theTen': 3921, 'theUpper': 3922, 'theVoid': 3923, 'theWater': 3924, 'theWay': 3925, 'theWays': 3926, 'theWind': 3927, 'theabilities': 3928, 'theability': 3929, 'theadvantage': 3930, 'theages': 3931, 'theapplic-': 3932, 'theapplication': 3933, 'thearchitectural': 3934, 'thearmies': 3935, 'theattacking': 3936, 'theatti-': 3937, 'thebackground': 3938, 'thebad': 3939, 'thebattle': 3940, 'thebenefit': 3941, 'thebest': 3942, 'thebiggest': 3943, 'thebloom': 3944, 'thebody': 3945, 'thebook': 3946, 'thebow': 3947, 'thebuilding': 3948, 'thebuttocks': 3949, 'thecapital': 3950, 'thecarpenter': 3951, 'thechance': 3952, 'thecombat': 3953, 'thecommander': 3954, 'thecompanion': 3955, 'theconnection': 3956, 'thecontents': 3957, 'thecontest': 3958, 'thecorrect': 3959, 'thecountry': 3960, 'thecraft': 3961, 'thecut': 3962, 'thedeep': 3963, 'thedeepest': 3964, 'thedefensive': 3965, 'thedepths': 3966, 'thedetails': 3967, 'thedirect': 3968, 'thedirection': 3969, 'thedisciple': 3970, 'thedistance': 3971, 'theelbows': 3972, 'theen-': 3973, 'theenemies': 3974, 'theenemy': 3975, 'theentrance': 3976, 'theessence': 3977, 'theex-': 3978, 'theeyes': 3979, 'theface': 3980, 'thefast': 3981, 'thefavour': 3982, 'thefeeling': 3983, 'thefeet': 3984, 'thefield': 3985, 'thefight': 3986, 'thefinished': 3987, 'thefirebehind': 3988, 'thefirebesmall': 3989, 'thefirst': 3990, 'thefive': 3991, 'theflank': 3992, 'theflower': 3993, 'thefluctuation': 3994, 'thefolding': 3995, 'thefootwork': 3996, 'theford': 3997, 'theforeman': 3998, 'thegaze': 3999, 'thegeneral': 4000, 'theguardians': 4001, 'thegun': 4002, 'thehalberd': 4003, 'thehand': 4004, 'thehands': 4005, 'thehat': 4006, 'thehead': 4007, 'thehuman': 4008, 'theimmortality': 4009, 'theimport-': 4010, 'theimportance': 4011, 'theimpression': 4012, 'theinapplicable': 4013, 'theinitiative': 4014, 'theinterior': 4015, 'their': 4016, 'theissue': 4017, 'thejustman': 4018, 'theknees': 4019, 'thelarge': 4020, 'thelead': 4021, 'thelength': 4022, 'theline': 4023, 'thelong': 4024, 'thelongsword': 4025, 'thelovemaking': 4026, 'thelower': 4027, 'them': 4028, 'themany': 4029, 'themeaning': 4030, 'themeans': 4031, 'themiddle': 4032, 'themilitary': 4033, 'themountains': 4034, 'themusic': 4035, 'then': 4036, 'thename': 4037, 'thence': 4038, 'thenutand': 4039, 'thenuthasbecome': 4040, 'theopponent': 4041, 'theopportunity': 4042, 'theorder': 4043, 'theorist': 4044, 'theory': 4045, 'theother': 4046, 'theoutset': 4047, 'thepath': 4048, 'thepeople': 4049, 'theperilous': 4050, 'theperspectival': 4051, 'theplace': 4052, 'theplans': 4053, 'thepoint': 4054, 'theprin-': 4055, 'theprinciple': 4056, 'theprinciples': 4057, 'theprogress': 4058, 'theranks': 4059, 'there': 4060, 'thereafter': 4061, 'thereason': 4062, 'therelevant': 4063, 'therole': 4064, 'theroute': 4065, 'theruler': 4066, 'therules': 4067, 'thesacred': 4068, 'thesame': 4069, 'theschools': 4070, 'these': 4071, 'thesea': 4072, 'theseaatastrait': 4073, 'thesecular': 4074, 'theshallowest': 4075, 'theshape': 4076, 'theshort': 4077, 'theshoulders': 4078, 'thesituation': 4079, 'thesmallest': 4080, 'thespace': 4081, 'thespear': 4082, 'thespeed': 4083, 'thespirit': 4084, 'thespiritual': 4085, 'thestakes': 4086, 'thestart': 4087, 'thestory': 4088, 'thestrategies': 4089, 'thestrategy': 4090, 'thestrength': 4091, 'thestyle': 4092, 'thesubjective': 4093, 'thesun': 4094, 'thesuperior': 4095, 'thesupreme': 4096, 'thesword': 4097, 'theterm': 4098, 'thethings': 4099, 'thethree': 4100, 'thethresholds': 4101, 'thetim-': 4102, 'thetime': 4103, 'thetimes': 4104, 'thetiming': 4105, 'thetips': 4106, 'thetradi-': 4107, 'thetradition': 4108, 'thetraining': 4109, 'thetroops': 4110, 'thetrue': 4111, 'thetwofold': 4112, 'theunjust': 4113, 'theunsatisfactory': 4114, 'theuseofTaoist': 4115, 'thevalue': 4116, 'theviewpoint': 4117, 'thevirtue': 4118, 'thevirtues': 4119, 'thevoice': 4120, 'thewarrior': 4121, 'theway': 4122, 'theweapon': 4123, 'thewhole': 4124, 'thewisdom': 4125, 'thework': 4126, 'theworld': 4127, 'they': 4128, 'theyears': 4129, 'thflower': 4130, 'thing': 4131, 'things': 4132, 'think': 4133, 'thinking': 4134, 'thinks': 4135, 'thirteen': 4136, 'thirty': 4137, 'this': 4138, 'thor-': 4139, 'thoroughly': 4140, 'those': 4141, 'though': 4142, 'thought': 4143, 'thoughtlessly': 4144, 'thousand': 4145, 'thousand-mile': 4146, 'thread': 4147, 'three': 4148, 'thresholds': 4149, 'thriving': 4150, 'through': 4151, 'throughout': 4152, 'throw': 4153, 'thrust': 4154, 'thrusting': 4155, 'thumb': 4156, 'thus': 4157, 'thwarted': 4158, 'tially': 4159, 'tical': 4160, 'tiger': 4161, 'tight': 4162, 'tillnight': 4163, 'tim-': 4164, 'timber': 4165, 'time': 4166, 'times': 4167, 'timing': 4168, 'timings': 4169, 'tination': 4170, 'ting': 4171, 'tion': 4172, 'tional': 4173, 'tions': 4174, 'tired': 4175, 'titude': 4176, 'tleman': 4177, 'to': 4178, 'to-': 4179, 'toHeidegger': 4180, 'toKwannon': 4181, 'toNandi': 4182, 'toTomonobu': 4183, 'toWestern': 4184, 'toaPhilosophy': 4185, 'toachieve': 4186, 'toactually': 4187, 'toadestructive': 4188, 'toadvance': 4189, 'toap-': 4190, 'toappraise': 4191, 'toappreciate': 4192, 'toapproach': 4193, 'toarrest': 4194, 'toat-': 4195, 'toattack': 4196, 'toattain': 4197, 'toawkward': 4198, 'tobe': 4199, 'tobeable': 4200, 'tobeat': 4201, 'tobecome': 4202, 'tobeinthe': 4203, 'tobeledabout': 4204, 'tobepassed': 4205, 'tobepiled': 4206, 'tobeswayed': 4207, 'tobethe': 4208, 'toblink': 4209, 'toboth': 4210, 'tobring': 4211, 'tocause': 4212, 'tochange': 4213, 'tochanging': 4214, 'tochase': 4215, 'toclose': 4216, 'tocollapse': 4217, 'tocome': 4218, 'tocomprehend': 4219, 'toconfuse': 4220, 'toconsider': 4221, 'tocontrol': 4222, 'tocorrelate': 4223, 'tocounter-cut': 4224, 'tocountering': 4225, 'tocrush': 4226, 'tocut': 4227, 'tocutanenemy': 4228, 'tocutatall': 4229, 'tocuthim': 4230, 'tocutquickly': 4231, 'tocuttheenemy': 4232, 'tocutting': 4233, 'today': 4234, 'todefeat': 4235, 'todescribe': 4236, 'todie': 4237, 'todiscern': 4238, 'todiscover': 4239, 'todisturb': 4240, 'todoit': 4241, 'todraw': 4242, 'toes': 4243, 'toestablish': 4244, 'toeveryday': 4245, 'toexplain': 4246, 'toexplaining': 4247, 'toface': 4248, 'tofalling': 4249, 'tofinish': 4250, 'tofloat': 4251, 'toflourishing': 4252, 'tofollow': 4253, 'tofreely': 4254, 'tofully': 4255, 'together': 4256, 'togodeeper': 4257, 'tograpple': 4258, 'togrips': 4259, 'tohand': 4260, 'tohasten': 4261, 'tohave': 4262, 'toheaven': 4263, 'tohelp': 4264, 'tohiscondition': 4265, 'tohit': 4266, 'tohitinthetiming': 4267, 'tohold': 4268, 'tojump': 4269, 'toknock': 4270, 'toknow': 4271, 'tolarge': 4272, 'tolarge-scale': 4273, 'tolast': 4274, 'tolayhisplans': 4275, 'tolead': 4276, 'tolearn': 4277, 'tolike': 4278, 'tolook': 4279, 'tomaintain': 4280, 'tomake': 4281, 'toman': 4282, 'tomaster': 4283, 'tome': 4284, 'tomorrow': 4285, 'tomove': 4286, 'tomyhaving': 4287, 'tonature': 4288, 'too': 4289, 'tools': 4290, 'toone': 4291, 'tooquickly': 4292, 'tooslowly': 4293, 'toparry': 4294, 'topay': 4295, 'toperceive': 4296, 'topic': 4297, 'topics': 4298, 'toplan': 4299, 'topoint': 4300, 'topredict': 4301, 'toprovince': 4302, 'toput': 4303, 'toraise': 4304, 'tore-': 4305, 'toreal': 4306, 'torealize': 4307, 'torecognize': 4308, 'torecord': 4309, 'torecover': 4310, 'torenew': 4311, 'torepeat': 4312, 'toright': 4313, 'torush': 4314, 'tosacred': 4315, 'tosaythat': 4316, 'toscold': 4317, 'toseedistant': 4318, 'tosell': 4319, 'toshoot': 4320, 'toshow': 4321, 'toside': 4322, 'toslash': 4323, 'tosomething': 4324, 'tostart': 4325, 'tostrike': 4326, 'tostrive': 4327, 'tosword-fencing': 4328, 'totake': 4329, 'tothe': 4330, 'totheWay': 4331, 'totheWestern': 4332, 'tothebow': 4333, 'totheen-': 4334, 'totheenemy': 4335, 'tothefive': 4336, 'totheir': 4337, 'tothelong': 4338, 'tothepupil': 4339, 'totheright': 4340, 'tothesituation': 4341, 'tothespear': 4342, 'tothetips': 4343, 'tothetrue': 4344, 'totheuse': 4345, 'tothewis-': 4346, 'tothink': 4347, 'tothis': 4348, 'tothrust': 4349, 'totime': 4350, 'totraining': 4351, 'totreat': 4352, 'totrytocut': 4353, 'totwenty': 4354, 'touch': 4355, 'touching': 4356, 'toun-': 4357, 'tounderstand': 4358, 'tounderstanding': 4359, 'tousethefive': 4360, 'tousethese': 4361, 'tousetomake': 4362, 'tousetwo': 4363, 'toutterly': 4364, 'towards': 4365, 'towatch': 4366, 'towhether': 4367, 'towield': 4368, 'towielding': 4369, 'towin': 4370, 'towithdraw': 4371, 'towithin': 4372, 'toyour': 4373, 'tra-': 4374, 'trace': 4375, 'tradition': 4376, 'traditions': 4377, 'train': 4378, 'trained': 4379, 'training': 4380, 'translations': 4381, 'transmission': 4382, 'trapped': 4383, 'traterrestrial': 4384, 'traveling': 4385, 'tread': 4386, 'treading': 4387, 'treatise': 4388, 'tree': 4389, 'tribution': 4390, 'trickle': 4391, 'tried': 4392, 'tries': 4393, 'trifles': 4394, 'trod': 4395, 'trooper': 4396, 'troopers': 4397, 'troops': 4398, 'troy': 4399, 'true': 4400, 'trustworthy': 4401, 'truth': 4402, 'trying': 4403, 'trythis': 4404, 'tryto': 4405, 'trytobeat': 4406, 'trytocutquickly': 4407, 'trytohityour': 4408, 'trytokeep': 4409, 'trytowield': 4410, 'tude': 4411, 'tudes': 4412, 'turn': 4413, 'tury': 4414, 'twelve': 4415, 'twenty': 4416, 'twenty-eight': 4417, 'twenty-one': 4418, 'twice': 4419, 'two': 4420, 'twofold': 4421, 'ular': 4422, 'un-': 4423, 'un-knotted': 4424, 'unawares': 4425, 'uncertain': 4426, 'unchanging': 4427, 'unconstricted': 4428, 'und': 4429, 'undecided': 4430, 'under': 4431, 'understand': 4432, 'understanding': 4433, 'understands': 4434, 'undesirable': 4435, 'undisturbed': 4436, 'undrawn': 4437, 'unexpected': 4438, 'unguarded': 4439, 'unreasonable': 4440, 'unsatisfactory': 4441, 'unsettling': 4442, 'unsuccessfully': 4443, 'until': 4444, 'up': 4445, 'up-': 4446, 'up-itisman': 4447, 'upanattitude': 4448, 'upand': 4449, 'uparear': 4450, 'upasword': 4451, 'upfrom': 4452, 'upoftwo': 4453, 'upon': 4454, 'uptheattack': 4455, 'uptheenemies': 4456, 'upto': 4457, 'uptoheavy': 4458, 'upyour': 4459, 'usage': 4460, 'usand': 4461, 'use': 4462, 'useashorter': 4463, 'useboth': 4464, 'used': 4465, 'useequipment': 4466, 'useful': 4467, 'useless': 4468, 'usenowadays': 4469, 'useof': 4470, 'useofthecompanion': 4471, 'useofyour': 4472, 'users': 4473, 'uses': 4474, 'usetheadvantage': 4475, 'usethis': 4476, 'using': 4477, 'usually': 4478, 'utterance': 4479, 'utterances': 4480, 'utterly': 4481, 'valid': 4482, 'valleys': 4483, 'value': 4484, 'values': 4485, 'vantageous': 4486, 'varied': 4487, 'various': 4488, 'venient': 4489, 'verandas': 4490, 'vergence': 4491, 'very': 4492, 'victories': 4493, 'victory': 4494, 'view': 4495, 'viewpoint': 4496, 'vigour': 4497, 'vious': 4498, 'virtue': 4499, 'virtues': 4500, 'visual': 4501, 'voice': 4502, 'void': 4503, 'von': 4504, 'vour': 4505, 'voyage': 4506, 'wait': 4507, 'waiting': 4508, 'walk': 4509, 'walking': 4510, 'want': 4511, 'wants': 4512, 'ward': 4513, 'wards': 4514, 'warning': 4515, 'warrior': 4516, 'warriors': 4517, 'was': 4518, 'water': 4519, 'waver': 4520, 'waves': 4521, 'way': 4522, 'ways': 4523, 'we': 4524, 'weak': 4525, 'weaken': 4526, 'weakly': 4527, 'weakness': 4528, 'weapon': 4529, 'weaponry': 4530, 'weapons': 4531, 'wearedeadlocked': 4532, 'wearefighting': 4533, 'webecome': 4534, 'wecan': 4535, 'wecanattain': 4536, 'wecanconfuse': 4537, 'wecandes-': 4538, 'wecannot': 4539, 'wecanuseourtroops': 4540, 'wecanwield': 4541, 'wecanwin': 4542, 'wecutwith': 4543, 'wedeploy': 4544, 'wedges': 4545, 'wedging': 4546, 'week': 4547, 'wefind': 4548, 'wehave': 4549, 'weighing': 4550, 'weknock': 4551, 'well': 4552, 'welook': 4553, 'wemake': 4554, 'wemove': 4555, 'wemust': 4556, 'weneed': 4557, 'were': 4558, 'weseearts': 4559, 'weseethat': 4560, 'weshout': 4561, 'wethink': 4562, 'weusually': 4563, 'what': 4564, 'whatever': 4565, 'when': 4566, 'whenever': 4567, 'where': 4568, 'whereas': 4569, 'whether': 4570, 'which': 4571, 'while': 4572, 'who': 4573, 'whole': 4574, 'wide': 4575, 'widely': 4576, 'wield': 4577, 'will': 4578, 'win': 4579, 'wind': 4580, 'winding': 4581, 'wine': 4582, 'winning': 4583, 'wins': 4584, 'wisdom': 4585, 'wishes': 4586, 'with': 4587, 'withdrawing': 4588, 'withdraws': 4589, 'within': 4590, 'without': 4591, 'women': 4592, 'won': 4593, 'woods': 4594, 'word': 4595, 'words': 4596, 'work': 4597, 'works': 4598, 'world': 4599, 'worldly': 4600, 'worry': 4601, 'worrying': 4602, 'would': 4603, 'write': 4604, 'writing': 4605, 'written': 4606, 'wrong': 4607, 'wrongs': 4608, 'yards': 4609, 'yawning': 4610, 'year': 4611, 'years': 4612, 'yet': 4613, 'yetanother': 4614, 'yetdeeper': 4615, 'yetnot': 4616, 'yetunbiased': 4617, 'yetundefeated': 4618, 'yetyou': 4619, 'yoga': 4620, 'you': 4621, 'young': 4622, 'your': 4623, 'yourself': 4624, 'yout': 4625, 'youth': 4626, '—although': 4627, '—byOkakura': 4628}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we have vocab with associated unique integer . But later we need a function which will convert these numbers back into words or respective tokens"
      ],
      "metadata": {
        "id": "43sKipSi1Ywx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's create a class for this"
      ],
      "metadata": {
        "id": "YYdUEzPB182d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab  #wil be used for encoding\n",
        "    self.int_to_str = {id:token for token,id in vocab.items()}  #will be used for decoding\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?;:_!\"()\\'] | --|\\s)',text)\n",
        "\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "C2Ywaqgh18kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizer1(vocab)\n",
        "\n",
        "test_text = \"\"\"The Book of Five Rings\n",
        "Miyamoto, Musashi\"\"\"\n",
        "ids = tokenizer.encode(test_text)\n",
        "print(ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrJwPNgFqTXw",
        "outputId": "8330c9a1-4b3b-4983-d8c9-57a3a62e793c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[668, 158, 2992, 256, 579, 489, 6, 494]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ouVjJNo42xl7",
        "outputId": "383f165d-68b5-457e-e05e-5c514b8ff1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Book of Five Rings Miyamoto, Musashi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our encoding and decoding is working fine but what if we get any word which is not present in our training data we'll get error so we need a large and diverse training data but still we can get any word which is not present in training data"
      ],
      "metadata": {
        "id": "M3iYCWl292CH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Special Context Tokens"
      ],
      "metadata": {
        "id": "VbK81cgR-HfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we will modiy our SImpleEncoder function to handle unkown words which were not present in training data\n"
      ],
      "metadata": {
        "id": "KwRR-Axl-Pz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<|unk|> and <|endoftext|> these two will be last two tokens so they will be assigned largest id     ex: last word of vocab is king->700 sp <|unk|>->702,<|endoftext|>->701"
      ],
      "metadata": {
        "id": "PgO8-Gom-0xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**so if we get any unkown text it'll be assigned |unk|**"
      ],
      "metadata": {
        "id": "N4lOrALqAexE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|endoftext| will be used after a document is completed.    ex- while training gpt multiple books were used so after a book all data was encoded . this |endoftext| is added so there is no relation between previous book and next book"
      ],
      "metadata": {
        "id": "zHr1IT8cCdds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll add these two token into our tokenizer"
      ],
      "metadata": {
        "id": "mxYcs9LrC4Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed_data)))\n",
        "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
        "\n",
        "vocab = {token: id for id,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "mn_JTE8l8F9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEBJpGnhDWNQ",
        "outputId": "1be0fe0d-3a15-4b75-9ef6-d54276b3ba13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4631"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's implement version 2**"
      ],
      "metadata": {
        "id": "FjnL5BYCD2wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer2:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab  #wil be used for encoding\n",
        "    self.int_to_str = {id:token for token,id in vocab.items()}  #will be used for decoding\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?;:_!\"()\\'] | --|\\s)',text)\n",
        "\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "U_B3YAtRDbL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = SimpleTokenizer2(vocab)\n",
        "text1 = \"Habu Dabu,do you like tea?\"\n",
        "text2 = \"The Book of Five Rings\"\n",
        "\n",
        "text_final = \"<|endoftext|>\".join((text1,text2))\n",
        "print(text_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_UElp9qEM0D",
        "outputId": "23c7a540-d7f9-4ff0-eafc-72b6c1c9145a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Habu Dabu,do you like tea?<|endoftext|>The Book of Five Rings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2.encode(text_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhrJ-_R9FCmU",
        "outputId": "fcb41393-fe2b-4081-e766-7d29f74c9c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4630, 4630, 4621, 2746, 4630, 158, 2992, 256, 579]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2.decode(tokenizer2.encode(text_final))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z6rZgOKKFGyF",
        "outputId": "490a59f7-eff1-4b38-a871-ae1e9401a92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|> <|unk|> you like <|unk|> Book of Five Rings'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have |unk|,||endoftext| after these two we have some more special tokens    [BOS](Begining of sequence)-It marks the sart of a text.\n",
        "[EOS](end of sequence) - Similar to |endoftext| it's used to when two unrealted text are combined.\n",
        "\n",
        "[PAD]-Used to make the length of texts equal basic whi padding wala kaam"
      ],
      "metadata": {
        "id": "mJGOJZrpGx3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are able to handle the unkown text we are not getting any error"
      ],
      "metadata": {
        "id": "BEs4jj_nGs7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--Gpt tokenizer don't use any of these special tokens t only used    \"|endoftext|\" token"
      ],
      "metadata": {
        "id": "7VUCePqJJmqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--Gpt don't use |unk| to handle oov words instead it uses ***bye pair encoding***     (breaks word into subwords)"
      ],
      "metadata": {
        "id": "gvR1hBFLJzE7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfBIgz2xGAIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now implementing complete tokenizer like used for gpt is hard and will consume time so we'lll use the same tokenizer used by openai for gpt"
      ],
      "metadata": {
        "id": "Gm31qmPFjI8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now complete understanding of tokenizer so we can use prebuilt one"
      ],
      "metadata": {
        "id": "hxvrFrljjZKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB-5OEuljVh7",
        "outputId": "d15cdfed-ca74-4d9d-d5e9-d73aa8a742b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "NFmSqQkDjf2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It'll be similar to our Simpletokenizer2 we had implemented"
      ],
      "metadata": {
        "id": "hy-1UHwBjqtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = {\n",
        "    \"text\": \"Habu Dabu, do you like tea? <|enoftext|> Miyamoto Musashi (c. 1584–June 13 (Japanese calendar: May 19), 1645). My name is Prabhakar.\"\n",
        "}\n",
        "\n",
        "integers = tokenizer.encode(text1['text'], allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOUnIBfnjo-0",
        "outputId": "5636c311-cf07-484a-8c45-e7e485378c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[39, 397, 84, 360, 397, 84, 11, 466, 345, 588, 8887, 30, 1279, 91, 268, 1659, 5239, 91, 29, 29464, 25384, 2629, 12144, 357, 66, 13, 1315, 5705, 1906, 15749, 1511, 357, 25324, 11845, 25, 1737, 678, 828, 1467, 2231, 737, 2011, 1438, 318, 1736, 397, 43573, 283, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i460HDDOkf2C",
        "outputId": "1edf94ad-6890-45d7-d1a4-41ff0ad688c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Habu Dabu, do you like tea? <|enoftext|> Miyamoto Musashi (c. 1584–June 13 (Japanese calendar: May 19), 1645). My name is Prabhakar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**   Bpe reduces tokens count around 50k and it also handles oov by bring words to subwords and character level"
      ],
      "metadata": {
        "id": "nuxA_29YmmMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fun_try = tokenizer.encode(\"Kya be bhutnike\")\n",
        "print(fun_try)\n",
        "strs = tokenizer.decode(fun_try)\n",
        "print(strs)\n",
        "\n",
        "#working fine on any ranodm text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nax_B6kmm7-",
        "outputId": "74f076cf-6c24-4b32-eb02-8713b0220352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42, 3972, 307, 275, 71, 315, 77, 522]\n",
            "Kya be bhutnike\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "URTBcfldmVFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Input Output Pairs**"
      ],
      "metadata": {
        "id": "im3vKlQZnpeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's same like in next word prediction we use previous word to predict next ord,in next iteration starting two word will be used to predict 3rd word like this goes till last word. Similar to stock price prediction data"
      ],
      "metadata": {
        "id": "EH2bqvq6ptiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "basically starting word ko lenge next predict krenege baki ko mask kr denge, next iteration mai starting 2 word lenge or 3rd predict krenge ,next iteration mai 3 words lenge or 4th predict krenge or krte jyege.             this technique is called self supervised learning ,auto regressive learning,unsupervised learning"
      ],
      "metadata": {
        "id": "mEXZ8jZ5qOzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Input-output pairs"
      ],
      "metadata": {
        "id": "ymdb1ucprKAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "will create data laoder which will use sliding window appraoch"
      ],
      "metadata": {
        "id": "gxdmssVerOGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(text)\n",
        "print(len(enc_text))\n",
        "print(len(text))\n",
        "\n",
        "#we have applied byte pair encoding to text data now it's in subwords and characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeMuw9XbntUf",
        "outputId": "f903f007-5d7b-4534-c146-9853efe2243f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24021\n",
            "91238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "context size = sliding window size = no of words used to predict next word"
      ],
      "metadata": {
        "id": "BbFC5pujsdyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 6   #input len,means model is trained to look at a sequence of 6 words to predict next word in sequence.\n",
        "#The input  x is first six tokens [1,2,3,4,5,6] and target y is next 6 tokens [2,3,4,5,6,7]\n",
        "\n",
        "\n",
        "X = enc_text[:context_size]\n",
        "Y = enc_text[1:context_size+1]\n",
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHW5P7NfrGPs",
        "outputId": "00cabf08-11ec-44b9-fb53-87d90ef4cee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464, 4897, 286, 10579, 26028, 198]\n",
            "[4897, 286, 10579, 26028, 198, 44]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context = enc_text[:i]\n",
        "  desired_text =enc_text[i]\n",
        "  print(context,\"------->\",desired_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2O77L1DtfJ3",
        "outputId": "4c4e3a46-9567-4fca-d7c3-e3c0310a13f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464] -------> 4897\n",
            "[464, 4897] -------> 286\n",
            "[464, 4897, 286] -------> 10579\n",
            "[464, 4897, 286, 10579] -------> 26028\n",
            "[464, 4897, 286, 10579, 26028] -------> 198\n",
            "[464, 4897, 286, 10579, 26028, 198] -------> 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In llms one input-ouput pair have predictions equal to context size,like in regression and classification one input and target have one prediction**     ex 6 context size so 6 predictions"
      ],
      "metadata": {
        "id": "8qPfsUoIuwdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context = enc_text[:i]\n",
        "  desired_text =enc_text[i]\n",
        "  print(tokenizer.decode(context),\"------->\",tokenizer.decode([desired_text]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6FqjlKEuSiB",
        "outputId": "d1424064-57bd-40ae-8d09-37da15a0bc62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ------->  Book\n",
            "The Book ------->  of\n",
            "The Book of ------->  Five\n",
            "The Book of Five ------->  Rings\n",
            "The Book of Five Rings -------> \n",
            "\n",
            "The Book of Five Rings\n",
            " -------> M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a basic input output paris generator but we want a structured way or methods so we can compute the tokenization parallely on gpu or cpu"
      ],
      "metadata": {
        "id": "4VAuh4x-vy3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll implement a efficient data laoder that takes input and targets as pytorch tensors,which is n0dimentional arrays .we can do parallel processing"
      ],
      "metadata": {
        "id": "5F2aMz3-wEri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we want only 2 tensors one is input tensor containing text that llm will use a input to predict and tensor two which is target"
      ],
      "metadata": {
        "id": "LGQfXRL_wXvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implmenting a Data Loader**"
      ],
      "metadata": {
        "id": "QfJ4Em3g0JD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For efficient data loader implementation we'll use Pytorch built in Dataset and Dataloader classes"
      ],
      "metadata": {
        "id": "8gZ4v5Ux0STs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#step1: Toekenizer entire text\n",
        "#step2:Use sliding window appraoch to chunk the book into overlapping sequences of max_length\n",
        "#step3:Return total number of rows in dataset\n",
        "#step4:Return a single row from dataset"
      ],
      "metadata": {
        "id": "CJpM-aKDwC_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yls5zKvpvQu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDataset1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i+1:i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "JS0JDcKA04eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we'll create Dataloader which wil use GPTDataset1 to load the inputs in baches via pytoch Dataloader"
      ],
      "metadata": {
        "id": "z3W0_Qso3_ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "s1:initialize the tokenizer            \n",
        "s2:create Dataset                 \n",
        "s3:drop_last = True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training                     \n",
        "s4:no. of cpu processes to use for preprocessing"
      ],
      "metadata": {
        "id": "q22pJQPO4RIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDataset1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "XnhhbJmm4OgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test these"
      ],
      "metadata": {
        "id": "p8xD0A9v6aTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(text,batch_size=1,max_length=4,stride=1,shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)\n",
        "sec_batch = next(data_iter)\n",
        "print(sec_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C-fMPHq6beG",
        "outputId": "b248c9bd-f6c8-4aa0-8280-e6d20194638e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  464,  4897,   286, 10579]]), tensor([[ 4897,   286, 10579, 26028]])]\n",
            "[tensor([[ 4897,   286, 10579, 26028]]), tensor([[  286, 10579, 26028,   198]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "working fine first batch contains two tensors 1.input tenosr 2. output tensors"
      ],
      "metadata": {
        "id": "9E0PH5nk6_BG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "low stride - more overalp of words- overfitting"
      ],
      "metadata": {
        "id": "2NQYuimq-smV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Token Embeddings with examples"
      ],
      "metadata": {
        "id": "bfZdUM4TzzkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "quick  fox   is   in   the   house  4  0  3  2  5  1  , for keeping it simple we have took 6 words only and if size 3"
      ],
      "metadata": {
        "id": "bKULjHabe0g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2,3,5,1])\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n"
      ],
      "metadata": {
        "id": "Bei--2UM6wQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)   #this matrix is lookup table (lookup table is nothing we can pass the index and get the vector )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-ybv8dL7Zbi",
        "outputId": "e3a04ebb-4267-4ed3-8120-e1aeaee8ca78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP3I6yFS-Lft",
        "outputId": "cd775f31-d76b-4305-cdbe-77ee32c2e59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
      ],
      "metadata": {
        "id": "jkJscPlMgGYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4  #contex_size\n",
        "dataloader = create_dataloader_v1(text,batch_size=8,max_length=max_length,stride=1,shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs,targets = next(data_iter)\n",
        "print(targets)\n",
        "print(\"Input shape:\",inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8azAIqi45RZ",
        "outputId": "93060bbf-643f-4b14-dac6-74e24bba7ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4897,   286, 10579, 26028],\n",
            "        [  286, 10579, 26028,   198],\n",
            "        [10579, 26028,   198,    44],\n",
            "        [26028,   198,    44,  7745],\n",
            "        [  198,    44,  7745, 25384],\n",
            "        [   44,  7745, 25384,    11],\n",
            "        [ 7745, 25384,    11,  2629],\n",
            "        [25384,    11,  2629, 12144]])\n",
            "Input shape: torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have tensor of size 8*4 dim,meaning data batch consists of 8 text samples with 4 tokens each\n"
      ],
      "metadata": {
        "id": "aRuLTHA66bhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use embedding layer to embed these token ids into 256-dimensional vectors"
      ],
      "metadata": {
        "id": "KxLzuGzjCkll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)\n",
        "\n",
        "# sentence_embeddings = pooling_layer(token_embeddings)\n",
        "# print(sentence_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq7e6bo16CFH",
        "outputId": "48e88c8a-98e1-4c85-e18f-8ba1929c8cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for gpt's absolute embedding appraoch,we need to create another embedding layer that has same dimension"
      ],
      "metadata": {
        "id": "_awzfqJ9DWf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)"
      ],
      "metadata": {
        "id": "CDQeoR_1C7md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating  postional embedding vectors 4*256 becuase our context_size is 4"
      ],
      "metadata": {
        "id": "kkb4QUdREG6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embedding_layer = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embedding_layer.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm8LDuPiENg6",
        "outputId": "e651d907-06a6-410a-cb4e-b78f93c2c1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention Mechnaism**"
      ],
      "metadata": {
        "id": "v8tFdyQGcSp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing a simplified attention mechanism,will use 3 dimensional vector for this"
      ],
      "metadata": {
        "id": "FT9ZWHo7cXTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [[0.43,0.15,0.89], #Your (X^1)\n",
        "     [0.55,0.87,0.66], #journey(x^2)\n",
        "     [0.57,0.85,0.64], #starts (X^3)\n",
        "     [0.22,0.58,0.33], #with (X^5)\n",
        "     [0.05,0.80,0.55] #step (X^6)\n",
        "\n",
        "\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "sk_xYcEEER_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from  mpl_toolkits import Axes3D\n",
        "\n",
        "words = [\"Your\",\"journey\",\"starts\",\"with\",\"one\",\"step\"]\n"
      ],
      "metadata": {
        "id": "B7UPlCR3dkpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]  #2nd input token is query(journey)\n",
        "\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i,x_i in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(x_i,query)  #dot produc to find similarity between query and the embedding vectors or other words\n",
        "\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WraIeDpkgsPO",
        "outputId": "84184cd4-19af-4b1a-ecd0-87887685d411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now normalize the attention scores to make them fall in a range and their weights sum up to 1. 2nd llm work better if it's in a range 0-1 because normalization helps to reach optimal state fast"
      ],
      "metadata": {
        "id": "Lwjy7BoJh23i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2_tmp = attn_scores_2/attn_scores_2.sum()\n",
        "\n",
        "print(\"Attention weights:\",attn_weights_2_tmp)\n",
        "print(\"Sum:\",attn_weights_2_tmp.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWxSb-4HhCk0",
        "outputId": "0b9f7034-fd98-4af1-e183-cdac30f19504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1630, 0.2554, 0.2520, 0.1441, 0.1856])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above i ahve implemented a simple normalization but it's recommended to use softmax for normalization because it's gives values values which are favourable for gradient process."
      ],
      "metadata": {
        "id": "9AfodYpkjmmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_naive(X):\n",
        "  return torch.exp(X)/torch.exp(X).sum(dim=0)\n",
        "\n",
        "attn_weights_2_navie = softmax_naive(attn_scores_2)\n",
        "# print(\"Attention weights:\",attn_scores_2)1\n",
        "\n",
        "print(\"Normalized attention weights:\",attn_weights_2_navie)\n",
        "print(\"Sum:\",attn_weights_2_navie.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Z9QotiinXI",
        "outputId": "d73c5332-837e-4396-946a-6bfc658c7ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized attention weights: tensor([0.1554, 0.2667, 0.2616, 0.1390, 0.1773])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But there is problem with this navie softmax implementation it'll face problem of overflow when dealing with large number and it'll face problem of underflow when dealing with small number"
      ],
      "metadata": {
        "id": "1we80Hypk07i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2,dim=0)\n",
        "print(\"Attention weights:\",attn_weights_2)\n",
        "\n",
        "print(\"Sum:\",attn_weights_2.sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpykHWkhkW77",
        "outputId": "d7f2f446-ec3a-4d1a-ec37-1ae2a8d87a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1554, 0.2667, 0.2616, 0.1390, 0.1773])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1] #2nd input token\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "  context_vec_2 += attn_weights_2[i]*x_i\n",
        "\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33v4oYb3lQ-A",
        "outputId": "6d7fa9f1-0257-41c3-fa20-1f3c67bb3d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4021, 0.7002, 0.6251])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "till now we have computed attention weight for journey 2nd(1 index) token we can extend this and compute attention for all inputs"
      ],
      "metadata": {
        "id": "bIwrTSGTnWAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6,6)\n",
        "\n",
        "for i,x_i in enumerate(inputs):\n",
        "  for j,x_j in enumerate(inputs):\n",
        "    attn_scores[i,j] = torch.dot(x_i,x_j)\n",
        "\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y69jxhY6nKPM",
        "outputId": "1f0eb41f-1720-48dd-8be5-33e082cc16b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.9950e-01, 9.5440e-01, 9.4220e-01, 4.7530e-01, 6.3100e-01, 8.5403e+20],\n",
            "        [9.5440e-01, 1.4950e+00, 1.4754e+00, 8.4340e-01, 1.0865e+00, 1.6427e-07],\n",
            "        [9.4220e-01, 1.4754e+00, 1.4570e+00, 8.2960e-01, 1.0605e+00, 6.1949e-04],\n",
            "        [4.7530e-01, 8.4340e-01, 8.2960e-01, 4.9370e-01, 6.5650e-01, 4.5447e+30],\n",
            "        [6.3100e-01, 1.0865e+00, 1.0605e+00, 6.5650e-01, 9.4500e-01, 1.9284e+31],\n",
            "        [3.2314e-18, 1.1963e+22, 2.1069e-07, 1.0074e-11, 2.1888e+23, 4.1725e-08]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting correct result but 2 for loops are comptationally expensive so'll use another appraoch using linear lagebra"
      ],
      "metadata": {
        "id": "EL4TxP-8o6WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we used for loops which are slower ,we can get the same result by matrix multiplicaiton"
      ],
      "metadata": {
        "id": "194YgTQCpHYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T    #@ for matrix multiplication\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKd44jvnop3X",
        "outputId": "d916aaff-daf4-44e9-ab81-d00d97259f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.6565],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNip5qhApUoN",
        "outputId": "634e88fc-6878-4e25-bc45-900b5021017a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2390, 0.2285, 0.2257, 0.1415, 0.1653],\n",
            "        [0.1554, 0.2667, 0.2616, 0.1390, 0.1773],\n",
            "        [0.1563, 0.2664, 0.2616, 0.1397, 0.1760],\n",
            "        [0.1643, 0.2374, 0.2341, 0.1673, 0.1969],\n",
            "        [0.1537, 0.2423, 0.2361, 0.1576, 0.2103]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dim=-1 ,we are instructing softmax function to apply normalization along the last dimension of the attn_scores"
      ],
      "metadata": {
        "id": "z8Bs11S1qvR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "so let's calculate final context_vector instead of multiplying then adding the attn_weights and query ,we can get this result just by matrix multiplication"
      ],
      "metadata": {
        "id": "MNTCXSmurbLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLdmtgS6qkNU",
        "outputId": "d53861f4-f48c-454e-e855-2e88192f9482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3965, 0.6408, 0.6456],\n",
            "        [0.4021, 0.7002, 0.6251],\n",
            "        [0.4024, 0.6994, 0.6253],\n",
            "        [0.3813, 0.6847, 0.6162],\n",
            "        [0.3791, 0.6942, 0.6155]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find attention weights using this appraoch and can capture some context but still we can't understand and capture all context without trainable weights"
      ],
      "metadata": {
        "id": "igDERTnMsZEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex- The cat sat on the mat vecause it was warm      without Trainable weights: it'll not pay that much attnetion to mat and warm because both are not realated but it's not true we should focus on both\n",
        "\n",
        "\n",
        "With Trainable weights: model can learn \"warm\" should pay more attention to \"mat\" even if \"mat\" is not semantically similar to \"warm\""
      ],
      "metadata": {
        "id": "Yyj7wBT5srsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short without trainable weights it'll focus only on words related to query,and will leave others and will lost long-term dependency."
      ],
      "metadata": {
        "id": "jE7VnxPethRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Self Attention With Trainable Weights"
      ],
      "metadata": {
        "id": "CrPwnKhWr7yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "    [[0.43,0.15,0.89], #Your (X^1)\n",
        "     [0.55,0.87,0.66], #journey(x^2)\n",
        "     [0.57,0.85,0.64], #starts (X^3)\n",
        "     [0.22,0.58,0.33], #with (X^4)\n",
        "     [0.05,0.80,0.55] #step (X^5)\n",
        "\n",
        "\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "2TTYeYzGrvUX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A second input element      #B the input Embedding size,d=3    #C The output embedding size ,d_out=2"
      ],
      "metadata": {
        "id": "G17ur5JBsSkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_2 = inputs[1]  #A\n",
        "d_in = inputs.shape[1] #B  #here dIn is of same dim of input tensor for multiplication of metrics\n",
        "d_out = 2   #C"
      ],
      "metadata": {
        "id": "CSUMMo6QsIvd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In got models input and output dimentions are same but here we are using diffeerent input-outputu dim to better understand"
      ],
      "metadata": {
        "id": "dK1HzzPUswK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's initalize weight metrics Wq,Wk,Wv"
      ],
      "metadata": {
        "id": "drYhAT2qtF-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_quer = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "W_val = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
      ],
      "metadata": {
        "id": "8JK8bypbsoIx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_quer)\n",
        "print(W_key)\n",
        "print(W_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zc7xcu5uAp5",
        "outputId": "de11be7f-a508-49cd-a3d0-e8fa6b6f255d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.9089, 0.4462],\n",
            "        [0.3106, 0.7836],\n",
            "        [0.0770, 0.3783]])\n",
            "Parameter containing:\n",
            "tensor([[0.1432, 0.4438],\n",
            "        [0.1797, 0.1969],\n",
            "        [0.3622, 0.8838]])\n",
            "Parameter containing:\n",
            "tensor([[0.0536, 0.6529],\n",
            "        [0.9413, 0.9484],\n",
            "        [0.1727, 0.1237]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute query,key and value  for 2nd input vector"
      ],
      "metadata": {
        "id": "deEl7CpmvI97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = X_2 @ W_quer\n",
        "key_2 = X_2 @ W_key\n",
        "value_2 = X_2 @W_val\n",
        "\n",
        "print(query_2)\n",
        "print(key_2)\n",
        "print(value_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGuKGqWbuB_U",
        "outputId": "e52e7148-4732-44a7-9f18-0412a9a0b246"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8210, 1.1768])\n",
            "tensor([0.4742, 0.9987])\n",
            "tensor([0.9624, 1.2658])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate all key and values for all input via matrix multiplication"
      ],
      "metadata": {
        "id": "0Bjerozlv9as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @W_key\n",
        "values = inputs @ W_val\n",
        "queries = inputs @ W_quer\n",
        "\n",
        "print(\"keys shape:\",keys.shape)\n",
        "print(\"values\",values.shape)\n",
        "print(\"queries\",queries.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqjnsY9HviuJ",
        "outputId": "1fee464e-7987-4483-f82d-57b23f31d4ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys shape: torch.Size([5, 2])\n",
            "values torch.Size([5, 2])\n",
            "queries torch.Size([5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have successfully rjected 6 input tokens from a 3d into a 2d embedding space"
      ],
      "metadata": {
        "id": "ubOt-1awweIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate attention scores for query 2"
      ],
      "metadata": {
        "id": "QHUgRXARxu8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @keys.T\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhacHf-AwSW4",
        "outputId": "735bbbaa-bcfa-4f69-86d1-9f34ae2a0207"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.5224, 1.5646, 1.5430, 0.8021, 1.0710])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate attention scores for all input tokens"
      ],
      "metadata": {
        "id": "Cldw1RkyzKxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = queries @keys.T"
      ],
      "metadata": {
        "id": "gOPl3QNrzIeT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(attn_scores)"
      ],
      "metadata": {
        "id": "qtaOsEHZzXHh",
        "outputId": "df313d55-8389-40da-b66e-112920b87c6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8585, 0.8851, 0.8729, 0.4544, 0.6073],\n",
            "        [1.5224, 1.5646, 1.5430, 0.8021, 1.0710],\n",
            "        [1.5123, 1.5552, 1.5338, 0.7975, 1.0651],\n",
            "        [0.8489, 0.8689, 0.8570, 0.4446, 0.5931],\n",
            "        [1.0014, 1.0156, 1.0020, 0.5175, 0.6885]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention weight is nothing just normalized attention scores,earlier we were normalizing it by using softmax function but we'll divide scores by square root of embd dim keys"
      ],
      "metadata": {
        "id": "2fq0ZoCOHG0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2  = torch.softmax(attn_scores_2/d_k**0.5,dim = -1)\n",
        "print(attn_weights_2)\n",
        "print(d_k)"
      ],
      "metadata": {
        "id": "ViS3bNcmzY4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd383bb-18ad-4783-bb1e-0ff654ce81c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2287, 0.2356, 0.2321, 0.1374, 0.1662])\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "why we are doing square root of values let's understand with example below"
      ],
      "metadata": {
        "id": "BDY_BTTEIO94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([0.1,-0.2,0.3,-0.2,0.5])\n",
        "softmax_result = torch.softmax(tensor,dim=-1)\n",
        "print(\"Softmax without scaing:\",softmax_result)\n",
        "\n",
        "\n",
        "#multiply tensor by  8 and then apply softmax\n",
        "scaled_tensor = tensor*8\n",
        "softmax_scaled_result = torch.softmax(scaled_tensor,dim=-1)\n",
        "print(\"Softmax after scaling (tensor*8):\",softmax_scaled_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odPFmJuiHtwp",
        "outputId": "a9c687bc-5856-4197-bef4-55d5413c70b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax without scaing: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
            "Softmax after scaling (tensor*16): tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see the max value get's the highest score after normalizing as we can see the 0.28 became 0.8 while oterhs are too low infact some are 15 times less so it's no good"
      ],
      "metadata": {
        "id": "xpE26qoiJSFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So dot preoduct between query and key becomes too large(example *8) the atention scores results in a very sharp softmax distribtion ,making model overly confident in one particular \"key\".SUch sharp disrtibution can make learning unstable"
      ],
      "metadata": {
        "id": "WMqfvylEJ2ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re root not"
      ],
      "metadata": {
        "id": "rMUPhnn8Ka4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why square root not just dimension** bbecause dot preoduct of Q and K increases variance because mulitplying two random number increases vairance. And the increase in variance grows with dimensions\n",
        "\n",
        "\n",
        "Diving by square root keeps variace close to 1"
      ],
      "metadata": {
        "id": "VtNtQcUXKZoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_variance(dim,num_trials=1000):\n",
        "  dot_products = []\n",
        "  scaled_dot_products = []\n",
        "\n",
        "  for _ in range(num_trials):\n",
        "    q=np.random.randn(dim)\n",
        "    k=np.random.randn(dim)\n",
        "\n",
        "    #compute dot product\n",
        "    dot_product = np.dot(q,k)\n",
        "    dot_products.append(dot_product)\n",
        "\n",
        "    #scale dot product by sqrt(dim)\n",
        "    scaled_dot_product = dot_product/np.sqrt(dim)\n",
        "    scaled_dot_products.append(scaled_dot_product)\n",
        "\n",
        "  variance = np.var(dot_products)\n",
        "  scaled_variance = np.var(scaled_dot_products)\n",
        "\n",
        "  return variance,scaled_variance\n",
        "\n",
        "\n",
        "#For dim 5\n",
        "variance_before_5 ,variance_after_5 = compute_variance(5)\n",
        "print(f\"Variance before scaling: {variance_before_5}\")\n",
        "print(f\"Variance after scaling: {variance_after_5}\")\n",
        "\n",
        "\n",
        "\n",
        "#For dim 5\n",
        "variance_before_30 ,variance_after_30 = compute_variance(30)\n",
        "print(f\"Variance before scaling: {variance_before_30}\")\n",
        "print(f\"Variance after scaling: {variance_after_30}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INPvlpwUJCXg",
        "outputId": "4afa9363-1f63-4aca-cef3-bbf6fd8b6f50"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance before scaling: 4.93239625379989\n",
            "Variance after scaling: 0.9864792507599779\n",
            "Variance before scaling: 27.899841693462722\n",
            "Variance after scaling: 0.929994723115424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can observe as dim increase variance increase but if we divide it with square root of dim it's always between 0-1"
      ],
      "metadata": {
        "id": "evIDp49RMdlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compute context vector"
      ],
      "metadata": {
        "id": "y_PZ9fz-N6lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights_2 @ values    #context vector for journey\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvtpGKvKMIDO",
        "outputId": "cee32e52-318d-429d-ff2b-6c4133c300ae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7437, 0.9557])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement compact self attention python class"
      ],
      "metadata": {
        "id": "E6zZW5lKOStL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "s1: multiply input with weight,key and value to get weight_key,weight_value,weight_query metrics\n",
        "\n",
        "\n",
        "s2: multiply queries with keys's transpose to get attention scores\n",
        "\n",
        "\n",
        "s3:scaled the attention scores by dividing it with square root of key's dimension than took softmax of it\n",
        "\n",
        "\n",
        "s4: multiply the attention weights withv alue metrics to get the context vector"
      ],
      "metadata": {
        "id": "5B79zIOdPSj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "  def __init__(self,d_in,d_out):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.W_key = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.W_value = nn.Parameter(torch.rand(d_in,d_out))\n",
        "\n",
        "  def forward(self,X):\n",
        "    keys = X @ self.W_key\n",
        "    values = X @ self.W_value\n",
        "    queries = X @ self.W_query\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim = -1)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "s3p_t5dTODOj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have inherited pytorch's nn.module to get all functionalities for model layer creation and management"
      ],
      "metadata": {
        "id": "Y68mgYZNRdPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sa_v1 = SelfAttention_v1(d_in,d_out)\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O6p4QmTRKmn",
        "outputId": "673d46c3-191b-479f-b14e-03a2030a2de7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9120, 0.9843],\n",
            "        [0.9215, 0.9910],\n",
            "        [0.9207, 0.9905],\n",
            "        [0.8847, 0.9668],\n",
            "        [0.8958, 0.9730]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "got context vector for our input"
      ],
      "metadata": {
        "id": "QTMUpGE3VrCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's improve this selfattention class by using pytorch's nn.Linear layers ,which effectively perform matrix multiplication when bias units are disabled"
      ],
      "metadata": {
        "id": "u8NYCozXV70e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.layer has optimized weight initialization scheme"
      ],
      "metadata": {
        "id": "9FLX13q0WVMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **nn.Linear layer automatically initializes its weights using a variant of Xavier/Glorot initialization (specifically designed for layers with activations like ReLU)**"
      ],
      "metadata": {
        "id": "OCBS3q0VYHqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "  def __init__(self,d_in,d_out,qkv_bias =False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
        "\n",
        "  def forward(self,X):\n",
        "    keys = self.W_key(X)\n",
        "    values = self.W_value(X)\n",
        "    queries =  self.W_query(X)\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim = -1)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "BEe7uadBUtQ-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sa_v2 = SelfAttention_v2(d_in,d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "j2a8M9VcWokd",
        "outputId": "50ece3c0-5958-4f00-995e-36838c282743"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'd_in' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-016dc9d03e2a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msa_v2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelfAttention_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'd_in' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "resue selfattention_v2"
      ],
      "metadata": {
        "id": "9LEoJviMrDQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim = 1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "GMOJQGTDXJpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll use  pytorch tril function to create lower triangle for attention we'll use mask metric and multiply it with our attnetion scores"
      ],
      "metadata": {
        "id": "Rkh5iQq0tkFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length,context_length))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "id": "YJ4XsIxltBdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "id": "j7T09afOtRvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now these weights need to be normalized so they sum upto 1"
      ],
      "metadata": {
        "id": "QkhxvaVau89d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim = 1,keepdim = True)\n",
        "masked_simple_norm = masked_simple/row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "id": "h94WpWw0uxIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "normailization is done but there is a problem of data leakage becuase we applied softmax earlier and it takes sum of all tokens so it already affected our attention socres so there is data leakage and no benefit of masking the further tokens"
      ],
      "metadata": {
        "id": "UukUlKZevtR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(attn_scores)"
      ],
      "metadata": {
        "id": "_Rj7l5EDvScg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length,context_length),diagonal =1)\n",
        "masked = attn_scores.masked_fill(mask.bool(),-torch.inf)\n",
        "\n",
        "print(masked)"
      ],
      "metadata": {
        "id": "i_f-640J1XDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked/keys.shape[-1]**0.5,dim = 1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "HwgNJ8Fc2TLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "everything done successfully no data leakage"
      ],
      "metadata": {
        "id": "9Ip4_hmx4GjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "just need to apply dorpout also   \n",
        "\n",
        "2 ways to apply dropou\n",
        "1. after calculating attentin scores(most common way)\n",
        "2. after applying attention weights to value vectors"
      ],
      "metadata": {
        "id": "Y0U_afAy4WF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = torch.nn.Dropout(0.5)\n",
        "example = torch.ones(6,6)\n",
        "\n",
        "print(\"No dorpout :\",example)\n",
        "print(\"Dropout: \",dropout(example))"
      ],
      "metadata": {
        "id": "LW3_F-6E3h5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "half(50%) are dorp out so to compensate for reduction in active elements the remained values are doubled"
      ],
      "metadata": {
        "id": "6yD3oc_751Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout in cnn and rnn ann is different in those neaurons are deactivated and the values are not scaled"
      ],
      "metadata": {
        "id": "OcqlLA4C7HLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,qkv_bias = False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal = 1))\n",
        "\n",
        "  def forward(self,X):\n",
        "    b,num_tokens ,d_in = X.shape\n",
        "    keys  = self.W_key(X)\n",
        "    queries = self.W_query(X)\n",
        "    values = self.W_value(X)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1,2)\n",
        "    attn_scores = attn_scores.masked_fill(self.mask.bool()[:num_tokens,:num_tokens],-torch.inf)   # :num_token\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim = -1)\n",
        "\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "IIs2S07a5Z5T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aK5tr0Xk-m4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs,inputs),dim = 0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "id": "2uXftCwV-noO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d_out,d_in,batch.shape[1])"
      ],
      "metadata": {
        "id": "OpNL8Mwj-BIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in,d_out,context_length ,0.0)\n",
        "context_vec = ca(batch)\n",
        "print(\"Context_vecs.shape:\",context_vec.shape)"
      ],
      "metadata": {
        "id": "52-FvM_K-tBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extending SIngle Head Attention To Multi-Head Attention**"
      ],
      "metadata": {
        "id": "IZhzLTe8EBr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "implementing multi-head attention involves creating multiple instances of self-attnetion mechanism,each with their own weights combining their outputs"
      ],
      "metadata": {
        "id": "2gKobYaXEP6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In code we can achieve this by implementing simple MultiheadAttentionWrapper class that stacks multiple instances of our previous CausalAttentionModule:"
      ],
      "metadata": {
        "id": "W_2FxoR6EgOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout,num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return torch.cat([h(X) for h in self.heads], dim=-1)\n"
      ],
      "metadata": {
        "id": "xObJipbo_K5V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "    [[0.43,0.15,0.89], #Your (X^1)\n",
        "     [0.55,0.87,0.66], #journey(x^2)\n",
        "     [0.57,0.85,0.64], #starts (X^3)\n",
        "     [0.22,0.58,0.33], #with (X^4)\n",
        "     [0.77,0.25,.10],  #one (X^5)\n",
        "     [0.05,0.80,0.55] #step (X^6)\n",
        "\n",
        "\n",
        "    ]\n",
        ")\n",
        "batch = torch.stack((inputs,inputs),dim = 0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roSX6sa-FwZz",
        "outputId": "2b7f04d5-a985-421e-d0b2-6f3640622596"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = batch.shape[1]\n",
        "d_in ,d_out =3,2\n",
        "mha = MultiHeadAttentionWrapper(d_in,d_out,context_length,0,num_heads=2)\n",
        "context_vec = mha(batch)\n",
        "print(context_vec)\n",
        "print(\"Context_vecs.shape\",context_vec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YblCVHyGFLw",
        "outputId": "659ba136-945d-429b-ba0d-118282c4b5f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0132, -0.2648,  0.2740, -0.4249],\n",
            "         [-0.1484, -0.2138,  0.3073, -0.5088],\n",
            "         [-0.1939, -0.1927,  0.3172, -0.5316],\n",
            "         [-0.1965, -0.1726,  0.2879, -0.4977],\n",
            "         [-0.1581, -0.1004,  0.2509, -0.3895],\n",
            "         [-0.1874, -0.1318,  0.2578, -0.4365]],\n",
            "\n",
            "        [[ 0.0132, -0.2648,  0.2740, -0.4249],\n",
            "         [-0.1484, -0.2138,  0.3073, -0.5088],\n",
            "         [-0.1939, -0.1927,  0.3172, -0.5316],\n",
            "         [-0.1965, -0.1726,  0.2879, -0.4977],\n",
            "         [-0.1581, -0.1004,  0.2509, -0.3895],\n",
            "         [-0.1874, -0.1318,  0.2578, -0.4365]]], grad_fn=<CatBackward0>)\n",
            "Context_vecs.shape torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First dim of context vector is 2 since we have two input texts and second dim refers to 6 tokens in each input. The 3rd dim refers to  dimension bevause we had d_out(output dim =2) and 2 heads so 2*2=4"
      ],
      "metadata": {
        "id": "d4fvBMs4H_6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias =False):\n",
        "    super().__init__()\n",
        "    assert (d_out%num_heads==0),\\\n",
        "      \"d_out must be divisibe by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out//num_heads    #reduce projection dim to match desired output dim\n",
        "\n",
        "    self.W_query = nn.Linear(d_in,d_out,bias =qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in,d_out,bias =qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out,d_out)    #Linear layer to combine head outputs\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal = 1))   #register is used to transfer computation to cpu or gpu which is best\n",
        "\n",
        "  def forward(self,X):\n",
        "    b,num_tokens ,d_in = X.shape\n",
        "\n",
        "    keys  = self.W_key(X)   #shape: (batch ,num_tokens,id_out)\n",
        "    queries = self.W_query(X)\n",
        "    values = self.W_value(X)\n",
        "\n",
        "    #there is no head_dim so we need to create 4th dim for head\n",
        "    #unroll last dimension of keys,queries and values to include num_heads and head_dim\n",
        "    #head_dim = d_out/num_heads =6/2 = 3\n",
        "    #(batch,num_tokens,d_out) -> (batch,num_tokens,num_heads,head_dim)\n",
        "    #(1,2,6) -> (1,3,2,3)\n",
        "    keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "    #transpose to get shape (batch,num_heads,num_tokens,head_dim) ->(batch,num_heads,num_tokens,head_dim)\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    #calculate scaled dot-product attention with causal mask\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "\n",
        "    #orignal mask truncated to the number of tokens and converted to boolean\n",
        "    mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
        "\n",
        "    #use mask to fill attn scores\n",
        "    attn_scores.masked_fill(mask_bool,-torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores/self.head_dim**0.5,dim = -1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    #shape:(b,num_tokens,num_ehads,head_dim)\n",
        "    context_vec = (attn_weights @values).transpose(1,2)\n",
        "\n",
        "    #combine heads,where self.d_out = self.num_heads *self.head_dim\n",
        "    context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
        "    context_vec = self.out_proj(context_vec) #optional projection\n",
        "\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "ycw9RbJbGlYv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test his mha\n",
        "import torch\n",
        "\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43,0.15,0.89,0.55,0.87,0.66],\n",
        "     [0.57,0.85,0.64,0.22,0.58,0.33],\n",
        "     [0.77,0.25,0.10,0.05,0.80,0.55]]\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size ,context_length ,d_in = batch.shape\n",
        "\n",
        "d_out = 6\n",
        "\n",
        "mha = MultiHeadAttention(d_in,d_out,context_length,0,num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"Context_vecs.shape\",context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20KjxmfgccNg",
        "outputId": "5f55527f-701a-49f4-ce0a-9d29940c5b1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[-0.2734, -0.2139, -0.0916,  0.4742, -0.3268, -0.0979],\n",
            "         [-0.2725, -0.2143, -0.0922,  0.4737, -0.3265, -0.0992],\n",
            "         [-0.2742, -0.2132, -0.0918,  0.4752, -0.3260, -0.0984]],\n",
            "\n",
            "        [[-0.2734, -0.2139, -0.0916,  0.4742, -0.3268, -0.0979],\n",
            "         [-0.2725, -0.2143, -0.0922,  0.4737, -0.3265, -0.0992],\n",
            "         [-0.2742, -0.2132, -0.0918,  0.4752, -0.3260, -0.0984]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "Context_vecs.shape torch.Size([2, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Successfully implemented mha"
      ],
      "metadata": {
        "id": "f32QC1H_pntg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
      ],
      "metadata": {
        "id": "sb1csGc-prpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_config_124 = {\n",
        "    \"vocab_size\":50257,\n",
        "    \"context_lengh\":1024,\n",
        "    \"emb_dim\":768,\n",
        "    \"n_heads\":12,\n",
        "    \"n_layers\":12,\n",
        "    \"drop_rate\":0.1,\n",
        "    \"qkv_bias\":False\n",
        "}"
      ],
      "metadata": {
        "id": "sFKeIbSSimsi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT ARCHITECTURE PART1:DUMMY GPT MODEL CLASS"
      ],
      "metadata": {
        "id": "ns43oqEJqJGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ST1: USE placeholder for transformerBlock\n",
        "st2: Use a placeholder for layernorm"
      ],
      "metadata": {
        "id": "syzLBC_iqQ9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DummyGptModel(nn.Module):\n",
        "  def __init__(self,cfg ):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_lengh\"],cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "\n",
        "    #use a placeholder for TransformerBlock\n",
        "    self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "    #use a placeholder for LayerNorm\n",
        "    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias = False\n",
        "    )\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    batch_size ,seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len,device =in_idx.device))\n",
        "    X = tok_embeds +pos_embeds\n",
        "    X = self.drop_emb(X)\n",
        "    X = self.trf_blocks(X)\n",
        "    X = self.final_norm(X)\n",
        "    logits = self.out_head(X)\n",
        "    return logits\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    #A simple placeholder\n",
        "\n",
        "\n",
        "  def forward(self,X):\n",
        "    return X\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,X):\n",
        "    return X"
      ],
      "metadata": {
        "id": "9BaTjoanqG91"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTE3bsE-qbH2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7yxa2wD2rM7",
        "outputId": "00228fe7-bb56-473e-91e7-dee94df045d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP1: TOKENIZATION**"
      ],
      "metadata": {
        "id": "7L1XNNuJ2EpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "\n",
        "batch = torch.stack(batch)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AovNL5JJ2Iy4",
        "outputId": "0d882ec2-47d0-45ba-a3a9-a9e263b74e04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATE AN INSTANCE OF DUMMYGPTMODEL**"
      ],
      "metadata": {
        "id": "A7pJZEKS2wtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gGkH76ye3fWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DummyGptModel(GPT_config_124)\n",
        "logits = model(batch)\n",
        "print(\"Output shape:\",logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alet01oi2lX9",
        "outputId": "37e2e9f5-9386-4768-d043-7097700ee34e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 4, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT ARCHITECTURE PART2: LAYER NORMALIZATION"
      ],
      "metadata": {
        "id": "XJWYoXc0QolJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example input batch\n",
        "batch_example = torch.randn(2, 5)\n",
        "\n",
        "# Define the sequential model with a Linear layer followed by a ReLU activation\n",
        "layer = nn.Sequential(\n",
        "    nn.Linear(5, 6),  # Linear layer\n",
        "    nn.ReLU()         # ReLU activation (correct usage)\n",
        ")\n",
        "# Pass the batch example through the model\n",
        "out = layer(batch_example)\n",
        "\n",
        "# Print the output\n",
        "print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHjThKqU3D7V",
        "outputId": "526a3f39-fb84-491c-8901-7033267cc37f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0000, 0.9118, 0.0000, 0.0000, 0.6671, 0.0710],\n",
            "        [0.0000, 0.0000, 1.5023, 0.0000, 0.0000, 0.7273]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = out.mean(dim = -1,keepdim = True)   #dim =-1 because we have do along columns\n",
        "var = out.var(dim = -1,keepdim = True)\n",
        "print(\"Mean:\",mean)\n",
        "print(\"Variance:\",var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITi3SBVRQ6Md",
        "outputId": "e54b26ae-7cbb-49ae-edb9-b6860917b0e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: tensor([[0.2750],\n",
            "        [0.3716]], grad_fn=<MeanBackward1>)\n",
            "Variance: tensor([[0.1656],\n",
            "        [0.3915]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_norm = (out-mean)/torch.sqrt(var)\n",
        "mean = out_norm.mean(dim = -1,keepdim = True)\n",
        "var = out_norm.var(dim = -1,keepdim= True)\n",
        "print(\"Normalized layer outputs:\\n\",out_norm)\n",
        "\n",
        "print(\"mean:\\n\",mean)\n",
        "print(\"Variance :\\n\",var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlKsrFH0TbYt",
        "outputId": "3b7cf632-e423-4459-ccf4-ed2afa48873d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized layer outputs:\n",
            " tensor([[-0.6758,  1.5651, -0.6758, -0.6758,  0.9637, -0.5014],\n",
            "        [-0.5939, -0.5939,  1.8071, -0.5939, -0.5939,  0.5685]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "mean:\n",
            " tensor([[-5.9605e-08],\n",
            "        [-9.9341e-09]], grad_fn=<MeanBackward1>)\n",
            "Variance :\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have implemented layernormalization now let's create a class for it"
      ],
      "metadata": {
        "id": "T34No5FdWh3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self,X):\n",
        "    mean = X.mean(dim = -1,keepdim = True)\n",
        "    var = X.var(dim = -1,keepdim =True,unbiased = False)\n",
        "    norm_X = (X-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*norm_X + self.shift"
      ],
      "metadata": {
        "id": "Tdu-sXVdUoJl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "epsiloion is added to avoid zero division error"
      ],
      "metadata": {
        "id": "UATbEduTdnF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNorm(emb_dim = 5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim = -1,keepdim = True)\n",
        "var = out_ln.var(dim =-1,unbiased = False,keepdim = True)\n",
        "print(\"Mean:\\n\",mean)\n",
        "print(\"Variance:\\n\",var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-vYSATZVQdS",
        "outputId": "db3f3174-22e0-44f0-d987-ab62b463e6f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[4.7684e-08],\n",
            "        [4.7684e-08]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch normalization -normalizes around batch\n",
        "\n",
        "\n",
        "layer normalization normalizes around -layer\n",
        "\n",
        "\n",
        "cnn-batch   rnn,transformer->layer norm"
      ],
      "metadata": {
        "id": "6rn6q3F_hZ87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For llms gelu and swiglu is used**"
      ],
      "metadata": {
        "id": "81cw94aFmjf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,X):\n",
        "    return 0.5*X*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(X+0.044715*torch.pow(X,3))))"
      ],
      "metadata": {
        "id": "y4NXNWdgep51"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare relu and gelu to see the difference between these"
      ],
      "metadata": {
        "id": "WDvOpwfxnbKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Create GELU and ReLU instances\n",
        "gelu, relu = GELU(), nn.ReLU()\n",
        "\n",
        "# Generate input tensor\n",
        "X = torch.linspace(-3, 3, 100)\n",
        "\n",
        "# Apply activations\n",
        "y_gelu, y_relu = gelu(X), relu(X)\n",
        "\n",
        "# Plot both activations\n",
        "plt.figure(figsize=(6, 8))\n",
        "for i, (y, label, color) in enumerate([(y_gelu, \"GELU\", 'blue'), (y_relu, \"ReLU\", 'red')]):\n",
        "    plt.subplot(2, 1, i+1)\n",
        "    plt.plot(X.numpy(), y.numpy(), label=label, color=color)\n",
        "    plt.title(f\"{label} Activation Function\")\n",
        "    plt.xlabel(\"Input\")\n",
        "    plt.ylabel(\"Output\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "IdCSiQP9nZaa",
        "outputId": "c5f502f7-c22c-49f6-c67b-da8c58a8a5df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x800 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAMWCAYAAAAH8wnbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACf2ElEQVR4nOzdd3gUddfG8e+mJ0AoAgklEHqRXgWVolQRDAIC6gOoWJCgCKKCSlVQFLGgYAVFIkjHghJR5OEBCygqKChKEwg9CSSQLNl5/5g3C0sS2CzZzCa5P9c1V2an7dmTxRxnfsVmGIaBiIiIiFyWn9UBiIiIiBQUKpxERERE3KTCSURERMRNKpxERERE3KTCSURERMRNKpxERERE3KTCSURERMRNKpxERERE3KTCSURERMRNKpxExOdMnDgRm81myXvPmzcPm83Gnj17LHn/gmbIkCFER0dbHYZIvlHhJJKPdu/eTWxsLLVr1yYsLIywsDDq16/P8OHD+fXXX12OzSwecloSEhIA2LNnDzabjRdffDHH942Ojubmm2/Odt/mzZux2WzMmzfP7c/x+eefY7PZqFixIg6Hw+3zLpSamsrEiRNZt26dR+dfqalTp7JixQpL3jsn0dHROf6+z549a1lcBw8eZOLEiWzdutWyGER8RYDVAYgUFZ9++in9+/cnICCAO+64g8aNG+Pn58eOHTtYtmwZs2fPZvfu3VStWtXlvNmzZ1O8ePEs1ytVqlQ+RZ7VggULiI6OZs+ePXz99dd06tQp19dITU1l0qRJAHTo0MFl31NPPcUTTzyRF6HmaOrUqfTt25eYmBiX7f/5z38YMGAAwcHBXn3/nDRp0oTRo0dn2R4UFGRBNKaDBw8yadIkoqOjadKkicu+t99+2+PiWaQgUuEkkg/+/vtvBgwYQNWqVVm7di0VKlRw2f/888/zxhtv4OeX9SZw3759KVu2bH6FelkpKSmsXLmSadOmMXfuXBYsWOBR4XQpAQEBBARY858nf39//P39LXlvgEqVKnHnnXda9v65FRgYaHUIIvlKj+pE8sH06dNJSUlh7ty5WYomMAuFhx56iKioKAuiy53ly5dz5swZ+vXrx4ABA1i2bFm2j5HOnj3LxIkTqV27NiEhIVSoUIFbb72Vv//+mz179lCuXDkAJk2a5HwcNXHiRCBrG6cGDRrQsWPHLO/hcDioVKkSffv2dW578cUXadu2LVdddRWhoaE0b96cJUuWuJxns9lISUnh/fffd773kCFDgJzbOL3xxhtcffXVBAcHU7FiRYYPH05iYqLLMR06dKBBgwb8/vvvdOzYkbCwMCpVqsT06dPdTe8l5dT2K7uYMx/PbtiwgVatWhESEkL16tX54IMPspyfmJjII488QnR0NMHBwVSuXJlBgwZx7Ngx1q1bR8uWLQG46667nPnKfLSbXRunlJQURo8eTVRUFMHBwdSpU4cXX3wRwzBcjrPZbMTGxrJixQoaNGhAcHAwV199NV988cWVJUrEi1Q4ieSDTz/9lJo1a9K6detcn3vixAmOHTvmslz8Bzs/LViwgI4dOxIZGcmAAQM4deoUn3zyicsxGRkZ3HzzzUyaNInmzZszY8YMHn74YZKSkti2bRvlypVj9uzZAPTu3Zv58+czf/58br311mzfs3///qxfv97ZrivThg0bOHjwIAMGDHBue+WVV2jatCmTJ09m6tSpBAQE0K9fPz777DPnMfPnzyc4OJjrr7/e+d73339/jp954sSJDB8+nIoVKzJjxgz69OnDm2++SZcuXbDb7S7Hnjx5km7dutG4cWNmzJhB3bp1efzxx1m9erVb+bXb7Vl+36mpqW6de7Fdu3bRt29fOnfuzIwZMyhdujRDhgxh+/btzmNOnz7N9ddfz2uvvUaXLl145ZVXeOCBB9ixYwf//vsv9erVY/LkyQDcd999zny1a9cu2/c0DINevXoxc+ZMunXrxksvvUSdOnUYM2YMo0aNynL8hg0bePDBBxkwYADTp0/n7Nmz9OnTh+PHj3v0mUW8zhARr0pKSjIAIyYmJsu+kydPGkePHnUuqampzn0TJkwwgGyXOnXqOI/bvXu3ARgvvPBCjjFUrVrV6NGjR7b7fvzxRwMw5s6de9nPcvjwYSMgIMB4++23ndvatm1r3HLLLS7HvffeewZgvPTSS1mu4XA4DMMwjKNHjxqAMWHChCzHZH72TDt37jQA47XXXnM57sEHHzSKFy/ukrcL1w3DMNLT040GDRoYN9xwg8v2YsWKGYMHD87y3nPnzjUAY/fu3YZhGMaRI0eMoKAgo0uXLkZGRobzuFmzZhmA8d577zm3tW/f3gCMDz74wLktLS3NiIyMNPr06ZPlvS5WtWrVbH/fmTm6OC85xXzhtdavX+/cduTIESM4ONgYPXq0c9v48eMNwFi2bFmW62b+ri71HRk8eLBRtWpV5+sVK1YYgPHMM8+4HNe3b1/DZrMZu3btcm4DjKCgIJdtv/zyS7a/axFfoTtOIl6WnJwMkG0D7w4dOlCuXDnn8vrrr2c5ZunSpcTHx7ssc+fO9Xrc2Vm4cCF+fn706dPHuW3gwIGsXr2akydPOrctXbqUsmXLMmLEiCzX8GSYgdq1a9OkSRMWLVrk3JaRkcGSJUvo2bMnoaGhzu0Xrp88eZKkpCSuv/56fvrpp1y/L8BXX31Feno6I0eOdGmDdu+99xIeHu5yJwvM3/OFbZSCgoJo1aoV//zzj1vv17p16yy/70GDBnkUe/369bn++uudr8uVK0edOnVcYlm6dCmNGzemd+/eWc735Hf1+eef4+/vz0MPPeSyffTo0RiGkeXOW6dOnahRo4bzdaNGjQgPD3c7XyL5TY3DRbysRIkSgPlI5GJvvvkmp06d4vDhwzk2CG7Xrl2+NA5354/khx9+SKtWrTh+/LjzUUrTpk1JT09n8eLF3HfffYDZGL5OnTp52sC7f//+jBs3jgMHDlCpUiXWrVvHkSNH6N+/v8txn376Kc888wxbt24lLS0tV58vO3v37gWgTp06LtuDgoKoXr26c3+mypUrZ3mv0qVLZxluIidly5bNs8b2VapUybKtdOnSLkXu33//7VIIX6m9e/dSsWJF5/c+U7169Zz7cxujiC/RHScRLytZsiQVKlRg27ZtWfa1bt2aTp06ce2113o1hpCQEM6cOZPtvsz2MyEhIZe8xl9//cWPP/7Ihg0bqFWrlnO57rrrALPtkzf1798fwzBYvHgxAB9//DElS5akW7duzmP++9//0qtXL0JCQnjjjTf4/PPPiY+P5/bbb8/SMNlbcuqRlxfvn1Pxl5GRke+x5JWCEKPIhXTHSSQf9OjRg3feeYcffviBVq1a5fv7V61ald9//z3bfTt37nQecykLFiwgMDCQ+fPnZ/ljt2HDBl599VX27dtHlSpVqFGjBt9//z12uz3H7uq5vQNUrVo1WrVqxaJFi4iNjWXZsmXExMS4jLe0dOlSQkJC+PLLL122Z/do0933z8zLzp07qV69unN7eno6u3fvzvOhGC6ldOnSgNkL7sJxvC6+i5MbNWrUyLaov1BufldVq1blq6++4tSpUy53nXbs2OHcL1KQ6Y6TSD547LHHCAsL4+677+bw4cNZ9nv7/65vuukm/v333ywjZaelpfHOO+9Qvnx5mjVrdslrLFiwgOuvv57+/fvTt29fl2XMmDEAfPTRRwD06dOHY8eOMWvWrCzXyfysYWFhALnqIdi/f3++++473nvvPY4dO5blMZ2/vz82m83lDsyePXuyHSG8WLFibr13p06dCAoK4tVXX3X5Pb377rskJSXRo0cPt+O/UpltgdavX+/cljmsgqf69OnDL7/8wvLly7Psy/y8xYoVA9z7Xd10001kZGRk+d3PnDkTm81G9+7dPY5VxBfojpNIPqhVqxZxcXEMHDiQOnXqOEcONwyD3bt3ExcXh5+fH5UrV85y7pIlS7JtWN65c2ciIiKcr9euXZvteEoxMTHcd999vPfee/Tr14+7776bpk2bcvz4cRYtWsS2bdv44IMPLjky9ffff8+uXbuIjY3Ndn+lSpVo1qwZCxYs4PHHH2fQoEF88MEHjBo1ih9++IHrr7+elJQUvvrqKx588EFuueUWQkNDqV+/PosWLaJ27dqUKVOGBg0a0KBBgxzjuO2223j00Ud59NFHKVOmTJa7PT169OCll16iW7du3H777Rw5coTXX3+dmjVrZmlj1Lx5c7766iteeuklKlasSLVq1bIdLqJcuXKMHTuWSZMm0a1bN3r16sXOnTt54403aNmyZb4OVtmlSxeqVKnCPffcw5gxY/D39+e9996jXLly7Nu3z6NrjhkzhiVLlji/G82bN+fEiROsWrWKOXPm0LhxY2rUqEGpUqWYM2cOJUqUoFixYrRu3Zpq1apluV7Pnj3p2LEjTz75JHv27KFx48asWbOGlStXMnLkSJeG4CIFkkW9+USKpF27dhnDhg0zatasaYSEhBihoaFG3bp1jQceeMDYunWry7GXGo4AML755hvDMM4PR5DTMn/+fMMwzKEPHnnkEaNatWpGYGCgER4ebnTs2NFYvXr1ZeMeMWKEARh///13jsdMnDjRAIxffvnFMAxzWIAnn3zS+X6RkZFG3759Xa6xceNGo3nz5kZQUJBb3e4NwzCuvfZaAzCGDh2a7f53333XqFWrlhEcHGzUrVvXmDt3brbX27Fjh9GuXTsjNDTUAJxDE2TXtd8wzOEH6tatawQGBhoRERHGsGHDjJMnT7oc0759e+Pqq6/OEtPFXfZzcqlhIzJt2bLFaN26tREUFGRUqVLFeOmll3IcjiC7a7Vv395o3769y7bjx48bsbGxRqVKlYygoCCjcuXKxuDBg41jx445j1m5cqVRv359IyAgwGVoguw+26lTp4xHHnnEqFixohEYGGjUqlXLeOGFF5zDG2QCjOHDh2ebh+yGihDxBTbDUAs8EREREXeojZOIiIiIm1Q4iYiIiLhJhZOIiIiIm1Q4iYiIiLhJhZOIiIiIm1Q4iYiIiLipyA2A6XA4OHjwICVKlPB40k8REREpPAzD4NSpU1SsWBE/v0vfUypyhdPBgweJioqyOgwRERHxMfv37892BocLFbnCKXPSyf379xMeHp7n17fb7axZs4YuXbrkOLmpZKW8eU6585xy5xnlzXPKnWe8nbfk5GSioqJcJqbOSZErnDIfz4WHh3utcAoLCyM8PFz/KHJBefOccuc55c4zypvnlDvP5Ffe3GnCo8bhIiIiIm5S4SQiIiLiJhVOIiIiIm4qcm2c3JWRkYHdbs/1eXa7nYCAAM6ePUtGRoYXIit4AgMD8ff3tzoMERGRK6bC6SKGYZCQkEBiYqLH50dGRrJ//36NE3WBUqVKERkZqZyIiEiBpsLpIplFU/ny5QkLC8v1H3qHw8Hp06cpXrz4ZQfRKgoMwyA1NZUjR44AUKFCBYsjEhER8ZwKpwtkZGQ4i6arrrrKo2s4HA7S09MJCQlR4fT/QkNDAThy5Ajly5fXYzsRESmwLP3LPnv2bBo1auQcU6lNmzasXr36kucsXryYunXrEhISQsOGDfn888/zLJ7MNk1hYWF5dk0xZebUk3ZjIiIivsLSwqly5co899xzbNmyhc2bN3PDDTdwyy23sH379myP37hxIwMHDuSee+7h559/JiYmhpiYGLZt25ancakdTt5TTkVEpDCwtHDq2bMnN910E7Vq1aJ27do8++yzFC9enO+++y7b41955RW6devGmDFjqFevHlOmTKFZs2bMmjUrnyMXERGRoshn2jhlZGSwePFiUlJSaNOmTbbHbNq0iVGjRrls69q1KytWrMjxumlpaaSlpTlfJycnA+Yjo4sfG9ntdgzDwOFw4HA4PPochmE4f3p6jcLI4XBgGAZ2uz3bNk6Zvws9yss95c5zyp1nlDfPKXee2bbtHM8805qmTe14o49Rbn4flhdOv/32G23atOHs2bMUL16c5cuXU79+/WyPTUhIICIiwmVbREQECQkJOV5/2rRpTJo0Kcv2NWvWZGnLFBAQQGRkJKdPnyY9Pd2DT3PeqVOnruh8Txw+fJiZM2eyZs0aDh48SHh4ONWqVeO2225j4MCBhIWF0ahRI/bv35/l3PHjx/PII4+wb98+GjduzPr162nYsKHLMRs2bKBnz57s2bOHkiVLuuxr1KgRw4YNY9iwYdnGlp6ezpkzZ1i/fj3nzp3L8TPEx8d78MkFlLsrodx5RnnznHLnvsTEIB5/vB2HD0fyn//sZ9Son/L8PVJTU90+1vLCqU6dOmzdupWkpCSWLFnC4MGD+fbbb3MsnnJr7NixLnepMmdA7tKlS5ZJfs+ePcv+/fspXrw4ISEhHr2fYRicOnWKEiVK5Gu7nn/++YcOHTpQqlQppk6dSsOGDQkODua3337j7bffpkaNGvTq1Qs/Pz8mTZrE0KFDXc4vUaIExYoVo3jx4gAUK1YsS34yC80SJUpk2efn50dISEiOEyefPXuW0NBQ2rVrl21u7XY78fHxdO7cWRNf5pJy5znlzjPKm+eUu9xJTYUuXfw5fNiPiIgUPvjgKipWvCnP3yfzaZQ7LC+cgoKCqFmzJgDNmzfnxx9/5JVXXuHNN9/McmxkZCSHDx922Xb48GEiIyNzvH5wcDDBwcFZtgcGBmb50mZkZGCz2fDz8/N4KIHMx3OZ18kvsbGxBAQEsHnzZooVK+bcXrNmTXr37o1hGM5CLjw8nIoVK2Z7ncyYs8vBpfbBpT+zn58fNpst27xf6HL7JWfKneeUO88ob55T7i4vIwPuugt++AHKlDEYP34TFSu290recnNNywunizkcDpc2SRdq06YNa9euZeTIkc5t8fHxObaJyguGYVa87nI4ICUF/P3hSuumsDBw56bV8ePHWbNmDVOnTnUpmi6kXm0iIlKQPPYYLF8OQUGwZEkGyckpVocEWFw4jR07lu7du1OlShVOnTpFXFwc69at48svvwRg0KBBVKpUiWnTpgHw8MMP0759e2bMmEGPHj1YuHAhmzdv5q233vJajKmp8P9Pr9zkB5TKk/c+fRpyqINc7Nq1C8MwqFOnjsv2smXLcvbsWQCGDx/O888/D8Djjz/OU0895XLs6tWruf766/MkbhERkSsxaxa89JK5Pm8eXHedQR4O23hFLC2cjhw5wqBBgzh06BAlS5akUaNGfPnll3Tu3BmAffv2uTz6adu2LXFxcTz11FOMGzeOWrVqsWLFCho0aGDVR/BpP/zwAw6HgzvuuMPlLt6YMWMYMmSIy7GVKlXK5+hERESy+vRTePhhc33qVBg4EHypE6KlhdO77757yf3r1q3Lsq1fv37069fPSxFlFRZm3vlxl8PhIDk5mfDw8Ctu4+TuAOY1a9bEZrOxc+dOl+3Vq1cHzk95kqls2bLOdmW5kdnwOykpiVKlSrnsS0xMzNLTTkREJDe2bIH+/c1mL0OHwhNPWB1RVj7XxsnX2GzuPS7L5HCYDdqKFbvyNk7uuuqqq+jcuTOzZs1ixIgRObZzulK1atXCz8+PLVu2ULVqVef2f/75h6SkJGrXru2V9xURkcJv3z64+ebMnnTwxhvutfPNbyqcCok33niDa6+9lhYtWjBx4kQaNWqEn58fP/74Izt27KB58+bOY0+dOpVl7KuwsDCXoQQuvnsFcPXVVzN06FBGjx5NQEAADRs2ZP/+/Tz++ONcc801tG3b1nsfUERECq2kJOjRAxISoEED+Phj8NVOhyqcCokaNWrw888/M3XqVMaOHcu///5LcHAw9evX59FHH+XBBx90Hjt+/HjGjx/vcv7999/PnDlznK8HDBiQ5T3279/PK6+8wnPPPcfjjz/O3r17iYyMpHPnzjz77LPquSciIrlmt0PfvrBtG1SoAJ99Br7c8kOFUyFSoUIFXnvtNV577bUcj9mzZ88lrxEdHe2cNiYnEydOZOLEiR5EKCIicp5hwLBh8NVXZhOXTz+FKlWsjurSLJ3kV0RERIqu556Dd9812wQvXAjNmlkd0eWpcBIREZF8t3AhjBtnrr/yitkwvCBQ4SQiIiL56n//g8zhBEeOhNhYK6PJHRVOIiIikm927YJbboG0NPPniy9aHVHuqHASERGRfHH8ONx0k/mzRQtYsMCc27UgUeGUDYfDYXUIhY5yKiJStKWlQe/e8NdfULUqfPJJ7gaY9hUajuACQUFB+Pn5cfDgQcqVK0dQUFCuxyZyOBykp6dz9uzZK55ypTAwDIP09HSOHj2Kn58fQUFBVockIiL5zDDg7rvhv/+F8HBzrKbISKuj8owKpwv4+flRrVo1Dh06xMGDBz26hmEYnDlzhtDQUA0IeYGwsDCqVKmiYlJEpAiaMAHi4iAgAJYuhauvtjoiz6lwukhQUBBVqlTh3LlzZGRk5Pp8u93O+vXradeuHYG+Ol58PvP39ycgIECFpIhIEfT++zBlirk+Zw506mRtPFdKhVM2bDYbgYGBHhU+/v7+nDt3jpCQEBVOIiJSpH3zDdx7r7k+dizcc4+18eQFPTcRERGRPLdjB9x6qzkXXf/+8MwzVkeUN1Q4iYiISJ46csQcdiAxEdq2hXnzzGlVCoNC8jFERETEF5w5AzExsHs3VK8OK1ZASIjVUeUdFU4iIiKSJxwOcyqVTZugdGn4/HMoV87qqPKWCicRERHJE08/DR9/DIGBsGwZ1KljdUR5T4WTiIiIXLH33oOpU831t9+GDh0sDcdrVDiJiIjIFfn6a7j/fnP96adh8GBr4/EmFU4iIiLisT/+MIcdOHcOBg6ESZOsjsi7VDiJiIiIR44cgR49ICkJrr3WfFxX2CeJUOEkIiIiuXb2rOuwA8uXF65hB3KiwklERERyxeGAu+4yhx0oVQo++6zwDTuQExVOIiIikisTJsDChRAQYA47ULeu1RHlHxVOIiIi4rYPPjg/79xbb0HHjtbGk99UOImIiIhb1q+HoUPN9bFjzcd1RY0KJxEREbmsv/6C3r3Bbod+/c7fdSpqVDiJiIjIJZ04YQ47cOIEtG4N778PfkW0giiiH1tERETckZ5uDnD5119QtSqsXAmhoVZHZR0VTiIiIpItw4D77oNvv4USJeDTTyEiwuqorKXCSURERLL13HPnH8t9/DE0aGB1RNaztHCaNm0aLVu2pESJEpQvX56YmBh27tx5yXPmzZuHzWZzWUKKwlClIiIi+WjxYhg3zlx/7TXo1s3aeHyFpYXTt99+y/Dhw/nuu++Ij4/HbrfTpUsXUlJSLnleeHg4hw4dci579+7Np4hFREQKvx9+gEGDzPWHHoIHH7Q2Hl8SYOWbf/HFFy6v582bR/ny5dmyZQvt2rXL8TybzUZkZKS3wxMRESly9u2DXr3MuehuugleesnqiHyLT7VxSkpKAqBMmTKXPO706dNUrVqVqKgobrnlFrZv354f4YmIiBRqp07BzTfD4cPQsKE5rYq/v9VR+RZL7zhdyOFwMHLkSK699loaXKL1WZ06dXjvvfdo1KgRSUlJvPjii7Rt25bt27dTuXLlLMenpaWRlpbmfJ2cnAyA3W7Hbrfn+efIvKY3rl2YKW+eU+48p9x5RnnznC/nLiMDBgzw57ff/IiIMFi+/BwhIeaAl1bzdt5yc12bYRiGV6LIpWHDhrF69Wo2bNiQbQGUE7vdTr169Rg4cCBTpkzJsn/ixIlMmjQpy/a4uDjCwsKuKGYREZHC4t13G/DJJzUICsrgmWc2ULt2otUh5ZvU1FRuv/12kpKSCA8Pv+SxPlE4xcbGsnLlStavX0+1atVyfX6/fv0ICAjgo48+yrIvuztOUVFRHDt27LLJ8YTdbic+Pp7OnTsTGBiY59cvrJQ3zyl3nlPuPKO8ec5Xc/fWW37ExprP5OLiztG3r+WlgQtv5y05OZmyZcu6VThZ+qjOMAxGjBjB8uXLWbdunUdFU0ZGBr/99hs33XRTtvuDg4MJDg7Osj0wMNCrX1pvX7+wUt48p9x5TrnzjPLmOV/KXXw8PPywuf7MMzBwoM+04snCW3nLzTUtzc7w4cOJi4tj5cqVlChRgoSEBABKlixJ6P+P5z5o0CAqVarEtGnTAJg8eTLXXHMNNWvWJDExkRdeeIG9e/cyNHO6ZhEREXHLH3+YE/ZmZMCdd54ft0lyZmnhNHv2bAA6dOjgsn3u3LkMGTIEgH379uF3wUyCJ0+e5N577yUhIYHSpUvTvHlzNm7cSP369fMrbBERkQLv2DGzB11SElx7LbzzDthsVkfl+yx/VHc569atc3k9c+ZMZs6c6aWIRERECr+0NHPi3n/+gWrVYPlyyKZVi2TDp8ZxEhEREe8yDHjgAfjvfyE83Jy4t1w5q6MqOFQ4iYiIFCEvvADz5p2fuFctXXJHhZOIiEgRsWIFPPGEuf7KK9C1q6XhFEgqnERERIqArVvhjjvMR3UPPgixsVZHVDCpcBIRESnkDh2Cnj0hNRU6dzbvNolnVDiJiIgUYmfOQEwM/Psv1K1rtmsK8N0xLn2eCicREZFCyjDg7rvhhx+gTBn45BMoVcrqqAo2FU4iIiKF1JQpsHCheYdp6VKoWdPqiAo+FU4iIiKF0Mcfw4QJ5vqcOXDRJB3iIRVOIiIihcyPP8Lgweb6qFFwzz3WxlOYqHASEREpRA4cgFtugbNn4aabYPp0qyMqXFQ4iYiIFBKpqWbRdOgQXH01fPQR+PtbHVXhosJJRESkEHA4YMgQ2LIFypY1e9CFh1sdVeGjwklERKQQmDwZFi+GwEBYtgyqVbM6osJJhZOIiEgB9/HHMGmSuT5nDlx/vbXxFGYqnERERAqwzZtde9Ddfbe18RR2KpxEREQKqIMH1YMuv6lwEhERKYAy56A7eBDq1YO4OPWgyw8qnERERAoYw4ChQ82BLjPnoCtZ0uqoigYVTiIiIgXMc8+Zd5gCAmDJEqhRw+qIig4VTiIiIgXIypUwbpy5PmsWdOxobTxFjQonERGRAuK33+COO8z12Fi4/35r4ymKVDiJiIgUAEePQs+ekJICN94IM2daHVHRpMJJRETEx6WnQ9++sHcv1KxpDngZEGB1VEWTCicREREfZhgwYgSsX2/OPbdqldmTTqyhwklERMSHvf46vPUW2Gzw0UfmmE1iHRVOIiIiPmrtWhg50lx//nlzdHCxlgonERERH7RrF/TrBxkZ8J//wKOPWh2RgAonERERn5OcDL16wcmT0Lr1+Ud1Yj0VTiIiIj7E4YA774Q//oCKFWH5cggJsToqyaTCSURExIc8/bQ591xwMKxYARUqWB2RXEiFk4iIiI9YuBCmTjXX33kHWra0Nh7JSoWTiIiID/jpJ7j7bnN9zBjzcZ34HhVOIiIiFjt8GGJi4MwZ6N4dpk2zOiLJiaWF07Rp02jZsiUlSpSgfPnyxMTEsHPnzsuet3jxYurWrUtISAgNGzbk888/z4doRURE8l56OvTpA/v3Q+3aEBcH/v5WRyU5sbRw+vbbbxk+fDjfffcd8fHx2O12unTpQkpKSo7nbNy4kYEDB3LPPffw888/ExMTQ0xMDNu2bcvHyEVERK6cYUBsLPzvf+enUylVyuqo5FIsnSLwiy++cHk9b948ypcvz5YtW2jXrl2257zyyit069aNMWPGADBlyhTi4+OZNWsWc+bM8XrMIiIieWXOHHj77fPTqdSpY3VEcjk+1cYpKSkJgDKXmL1w06ZNdOrUyWVb165d2bRpk1djExERyUvffgsPPWSuT5um6VQKCkvvOF3I4XAwcuRIrr32Who0aJDjcQkJCURERLhsi4iIICEhIdvj09LSSEtLc75OTk4GwG63Y7fb8yByV5nX9Ma1CzPlzXPKneeUO88ob57LzNmuXefo2zeAc+ds9O/v4JFHMlA6c+bt71xuruszhdPw4cPZtm0bGzZsyNPrTps2jUmTJmXZvmbNGsLCwvL0vS4UHx/vtWsXZsqb55Q7zyl3nlHePHP2rD833ZTOsWOhVK+eyK23bmD16gyrwyoQvPWdS01NdftYnyicYmNj+fTTT1m/fj2VK1e+5LGRkZEcPnzYZdvhw4eJjIzM9vixY8cyatQo5+vk5GSioqLo0qUL4eHhVx78Rex2O/Hx8XTu3JnAwMA8v35hpbx5TrnznHLnGeXNc+npdrp1O8GePSUpV85gzZpiVKnS1eqwfJ63v3OZT6PcYWnhZBgGI0aMYPny5axbt45q1apd9pw2bdqwdu1aRo4c6dwWHx9PmzZtsj0+ODiY4ODgLNsDAwO9+g/e29cvrJQ3zyl3nlPuPKO85d706X5s2FCZgACDpUtt1Kih/OWGt75zubmmpYXT8OHDiYuLY+XKlZQoUcLZTqlkyZKEhoYCMGjQICpVqsS0/x8N7OGHH6Z9+/bMmDGDHj16sHDhQjZv3sxbb71l2ecQERG5nNWr4emnzT5ZM2c6uP56DdZUEFnaq2727NkkJSXRoUMHKlSo4FwWLVrkPGbfvn0cOnTI+bpt27bExcXx1ltv0bhxY5YsWcKKFSsu2aBcRETESn/+CQMHgmHY6Nx5D/fd57A6JPGQ5Y/qLmfdunVZtvXr149+/fp5ISIREZG8lZxsTqeSlARt2ji4775fsdkqWR2WeMinxnESEREpTBwO+M9/4I8/oFIlWLQog8DAy980EN+lwklERMRLJk82p1EJDoZlyyCHDuBSgKhwEhER8YKVKyFzGME5c6BVK2vjkbyhwklERCSP/f473HmnuT5iBAwZYmk4kodUOImIiOShxESzMfjp09C+PcyYYXVEkpdUOImIiOSRjAy44w746y+IioKPPwaNEVq4qHASERHJIxMnwuefQ0gILF8O5ctbHZHkNRVOIiIieWDZMnjmGXP97beheXNr4xHvUOEkIiJyhX7/HQYPNtdHjjzfMFwKHxVOIiIiVyAp6Xxj8A4dYPp0qyMSb1LhJCIi4iGHw7y7pMbgRYcKJxEREQ9NmgSffnq+MXi5clZHJN6mwklERMQDK1eaU6oAvPWWGoMXFSqcREREcmnHDnPyXoCHHjq/LoWfCicREZFcSE6G3r3h1Clo1w5efNHqiCQ/qXASERFxk8Nhzju3YwdUqqTG4EWRCicRERE3Pfec2Qg8KMgc8DIiwuqIJL+pcBIREXHDF1/AU0+Z66+/Dq1aWRuPWEOFk4iIyGX8/TcMHAiGAfffD0OHWh2RWEWFk4iIyCWkpMCtt0JiIrRuDa+8YnVEYiUVTiIiIjkwDLjvPvj1VyhfHpYuheBgq6MSK6lwEhERycGrr0JcHAQEwOLFZk86KdpUOImIiGTj229h9GhzfcYMc8wmERVOIiIiF/n3X7jtNsjIgDvugBEjrI5IfIVHhVP16tU5fvx4lu2JiYlUr179ioMSERGxSloa9O0LR45A48bmPHQ2m9VRia/wqHDas2cPGRkZWbanpaVx4MCBKw5KRETEKiNHwvffQ6lS5iCXYWFWRyS+JCA3B69atcq5/uWXX1KyZEnn64yMDNauXUt0dHSeBSciIpKf5s6FOXPMO0xxcaCHKHKxXBVOMTExANhsNgYPHuyyLzAwkOjoaGbMmJFnwYmIiOSXLVtg2DBzfdIk6N7d2njEN+WqcHI4HABUq1aNH3/8kbJly3olKBERkfx07Bj06WO2b+rZE5580uqIxFflqnDKtHv37ryOQ0RExBIZGeZ0Knv3Qs2a8MEH4Kc+55IDjwqnyZMnX3L/+PHjPQpGREQkv40fD199ZTYCX7bMbBQukhOPCqfly5e7vLbb7ezevZuAgABq1KihwklERAqElSth6lRz/Z13oGFDa+MR3+dR4fTzzz9n2ZacnMyQIUPo3bv3FQclIiLibX/+CYMGmesPP2w+rhO5nDx7ihseHs6kSZN4+umn8+qSIiIiXnH6NNx6KyQnw3XXwQsvWB2RFBR52vwtKSmJpKQkt49fv349PXv2pGLFithsNlasWHHJ49etW4fNZsuyJCQkXGHkIiJSVBgGDB0K27dDZCR8/DEEBlodlRQUHj2qe/XVV11eG4bBoUOHmD9/Pt1zMfBFSkoKjRs35u677+bWW291+7ydO3cSHh7ufF2+fHm3zxURkaLtlVdg0SIICIDFi6FCBasjkoLEo8Jp5syZLq/9/PwoV64cgwcPZuzYsW5fp3v37rkqtDKVL1+eUur2ICIiufTf/8Kjj5rrM2aYj+lEcqNAjuPUpEkT0tLSaNCgARMnTuTaa6+1NB4REfF9hw7BbbedH7dpxAirI5KCyKPC6UL79+8HICoq6oqDuZwKFSowZ84cWrRoQVpaGu+88w4dOnTg+++/p1mzZtmek5aWRlpamvN1cnIyYA6hYLfb8zzGzGt649qFmfLmOeXOc8qdZwpi3ux26NfPn4QEP66+2uCNN85x7pwVcRS83PkCb+ctN9e1GYZh5PYNzp07x6RJk3j11Vc5ffo0AMWLF2fEiBFMmDCBQA9a2dlsNpYvX+6cD89d7du3p0qVKsyfPz/b/RMnTmTSpElZtsfFxRGmKa9FRIqEd99twCef1CAszM6LL35LxYopVockPiQ1NZXbb7+dpKQklzbU2fHojtOIESNYtmwZ06dPp02bNgBs2rSJiRMncvz4cWbPnu3JZT3SqlUrNmzYkOP+sWPHMmrUKOfr5ORkoqKi6NKly2WT4wm73U58fDydO3f2qIAsqpQ3zyl3nlPuPFPQ8rZokY1PPjH/3H3wgY1evdpbFktBy52v8HbeMp9GucOjwikuLo6FCxe6NOxu1KgRUVFRDBw4MF8Lp61bt1LhEl0igoODCQ4OzrI9MDDQq19ab1+/sFLePKfceU6580xByNv27XD//eb62LHQp88Vt1DJEwUhd77IW3nLzTU9+gYFBwcTHR2dZXu1atUICgpy+zqnT59m165dzte7d+9m69atlClThipVqjB27FgOHDjABx98AMDLL79MtWrVuPrqqzl79izvvPMOX3/9NWvWrPHkY4iISCGWnGwOcpmaCp06wZQpVkckhYFHA2DGxsYyZcoUl0bXaWlpPPvss8TGxrp9nc2bN9O0aVOaNm0KwKhRo2jatKlzrrtDhw6xb98+5/Hp6emMHj2ahg0b0r59e3755Re++uorbrzxRk8+hoiIFFKGAUOGmNOqREVBXBz4+1sdlRQGHs9Vt3btWipXrkzjxo0B+OWXX0hPT+fGG290Gcxy2bJlOV6nQ4cOXKpt+rx581xeP/bYYzz22GOehCwiIkXICy/A8uUQFARLlkC5clZHJIWFR4VTqVKl6NOnj8u2/BiOQERE5HK++cZszwTw6qvQqpW18Ujh4lHhNHfu3LyOQ0RE5Ir9+y/07w8OBwweDPfdZ3VEUth41MbphhtuIDExMcv25ORkbrjhhiuNSUREJNfS06FfPzh6FJo0gdmzwWazOiopbDwqnNatW0d6enqW7WfPnuW///3vFQclIiKSW6NHw3ffQalSsHQphIZaHZEURrl6VPfrr78613///XcSEhKcrzMyMvjiiy+oVKlS3kUnIiLihgULYNYsc33+fKhe3dp4pPDKVeHUpEkTbDYbNpst20dyoaGhvPbaa3kWnIiIyOX89hvce6+5/tRTcPPN1sYjhVuuCqfdu3djGAbVq1fnhx9+oNwF/TuDgoIoX748/hooQ0RE8klSkjnI5Zkz0KULTJxodURS2OWqcKpatSoADofDK8GIiIi4K3OQy127oEoV83Gd/t9dvM2j4Qgyp0DJyaBBgzwKRkRExF3Tp8OKFecHuSxb1uqIpCjwqHB6+OGHXV7b7XZSU1MJCgoiLCxMhZOIiHjVN9/AuHHm+quvQsuW1sYjRYdHwxGcPHnSZTl9+jQ7d+7kuuuu46OPPsrrGEVERJw0yKVYyaPCKTu1atXiueeey3I3SkREJK9cOMhl48bwxhsa5FLyV54VTgABAQEcPHgwLy8pIiLilDnIZcmS5iCXYWFWRyRFjUdtnFatWuXy2jAMDh06xKxZs7j22mvzJDAREZELXTjI5YcfQo0a1sYjRZNHhVNMTIzLa5vNRrly5bjhhhuYMWNGXsQlIiLitG3b+bZMTz6pQS7FOh4VTpnjOB09ehTAZSBMERGRvJScbA5ymZoKnTvDpElWRyRFWa7bOCUmJjJ8+HDKli1LZGQkkZGRlC1bltjYWBITE70QooiIFFWZg1z+9RdERUFcnAa5FGvl6o7TiRMnaNOmDQcOHOCOO+6gXr16gDnh77x581i7di0bN26kdOnSXglWRESKlhdegOXLNcil+I5cFU6TJ08mKCiIv//+m4iIiCz7unTpwuTJk5k5c2aeBikiIkXP11/D2LHm+quvQqtW1sYjArl8VLdixQpefPHFLEUTQGRkJNOnT2f58uV5FpyIiBRN//4LAwZokEvxPbkqnA4dOsTVV1+d4/4GDRqQkJBwxUGJiEjRdeEgl02awOzZGuRSfEeuCqeyZcuyZ8+eHPfv3r2bMmXKXGlMIiJShI0aZQ5yWaqUOchlaKjVEYmcl6vCqWvXrjz55JOkp6dn2ZeWlsbTTz9Nt27d8iw4EREpWubPh9dfP79evbq18YhcLNeNw1u0aEGtWrUYPnw4devWxTAM/vjjD9544w3S0tKYP3++t2IVEZFC7Jdf4P77zfWnntIgl+KbclU4Va5cmU2bNvHggw8yduxYDMMAzJHDO3fuzKxZs4iKivJKoCIiUnglJkKfPnDmDHTtChMnWh2RSPZyPXJ4tWrVWL16NSdPnuSvv/4CoGbNmmrbJCIiHnE4YNAg+PtvqFrVnJNOg1yKr/JoyhWA0qVL00qDaoiIyBWaNg0++QSCg83G4FddZXVEIjnL9ZQrIiIieWXNGnj6aXP9jTegeXNr4xG5HBVOIiJiiT17YOBAcz66e++Fu++2OiKRy1PhJCIi+e7MGbMx+IkT0KKFOaWKSEGgwklERPKVYcDw4fDTT+akvUuXQkiI1VGJuEeFk4iI5Ku33oK5c8HPDxYuhCpVrI5IxH0qnEREJN98/z2MGGGuT50KN95obTwiuaXCSURE8sXRo9C3L9jt0Ls3PPaY1RGJ5J6lhdP69evp2bMnFStWxGazsWLFisues27dOpo1a0ZwcDA1a9Zk3rx5Xo9TRESuzLlz0L8//Psv1KkD8+aBzWZ1VCK5Z2nhlJKSQuPGjXk9c0bHy9i9ezc9evSgY8eObN26lZEjRzJ06FC+/PJLL0cqIiJXYuxY+OYbKF4cli2D8HCrIxLxjMcjh+eF7t270717d7ePnzNnDtWqVWPGjBkA1KtXjw0bNjBz5ky6du3qrTBFROQKfPwxvPiiuT5vHtSvb2k4IlfE0sIptzZt2kSnTp1ctnXt2pWRI0fmeE5aWhppaWnO18nJyQDY7Xbsdnuex5h5TW9cuzBT3jyn3HlOufNMbvK2bRvcfXcAYOPRRzPo1ctBUU63vnOe8XbecnPdAlU4JSQkEBER4bItIiKC5ORkzpw5Q2hoaJZzpk2bxqRJk7JsX7NmDWFhYV6LNT4+3mvXLsyUN88pd55T7jxzubydPh3AY4+1JyUlkMaNj9CmzXd8/rmRT9H5Nn3nPOOtvKWmprp9bIEqnDwxduxYRo0a5XydnJxMVFQUXbp0IdwLD9ntdjvx8fF07tyZwMDAPL9+YaW8eU6585xy5xl38uZwQJ8+/hw86EfVqgarV5embFn3m2YUVvrOecbbect8GuWOAlU4RUZGcvjwYZdthw8fJjw8PNu7TQDBwcEEBwdn2R4YGOjVL623r19YKW+eU+48p9x55lJ5mzwZPvvMHBF82TIbFSoovxfSd84z3spbbq5ZoMZxatOmDWvXrnXZFh8fT5s2bSyKSERELvbppzBhgrk+ezY0a2ZtPCJ5ydLC6fTp02zdupWtW7cC5nADW7duZd++fYD5mG3QoEHO4x944AH++ecfHnvsMXbs2MEbb7zBxx9/zCOPPGJF+CIicpE//4Q77jDXY2NhyBBLwxHJc5YWTps3b6Zp06Y0bdoUgFGjRtG0aVPGjx8PwKFDh5xFFEC1atX47LPPiI+Pp3HjxsyYMYN33nlHQxGIiPiAU6cgJgaSk+H66+Gll6yOSCTvWdrGqUOHDhhGzj0sshsVvEOHDvz8889ejEpERHLL4YDBg+GPP6BSJVi8GNSERwqjAtXGSUREfNNzz8Hy5RAUBEuXwkUjx4gUGiqcRETkinz+OTz1lLn+xhvQurW18Yh4kwonERHx2J9/wu23g2HA/ffDPfdYHZGId6lwEhERjyQnm43Bk5Lg2mvh1VetjkjE+1Q4iYhIrjkccNdd/s7G4EuWmO2bRAo7FU4iIpJrH39ch08+8SM42GwUHhlpdUQi+UOFk4iI5MqqVTYWLqwLwJw50LKlxQGJ5CMVTiIi4rZt22DIEH8AYmMzNDK4FDkqnERExC3Hj8Mtt8Dp0zYaNjzK8887rA5JJN+pcBIRkcs6dw7694d//oFq1QzGjPlRI4NLkaTCSURELmv0aFi7FooVgyVLzhEebrc6JBFLqHASEZFLevfd82M0zZ8PDRtaG4+IlVQ4iYhIjv73Pxg2zFyfNAl697Y2HhGrqXASEZFs7d0Lt94Kdjv06XN+PjqRokyFk4iIZHH6NPTqBUeOQJMm8P774Ke/GCIqnERExJXDAf/5D/z6K0REwMqVZqNwEVHhJCIiF3n6aVixwpx7bvlyqFLF6ohEfIcKJxERcYqLg6lTzfV33oE2bayNR8TXqHASEREAvv8e7r7bXH/8cfNxnYi4UuEkIiLs3Ws2Bk9Lg549z991EhFXKpxERIq45GSzWDpyBBo3Nh/XqQedSPb0T0NEpAjLyICBA+G33yAyEj75BIoXtzoqEd+lwklEpAgbPRo+/xxCQ2HVKoiKsjoiEd+mwklEpIiaPRteecVc/+ADaNnS2nhECgIVTiIiRdAXX8CIEeb6s89C377WxiNSUKhwEhEpYn75Bfr1M9s3DRoEY8daHZFIwaHCSUSkCDlwAHr0MOei69gR3n4bbDaroxIpOFQ4iYgUEadOwc03m8VTvXqwdKk5rYqIuE+Fk4hIEXDuHAwYAFu3QvnyZk+60qWtjkqk4FHhJCJSyBkGPPTQ+WEHPvkEoqOtjkqkYFLhJCJSyD33nDn0gM0GCxZAq1ZWRyRScKlwEhEpxObPh3HjzPVXXoHeva2NR6SgU+EkIlJIxcfD3Xeb62PGnB+3SUQ8p8JJRKQQ+vlnuPVWs1H4wIHm4zoRuXI+UTi9/vrrREdHExISQuvWrfnhhx9yPHbevHnYbDaXJSQkJB+jFRHxbXv2wE03nR+rae5c8POJ/9qLFHyW/1NatGgRo0aNYsKECfz00080btyYrl27cuTIkRzPCQ8P59ChQ85l7969+RixiIjvOnoUunaFhARo2BCWL4fgYKujEik8LC+cXnrpJe69917uuusu6tevz5w5cwgLC+O9997L8RybzUZkZKRziYiIyMeIRUR806lT5p2mP/+EKlVg9WooWdLqqEQKlwAr3zw9PZ0tW7Yw9oKJkvz8/OjUqRObNm3K8bzTp09TtWpVHA4HzZo1Y+rUqVx99dXZHpuWlkZaWprzdXJyMgB2ux273Z5Hn+S8zGt649qFmfLmOeXOc4Upd2lpEBPjz+bNfpQta/DZZ+coXx688dEKU97ym3LnGW/nLTfXtRmGYXglCjccPHiQSpUqsXHjRtq0aePc/thjj/Htt9/y/fffZzln06ZN/PXXXzRq1IikpCRefPFF1q9fz/bt26lcuXKW4ydOnMikSZOybI+LiyMsLCxvP5CIiAUyMmDGjBZs3FiJkJBzTJnyP2rVSrQ6LJECIzU1ldtvv52kpCTCw8Mveayld5w80aZNG5ciq23bttSrV48333yTKVOmZDl+7NixjBo1yvk6OTmZqKgounTpctnkeMJutxMfH0/nzp0JDAzM8+sXVsqb55Q7zxWG3BkGjBjhx8aN/gQGGixbBp06tfXqexaGvFlFufOMt/OW+TTKHZYWTmXLlsXf35/Dhw+7bD98+DCRkZFuXSMwMJCmTZuya9eubPcHBwcTnE3LyMDAQK9+ab19/cJKefOccue5gpy7p56Ct94yRwX/8EMb3bvn33/WC3LerKbcecZbecvNNS1tHB4UFETz5s1Zu3atc5vD4WDt2rUud5UuJSMjg99++40KFSp4K0wREZ80fTo8+6y5PmsW3HabtfGIFAWWP6obNWoUgwcPpkWLFrRq1YqXX36ZlJQU7rrrLgAGDRpEpUqVmDZtGgCTJ0/mmmuuoWbNmiQmJvLCCy+wd+9ehg4dauXHEBHJV3PmwOOPm+vPPQcPPmhtPCJFheWFU//+/Tl69Cjjx48nISGBJk2a8MUXXziHGNi3bx9+F4zcdvLkSe69914SEhIoXbo0zZs3Z+PGjdSvX9+qjyAikq8+/PB8oTRu3PkCSkS8z/LCCSA2NpbY2Nhs961bt87l9cyZM5k5c2Y+RCUi4ntWrIAhQ8xG4bGx8MwzVkckUrRYPgCmiIi454svoH9/c/iBwYPhlVfMRuEikn9UOImIFADx8RATA+np0LcvvPOO5p8TsYL+2YmI+Livv4ZevczRwW+5BeLiIMAnGlqIFD0qnEREfNj69dCzJ5w9Cz16wKJFoOF/RKyjwklExEf973/mpL2pqdC1KyxZAtmM5ysi+UiFk4iID/rvf6FbN0hJgU6dYPlyCAmxOioRUeEkIuJjvvnGLJpOn4YbboCVKyE01OqoRARUOImI+JT4eLMtU2oqdOkCn34KYWFWRyUimVQ4iYj4iNWrzYbgZ86YxZPuNIn4HhVOIiI+YOVKc5ymtDTz57JlatMk4otUOImIWGz+fOjTxxzcsl8/+PhjCAqyOioRyY4KJxERC732GgwaZE6jMmiQObilxmkS8V0qnERELGAYMGUKPPSQ+fqhh2DuXI0ILuLrVDiJiOQzhwNGjYLx483XEyfCyy9r7jmRgkD/byMiko/S0+Huu2HBAvP1yy/Dww9bGpKI5IIKJxGRfJKUZDYCX7sW/P3h3Xdh8GCroxKR3FDhJCKSDw4cMOed+/VXKFYMli41558TkYJFhZOIiJdt3w7du8P+/RARAZ9/Ds2aWR2ViHhCTRFFRLxo7Vq47jqzaKpTBzZtUtEkUpCpcBIR8ZI5c8zHcYmJ0LYt/O9/UK2a1VGJyJVQ4SQiksfOnTPHZRo2zBzY8s47zTtPV11ldWQicqXUxklEJA8lJkL//rBmjfl66lR44gmw2SwNS0TyiAonEZE88scf0Ls37NwJYWHmHHS33mp1VCKSl/SoTkQkDyxdCq1amUVT5cqwYYOKJpHCSIWTiMgVOHcOHnsM+vaF06ehQwfYsgWaNrU6MhHxBj2qExHx0NGjMGAAfP21+Xr0aHjuOU3UK1KY6Z+3iIgH1q2DO+6AgwfNkcDfew9uu83qqETE2/SoTkQkF86dg/Hj4YYbzKKpbl34/nsVTSJFhe44iYi4ad8+8y7Thg3m67vvhldfNe84iUjRoDtOIiJuWLwYmjQxi6YSJSAuDt59V0WTSFGjO04iIpdw/DgMHw6LFpmvW7aEjz6CGjWsjUtErKE7TiIiOfj0U2jQwCya/P3h6afNO04qmkSKLt1xEhG5yMmT5tACc+ear+vVg/ffN+82iUjR5hN3nF5//XWio6MJCQmhdevW/PDDD5c8fvHixdStW5eQkBAaNmzI559/nk+RikhhZhjmY7i6dc2iyWaDRx+Fn35S0SQiJssLp0WLFjFq1CgmTJjATz/9ROPGjenatStHjhzJ9viNGzcycOBA7rnnHn7++WdiYmKIiYlh27Zt+Ry5iBQmf/8N3brB7bfDkSPmXab16+GFFyAkxOroRMRXWF44vfTSS9x7773cdddd1K9fnzlz5hAWFsZ7772X7fGvvPIK3bp1Y8yYMdSrV48pU6bQrFkzZs2alc+Ri0hhkJ7ux7RpfjRoAGvWQHAwPPMMbN0K111ndXQi4mssLZzS09PZsmULnTp1cm7z8/OjU6dObNq0KdtzNm3a5HI8QNeuXXM8XkQkO4YBixfbGD78RiZM8OfsWejUCbZtgyefhKAgqyMUEV9kaePwY8eOkZGRQUREhMv2iIgIduzYke05CQkJ2R6fkJCQ7fFpaWmkpaU5XycnJwNgt9ux2+1XEn4Wv/8OXbsGcO5cF4oV8ycw0MDPz5y3KiAAAgONC9bPLxe+DgrKXAyCgsxtwcHmtuBgcwkJMfeFhLguoaHmvtBQcz0szFwCA822Gr4s83eR17+TokC5y70tW2yMHu3Hxo0BQACVKjmYOtXBgAEGNhsolZem75znlDvPeDtvubluoe9VN23aNCZNmpRl+5o1awgLC8vT9/r775IcPtwBCOX48eyOsKZ68fNzEBycQUhIBsHB5wgJySA09Nz/bztHaKjrEhZ2jtBQO8WKnf9ZrJidYsXshIba8ff3Xqzx8fHeu3ghp9xd3sGDxVi4sA7r10cBEBx8jltv/YuYmL8JDs5g9WqLAyxg9J3znHLnGW/lLTU11e1jLS2cypYti7+/P4cPH3bZfvjwYSIjI7M9JzIyMlfHjx07llGjRjlfJycnExUVRZcuXQgPD7/CT+DqzBm49toz/O9/39Oy5TVAAOfOkWWx288v51/bSE831zN/pqWd/5mebiMtDZfl7NnMxcaZM+a2M2fMJTUVHA6zUHM4/Dhzxo8zZwLz5HOGhxuULg2lSkGZMsb//zTXr7oKypY1KFMGypY118uVg/DwS9/1stvtxMfH07lzZwID8ybOokK5u7w9e2DqVH/mz7eRkWF+Ee+808GECXa2b/9Tucslfec8p9x5xtt5y3wa5Q5LC6egoCCaN2/O2rVriYmJAcDhcLB27VpiY2OzPadNmzasXbuWkSNHOrfFx8fTpk2bbI8PDg4mODg4y/bAwMA8T35gIDRrBgkJSVxzjT+Bgdal1zDMois11VxSUsyfp0+b6ykp5vrp03DqlOuSnAxJSed/Zi6ZBXlyso3kZNi7F9y9ixYUBOXKQfny5hIZCRER55fy5W38+29xUlMDueqqQJ9/tOiLvPGdLuj27YPnn4e33z7/+K1HD5g8GZo188NuD2D7duXOU8qb55Q7z3grb7m5puWP6kaNGsXgwYNp0aIFrVq14uWXXyYlJYW77roLgEGDBlGpUiWmTZsGwMMPP0z79u2ZMWMGPXr0YOHChWzevJm33nrLyo/hc2y28+2lSpXKm2ump5sF1MmTkJgIJ06Yy8mT5rQUJ07AsWPm+rFj5nL0qFmkpafDgQPmkr0A4EZiY812WRUqQKVK55fKlc0lKspcIiLAz/I+oeKrfv3VHEZg4ULzri6YDb+nTIFrrrE2NhEp2CwvnPr378/Ro0cZP348CQkJNGnShC+++MLZAHzfvn34XfAXsm3btsTFxfHUU08xbtw4atWqxYoVK2jQoIFVH6HIyLxrVK5c7s47c8YsoI4cMZfDh12XhAQ4eNBg//5zpKYGkppqjqnz9985XzMw0CyoqlSBqlXNpUoViI42lypVzIb0UnQYBnzzjVkwffHF+e033GBOldKhg2WhiUghYnnhBBAbG5vjo7l169Zl2davXz/69evn5agkr4SGmoVMlSo5H2O3n+Pzzz+nQ4ebOHo0kIMHz9+hylz27zeXQ4fMxy579phLdmw2865VtWrmUr2661Khgu5YFRYnT8IHH8CcOZDZGdfPD/r1gzFjoHlza+MTkcLFJwonkUxhYVCzprnk5Nw5OHjQLKL27TPbWmX+3LsXdu8222MdPGgu//tf1muEhJgFVc2a5oStme9Zs6Z59ypA/zJ8mmHApk1m26WFC81OEgDFisGQITBqlFkgi4jkNf15kAInIOD8Haxrr8263zDM9lV79phF1O7d8M8/5rJ7t1lcnT0Lf/xhLtldPzoaatXKulStileHY5BL+/13iIszl927z29v1AiGDTOnS8njzrIiIi5UOEmhY7Odb4uV3cSs586Zd6gy21Ht2mUumetnz57fdvG4PkFB5p2MWrWgdm3Xn5Uq+f5AowWNYcD27fDJJ/Dxx+Y0KJmKFYO+feGBB6B1a+VeRPKHCicpcgICzrd16tzZdZ/DYT7e++svc9m1y3U9Lc1sR5PdwPaZjxkzC6nMpXZts4jTH3b3pKXBhg2wapVZMF14ZykgALp3N+8s9epl5lxEJD+pcBK5gJ/f+aEPOnZ03edwwL//wp9/mstff53/+c8/ZruqX381l4uFh7u2o6pVy2xbVaOGOaZVUW6onp4OP/5o9oj75hvYuPF8myUwe0d26mQWSn36wFVXWReriIgKJxE3+fmdb1t10TzT2O3mnZELi6nMZd8+czDRn34yl4uFhJh3v2rUON8LMDr6/M+SJfPj0+UPh8PMyY8/wubN5s+ffzaHrLhQRIQ5UGWvXmauixWzJl4RkYupcBLJA4GB5iO52rXNP/gXOnvWvCOV2W4qs6D655/zDdV//91cslOypDnoZ2bRFhUFFSuabaoqVjQfAxqG9z9jbpw+bRaMu3aZn+uPP8yfO3aY+y5Wtqw5zlLHjuZSt64ebYqIb1LhJOJlISFQv765XMxuN4unf/4xG6dn9gTM/Hns2Pkpb7Zty+kdAgkMvJnISD/nlDbly5vzB5Yu7boUL27evSlWzGwfFBZmFn0BAed/2mxmA/qMDHM5d84s7jKn6ElJMafmOX7cHNg0czl8+PywECdOXDofTZuaDfdbtoQWLcyCsyg/rhSRgkOFk4iFAgMvPW7V6dPnx6vKXDIHAT1wwGzIfvIk2O3+zgFCfUWpUubjxnr1zKV+ffNnzZrm5xYRKYhUOIn4sOLFzxceOUlOtrNw4Tc0aHADJ04EOKe2OXHCnFPw5Mnz8wteOMlz5hyC7ipW7Pwdq+LFzUbamcM+lC1r3uW6cAocjackIoWRCieRAi40FCIiztCypZHrOzmZj+IuXBwOc5DPgADzp7+/OX6VHqWJiKhwEinSMgsjTYgsIuIe/T+kiIiIiJtUOImIiIi4SYWTiIiIiJtUOImIiIi4SYWTiIiIiJtUOImIiIi4SYWTiIiIiJtUOImIiIi4SYWTiIiIiJtUOImIiIi4qchNuWIYBgDJycleub7dbic1NZXk5GQCNQW825Q3zyl3nlPuPKO8eU6584y385ZZE2TWCJdS5AqnU6dOARAVFWVxJCIiIuJLTp06RcmSJS95jM1wp7wqRBwOBwcPHqREiRLYbLY8v35ycjJRUVHs37+f8PDwPL9+YaW8eU6585xy5xnlzXPKnWe8nTfDMDh16hQVK1bEz+/SrZiK3B0nPz8/Kleu7PX3CQ8P1z8KDyhvnlPuPKfceUZ585xy5xlv5u1yd5oyqXG4iIiIiJtUOImIiIi4SYVTHgsODmbChAkEBwdbHUqBorx5TrnznHLnGeXNc8qdZ3wpb0WucbiIiIiIp3THSURERMRNKpxERERE3KTCSURERMRNKpxERERE3KTCyYt69epFlSpVCAkJoUKFCvznP//h4MGDVofl8/bs2cM999xDtWrVCA0NpUaNGkyYMIH09HSrQ/N5zz77LG3btiUsLIxSpUpZHY5Pe/3114mOjiYkJITWrVvzww8/WB2Sz1u/fj09e/akYsWK2Gw2VqxYYXVIBcK0adNo2bIlJUqUoHz58sTExLBz506rwyoQZs+eTaNGjZwDX7Zp04bVq1dbGpMKJy/q2LEjH3/8MTt37mTp0qX8/fff9O3b1+qwfN6OHTtwOBy8+eabbN++nZkzZzJnzhzGjRtndWg+Lz09nX79+jFs2DCrQ/FpixYtYtSoUUyYMIGffvqJxo0b07VrV44cOWJ1aD4tJSWFxo0b8/rrr1sdSoHy7bffMnz4cL777jvi4+Ox2+106dKFlJQUq0PzeZUrV+a5555jy5YtbN68mRtuuIFbbrmF7du3WxaThiPIR6tWrSImJoa0tDTNip1LL7zwArNnz+aff/6xOpQCYd68eYwcOZLExESrQ/FJrVu3pmXLlsyaNQsw57CMiopixIgRPPHEExZHVzDYbDaWL19OTEyM1aEUOEePHqV8+fJ8++23tGvXzupwCpwyZcrwwgsvcM8991jy/rrjlE9OnDjBggULaNu2rYomDyQlJVGmTBmrw5BCID09nS1bttCpUyfnNj8/Pzp16sSmTZssjEyKiqSkJAD9Ny2XMjIyWLhwISkpKbRp08ayOFQ4ednjjz9OsWLFuOqqq9i3bx8rV660OqQCZ9euXbz22mvcf//9VocihcCxY8fIyMggIiLCZXtERAQJCQkWRSVFhcPhYOTIkVx77bU0aNDA6nAKhN9++43ixYsTHBzMAw88wPLly6lfv75l8ahwyqUnnngCm812yWXHjh3O48eMGcPPP//MmjVr8Pf3Z9CgQRTVp6O5zR3AgQMH6NatG/369ePee++1KHJreZI3EfFNw4cPZ9u2bSxcuNDqUAqMOnXqsHXrVr7//nuGDRvG4MGD+f333y2LR22ccuno0aMcP378ksdUr16doKCgLNv//fdfoqKi2Lhxo6W3Ga2S29wdPHiQDh06cM011zBv3jz8/Ipmne/Jd05tnHKWnp5OWFgYS5YscWmfM3jwYBITE3VX2E1q45R7sbGxrFy5kvXr11OtWjWrwymwOnXqRI0aNXjzzTctef8AS961ACtXrhzlypXz6FyHwwFAWlpaXoZUYOQmdwcOHKBjx440b96cuXPnFtmiCa7sOydZBQUF0bx5c9auXev8o+9wOFi7di2xsbHWBieFkmEYjBgxguXLl7Nu3ToVTVfI4XBY+ndUhZOXfP/99/z4449cd911lC5dmr///punn36aGjVqFMm7Tblx4MABOnToQNWqVXnxxRc5evSoc19kZKSFkfm+ffv2ceLECfbt20dGRgZbt24FoGbNmhQvXtza4HzIqFGjGDx4MC1atKBVq1a8/PLLpKSkcNddd1kdmk87ffo0u3btcr7evXs3W7dupUyZMlSpUsXCyHzb8OHDiYuLY+XKlZQoUcLZlq5kyZKEhoZaHJ1vGzt2LN27d6dKlSqcOnWKuLg41q1bx5dffmldUIZ4xa+//mp07NjRKFOmjBEcHGxER0cbDzzwgPHvv/9aHZrPmzt3rgFku8ilDR48ONu8ffPNN1aH5nNee+01o0qVKkZQUJDRqlUr47vvvrM6JJ/3zTffZPv9Gjx4sNWh+bSc/ns2d+5cq0PzeXfffbdRtWpVIygoyChXrpxx4403GmvWrLE0JrVxEhEREXFT0W04IiIiIpJLKpxERERE3KTCSURERMRNKpxERERE3KTCSURERMRNKpxERERE3KTCSURERMRNKpxERERE3KTCSURERMRNKpxEpMAaMmSIc6Le/DJv3jxKlSqVr+8pIr5DhZOIiIiIm1Q4iUih0KFDBx566CEee+wxypQpQ2RkJBMnTnQ5xmazMXv2bLp3705oaCjVq1dnyZIlzv3r1q3DZrORmJjo3LZ161ZsNht79uxh3bp13HXXXSQlJWGz2bDZbFneQ0QKNxVOIlJovP/++xQrVozvv/+e6dOnM3nyZOLj412Oefrpp+nTpw+//PILd9xxBwMGDOCPP/5w6/pt27bl5ZdfJjw8nEOHDnHo0CEeffRRb3wUEfFRKpxEpNBo1KgREyZMoFatWgwaNIgWLVqwdu1al2P69evH0KFDqV27NlOmTKFFixa89tprbl0/KCiIkiVLYrPZiIyMJDIykuLFi3vjo4iIj1LhJCKFRqNGjVxeV6hQgSNHjrhsa9OmTZbX7t5xEhFR4SQihUZgYKDLa5vNhsPhcPt8Pz/zP4mGYTi32e32vAlORAoFFU4iUqR89913WV7Xq1cPgHLlygFw6NAh5/6tW7e6HB8UFERGRoZ3gxQRn6XCSUSKlMWLF/Pee+/x559/MmHCBH744QdiY2MBqFmzJlFRUUycOJG//vqLzz77jBkzZricHx0dzenTp1m7di3Hjh0jNTXVio8hIhZR4SQiRcqkSZNYuHAhjRo14oMPPuCjjz6ifv36gPmo76OPPmLHjh00atSI559/nmeeecbl/LZt2/LAAw/Qv39/ypUrx/Tp0634GCJiEZtx4cN8EZFCzGazsXz58nwfbVxECg/dcRIRERFxkwonERERETcFWB2AiEh+UcsEEblSuuMkIiIi4iYVTiIiIiJuUuEkIiIi4iYVTiIiIiJuUuEkIiIi4iYVTiIiIiJuUuEkIiIi4iYVTiIiIiJuUuEkIiIi4iYVTiIiIiJuUuEkIiIi4iYVTiIiIiJuUuEkIiIi4iYVTiLiU4YMGUJ0dLQl7z1x4kRsNpsl710QdejQgQ4dOlgdhki+UuEkkk/mzZuHzWZzLgEBAVSqVIkhQ4Zw4MABj665bt06bDYbS5YsyfEYm81GbGxstvuWLFmCzWZj3bp1br/nG2+8gc1mo3Xr1rkN1+ngwYNMnDiRrVu3enwNT6WmpjJx4sRcfeb8cOF348IlMjLS0rh+//13Jk6cyJ49eyyNQ8RXBFgdgEhRM3nyZKpVq8bZs2f57rvvmDdvHhs2bGDbtm2EhIRYHd5lLViwgOjoaH744Qd27dpFzZo1c32NgwcPMmnSJKKjo2nSpInLvrfffhuHw5FH0WaVmprKpEmTALLcLXnqqad44oknvPbel9O5c2cGDRrksi00NNSiaEy///47kyZNokOHDlnuBK5Zs8aaoEQspMJJJJ91796dFi1aADB06FDKli3L888/z6pVq7jtttssju7Sdu/ezcaNG1m2bBn3338/CxYsYMKECXn6HoGBgXl6vdwICAggIMC6/yzWrl2bO++807L3z62goCCrQxDJd3pUJ2Kx66+/HoC///7bZfuOHTvo27cvZcqUISQkhBYtWrBq1SorQnRasGABpUuXpkePHvTt25cFCxZke1xiYiKPPPII0dHRBAcHU7lyZQYNGsSxY8dYt24dLVu2BOCuu+5yPpKaN28e4NrGyW63U6ZMGe66664s75GcnExISAiPPvooAOnp6YwfP57mzZtTsmRJihUrxvXXX88333zjPGfPnj2UK1cOgEmTJjnfe+LEiUD2bZzOnTvHlClTqFGjBsHBwURHRzNu3DjS0tJcjouOjubmm29mw4YNtGrVipCQEKpXr84HH3yQuyTnIKe2X9nFnPl4dsWKFTRo0IDg4GCuvvpqvvjiiyznHzhwgHvuuYeKFSsSHBxMtWrVGDZsGOnp6cybN49+/foB0LFjR2e+Mh9zZtfG6ciRI9xzzz1EREQQEhJC48aNef/9912O2bNnDzabjRdffJG33nrLmduWLVvy448/ep4kkXygO04iFstsO1K6dGnntu3bt3PttddSqVIlnnjiCYoVK8bHH39MTEwMS5cupXfv3pbEumDBAm699VaCgoIYOHAgs2fP5scff3QWQgCnT5/m+uuv548//uDuu++mWbNmHDt2jFWrVvHvv/9Sr149Jk+ezPjx47nvvvuchWPbtm2zvF9gYCC9e/dm2bJlvPnmmy53OFasWEFaWhoDBgwAzELqnXfeYeDAgdx7772cOnWKd999l65du/LDDz/QpEkTypUrx+zZsxk2bBi9e/fm1ltvBaBRo0Y5fuahQ4fy/vvv07dvX0aPHs3333/PtGnT+OOPP1i+fLnLsbt27aJv377cc889DB48mPfee48hQ4bQvHlzrr766svm9+zZsxw7dsxlW4kSJQgODr7suRfbsGEDy5Yt48EHH6REiRK8+uqr9OnTh3379nHVVVcB5iPTVq1akZiYyH333UfdunU5cOAAS5YsITU1lXbt2vHQQw/x6quvMm7cOOrVqwfg/HmxM2fO0KFDB3bt2kVsbCzVqlVj8eLFDBkyhMTERB5++GGX4+Pi4jh16hT3338/NpuN6dOnc+utt/LPP/9YeudR5JIMEckXc+fONQDjq6++Mo4ePWrs37/fWLJkiVGuXDkjODjY2L9/v/PYG2+80WjYsKFx9uxZ5zaHw2G0bdvWqFWrlnPbN998YwDG4sWLc3xfwBg+fHi2+xYvXmwAxjfffHPZ+Ddv3mwARnx8vDOeypUrGw8//LDLcePHjzcAY9myZVmu4XA4DMMwjB9//NEAjLlz52Y5ZvDgwUbVqlWdr7/88ksDMD755BOX42666SajevXqztfnzp0z0tLSXI45efKkERERYdx9993ObUePHjUAY8KECVnee8KECcaF/1ncunWrARhDhw51Oe7RRx81AOPrr792bqtataoBGOvXr3duO3LkiBEcHGyMHj06y3tdDMh2yczRxXnJKebMawUFBRm7du1ybvvll18MwHjttdec2wYNGmT4+fkZP/74Y5brZv6uLvUdad++vdG+fXvn65dfftkAjA8//NC5LT093WjTpo1RvHhxIzk52TAMw9i9e7cBGFdddZVx4sQJ57ErV67M9nct4kv0qE4kn3Xq1Ily5coRFRVF3759KVasGKtWraJy5coAnDhxgq+//prbbruNU6dOcezYMY4dO8bx48fp2rUrf/31l8e98K7EggULiIiIoGPHjoD5OKh///4sXLiQjIwM53FLly6lcePG2d4V86Sr/w033EDZsmVZtGiRc9vJkyeJj4+nf//+zm3+/v7OO1IOh4MTJ05w7tw5WrRowU8//ZTr9wX4/PPPARg1apTL9tGjRwPw2WefuWyvX7++8w4aQLly5ahTpw7//POPW+93yy23EB8f77J07drVo9g7depEjRo1nK8bNWpEeHi4MxaHw8GKFSvo2bOns83dhTz5XX3++edERkYycOBA57bAwEAeeughTp8+zbfffutyfP/+/V3utGbmzt18iVhBj+pE8tnrr79O7dq1SUpK4r333mP9+vUuj2J27dqFYRg8/fTTPP3009le48iRI1SqVCnPYrrcH8mMjAwWLlxIx44d2b17t3N769atmTFjBmvXrqVLly6A2VarT58+eRZbQEAAffr0IS4ujrS0NIKDg1m2bBl2u92lcAJ4//33mTFjBjt27MButzu3V6tWzaP33rt3L35+fll6DkZGRlKqVCn27t3rsr1KlSpZrlG6dGlOnjzp1vtVrlyZTp06eRTrxS4Xy9GjR0lOTqZBgwZ58n5g5qtWrVr4+bn+P3nmo73L5SuziHI3XyJWUOEkks9atWrl/D/8mJgYrrvuOm6//XZ27txJ8eLFnV3xH3300RzvNuRmCIDg4GDOnDmT7b7U1FSAyw6D8PXXX3Po0CEWLlzIwoULs+xfsGCBs3DyhgEDBvDmm2+yevVqYmJi+Pjjj6lbty6NGzd2HvPhhx8yZMgQYmJiGDNmDOXLl8ff359p06ZlaXifW+7effH39892u2EYV/T+l4rhwrt9+RVLXikIMYpcTIWTiIUy/7B37NiRWbNm8cQTT1C9enXAfMSRF3cfqlatys6dO7Pdl7m9atWql7zGggULKF++PK+//nqWfcuWLWP58uXMmTOH0NBQatSowbZt2y55vdw+BmrXrh0VKlRg0aJFXHfddXz99dc8+eSTLscsWbKE6tWrs2zZMpfrXzxcQm7eu2rVqjgcDv766y+XBtGHDx8mMTHxsnnLS6VLlyYxMTHL9ovv4rirXLlyhIeH5+nvqmrVqvz66684HA6Xu047duxw7hcp6NTGScRiHTp0oFWrVrz88sucPXuW8uXL06FDB958800OHTqU5fijR4/m6vo33XQT3333HVu2bHHZnpiYyIIFC2jSpMklR6c+c+YMy5Yt4+abb6Zv375ZltjYWE6dOuUcKqFPnz788ssvWXqcwfk7CcWKFXPG4A4/Pz/69u3LJ598wvz58zl37lyWx3SZdy8uvFvx/fffs2nTJpfjwsLC3H7vm266CYCXX37ZZftLL70EQI8ePdyKPy/UqFGDpKQkfv31V+e2Q4cOZZtnd/j5+RETE8Mnn3zC5s2bs+z35Hd10003kZCQ4NIe7dy5c7z22msUL16c9u3bexSriC/RHScRHzBmzBj69evHvHnzeOCBB3j99de57rrraNiwIffeey/Vq1fn8OHDbNq0iX///ZdffvnF5fylS5c6/6/+QoMHD+aJJ55g8eLFtGvXjvvvv5+6dety8OBB5s2bx6FDh5g7d+4lY1u1ahWnTp2iV69e2e6/5pprKFeuHAsWLKB///6MGTOGJUuW0K9fP+6++26aN2/OiRMnWLVqFXPmzKFx48bUqFGDUqVKMWfOHEqUKEGxYsVo3br1Jdsi9e/fn9dee40JEybQsGHDLF3ib775ZpYtW0bv3r3p0aMHu3fvZs6cOdSvX5/Tp087jwsNDaV+/fosWrSI2rVrU6ZMGRo0aJBtW5/GjRszePBg3nrrLRITE2nfvj0//PAD77//PjExMc6G8vlhwIABPP744/Tu3ZuHHnqI1NRUZs+eTe3atT1u/D516lTWrFlD+/btue+++6hXrx6HDh1i8eLFbNiwgVKlStGkSRP8/f15/vnnSUpKIjg4mBtuuIHy5ctnud59993Hm2++yZAhQ9iyZQvR0dEsWbKE//3vf7z88suUKFHiStMgYj0Le/SJFCmZwxFk1/U7IyPDqFGjhlGjRg3j3LlzhmEYxt9//20MGjTIiIyMNAIDA41KlSoZN998s7FkyRLneZnDEeS0/Pe//zUMwzD+/fdfY+jQoUalSpWMgIAAo0yZMsbNN99sfPfdd5eNu2fPnkZISIiRkpKS4zFDhgwxAgMDjWPHjhmGYRjHjx83YmNjjUqVKhlBQUFG5cqVjcGDBzv3G4bZ9bx+/fpGQECAW93uHQ6HERUVZQDGM888k+3+qVOnGlWrVjWCg4ONpk2bGp9++mm219u4caPRvHlzIygoyGVoguy69tvtdmPSpElGtWrVjMDAQCMqKsoYO3asy1ARhmEOR9CjR48scV3cZT8nXGLYiExr1qwxGjRoYAQFBRl16tQxPvzwwxyHI8juWlWrVjUGDx7ssm3v3r3GoEGDnMNiVK9e3Rg+fLjL0A5vv/22Ub16dcPf399laILsPtvhw4eNu+66yyhbtqwRFBRkNGzYMMuwE5nDEbzwwgvZ5iG7oSJEfIXNMNQKT0RERMQdauMkIiIi4iYVTiIiIiJuUuEkIiIi4iYVTiIiIiJuUuEkIiIi4iYVTiIiIiJuUuEkIiIi4qYiN3K4w+Hg4MGDlChRItfzZYmIiEjhYxgGp06domLFii7zLGanyBVOBw8eJCoqyuowRERExMfs37+fypUrX/KYIlc4Zc6VtH//fsLDw/P8+na7nTVr1tClSxcCAwPz/PqFlfLmOeXOc8qdZ5Q3zyl3nvF23pKTk4mKinJrPsUiVzhlPp4LDw/3WuEUFhZGeHi4/lHkgvLmOeXOc8qdZ5Q3zyl3nsmvvLnThEeNw0VERETcpMJJRERExE0qnERERETcVOTaOLkrIyMDu92e6/PsdjsBAQGcPXuWjIwML0RWcAQGBuLv7291GCIiInlGhdNFDMMgISGBxMREj8+PjIxk//79GicKKFWqFJGRkcqFiIgUCiqcLpJZNJUvX56wsLBc/8F3OBycPn2a4sWLX3YQrcLMMAxSU1M5cuQIABUqVLA4IhERkSunwukCGRkZzqLpqquu8ugaDoeD9PR0QkJCinThBBAaGgrAkSNHKF++vB7biYhIgWfpX/bZs2fTqFEj55hKbdq0YfXq1Zc8Z/HixdStW5eQkBAaNmzI559/nmfxZLZpCgsLy7NrFnWZufSkvZiIiIivsbRwqly5Ms899xxbtmxh8+bN3HDDDdxyyy1s37492+M3btzIwIEDueeee/j555+JiYkhJiaGbdu25Wlcao+Td5RLEREpTCwtnHr27MlNN91ErVq1qF27Ns8++yzFixfnu+++y/b4V155hW7dujFmzBjq1avHlClTaNasGbNmzcrnyEVERCTfOByU+usvq6MAfKiNU0ZGBosXLyYlJYU2bdpke8ymTZsYNWqUy7auXbuyYsWKHK+blpZGWlqa83VycjJgPjq6+PGR3W7HMAwcDgcOh8Ojz2EYhvOnp9fwxF133cUHH3wAQEBAAJUrV6Zv375MmjSJkJCQy56/Z88eatSowZYtW2jSpInLvnXr1nHjjTdy/PhxSpUq5bKvevXqPPzwwzz88MPZXtfhcGAYBna7/ZJtnDJ/F3qkl3vKneeUO88ob55T7jw0fjztXngBu2FgHzkyzy+fm9+H5YXTb7/9Rps2bTh79izFixdn+fLl1K9fP9tjExISiIiIcNkWERFBQkJCjtefNm0akyZNyrJ9zZo1WdoyBQQEEBkZyenTp0lPT/fg05x36tSpKzo/t+x2OzfeeCOvv/46drudX375hWHDhpGenp7t57/Y6dOnAUhJSXEWl5lSU1MB8zNd3ODd4XBw9uzZLOdkSk9P58yZM6xfv55z585dNo74+PjLHiPZU+48p9x5RnnznHLnvkrffkuLmTMB2HbgAPvzsG1zpsy/c+6wvHCqU6cOW7duJSkpiSVLljB48GC+/fbbHIun3Bo7dqzLXarMGZC7dOmSZZLfs2fPsn//fooXL+7WXZrsGIbBqVOnKFGiRL627wkMDKRYsWLUqlULgPr167N06VL++9//Eh4ejsPhYPr06bz99tskJCRQu3ZtnnzySfr27QtA8eLFAShWrFiWvGQWmCVKlMiyz8/Pj5CQkBwnTD579iyhoaG0a9fukjm12+3Ex8fTuXNnTXyZS8qd55Q7zyhvnlPucsf244/4v/EGAH/17k3dqVNp6IW85fQ//9mxvHAKCgqiZs2aADRv3pwff/yRV155hTfffDPLsZGRkRw+fNhl2+HDh4mMjMzx+sHBwQQHB2fZHhgYmOVLm5GRgc1mw8/P7/ydFcOAXFSiDocDUlKw+ftf+XAEYWHgZvFls9mcsQNs27aNTZs2UbVqVfz8/Jg2bRoffvghc+bMoVatWqxfv55BgwYRERFB+/btnee5fPb/d6l9me+d02f18/PDZrNlm+/suHucZKXceU6584zy5jnlzg3//gt9+kBaGo4ePfj9zjuJ9lLecnNNywunizkcDpc2SRdq06YNa9euZeQFzzfj4+NzbBOVJ1JT4f/vxrjDDyiVV+99+jQUK+b24Z9++inFixfn3LlzpKWl4efnx6xZs0hLS2Pq1Kl89dVXzlxVr16dDRs28Oabb9K+ffu8ilhEROTKpaTALbdAQgI0aEDGBx/Af/9rdVSAxYXT2LFj6d69O1WqVOHUqVPExcWxbt06vvzySwAGDRpEpUqVmDZtGgAPP/ww7du3Z8aMGfTo0YOFCxeyefNm3nrrLSs/hs/o2LEjs2fPJiUlhZkzZxIQEECfPn3Yvn07qampdO7c2eX49PR0mjZtalG0IiIi2XA4YMgQ+OknKFsWPvkESpSwOionSwunI0eOMGjQIA4dOkTJkiVp1KgRX375pfMP/L59+1weAbVt25a4uDieeuopxo0bR61atVixYgUNGjTwXpBhYeadHzc5HA6Sk5MJDw/Pm0d1uVCsWDHnY8/33nuPxo0b8+677zrz89lnn1GpUiWXc7J7jHmxzPZLSUlJWXrVJSYmUrJkyVzFKSIikqPJk2HJEggMhOXLIToafKgXoqWF07vvvnvJ/evWrcuyrV+/fvTr189LEWXDZsvV4zIcDsjIMM+xcMoVPz8/xo0bx6hRo/jzzz8JDg5m3759Hj2Wq1WrFn5+fmzZsoWqVas6t//zzz8kJSVRu3btvAxdRESKqo8/hsye4HPmwHXXWRtPNnyujZPknX79+jFmzBjefPNNHn30UR555BEcDgfXXXcdSUlJ/O9//yM8PJzBgwc7z9m5c2eW61x99dUMHTqU0aNHExAQQMOGDdm/fz+PP/4411xzDW3bts3PjyUiIoXR5s2Q+fdo9Gi4+25r48mBCqdCLCAggNjYWKZPn87u3bspV64c06ZN459//qFUqVI0a9aMcePGuZwzYMCALNfZv38/r7zyCs899xyPP/44e/fuJTIyks6dO/Pss89qWhUREbkyBw6YjcHPnoUePeD5562OKEcqnAqJefPmZbv9iSee4IknngC45Ajf0dHRzlHPczJx4kQmTpx4JWGKiIi4Sk2FmBg4eBDq14e4OLjETBNWs3SuOhERESnCDMN8JLd5M1x1ldmDLocBlX2FCicRERGxxpQpsGgRBATA0qVQvbrVEV2WCicRERHJf4sXw4QJ5vrs2VBABmNW4SQiIiL5a8uW8z3oRo6EoUMtDSc3VDiJiIhI/jl0yOxBd+YMdOsGL7xgdUS5osIpG5frXSbuUy5FRMTpzBmzB92BA1CvHixcaLZvKkBUOF0gc3bk1NRUiyMpPDJzqVnARUSKOMMwH8n98AOUKWP2oCuAU3YVrDLPy/z9/SlVqhRHjhwBICwsLNeDOzocDtLT0zl79uyVz1VXgBmGQWpqKkeOHKFUqVL4+/CYHCIikg+mTjXHaAoIMOeiq1HD6og8osLpIpGRkQDO4im3DMPgzJkzhIaGakRtoFSpUs6ciohIEbVsGTz1lLk+axZ07GhtPFdAhdNFbDYbFSpUoHz58tg9mI3Zbrezfv162rVrV+QfTwUGBupOk4hIUffzz/Cf/5jrI0bA/fdbG88VUuGUA39/f4/+6Pv7+3Pu3DlCQkKKfOEkIiJFXEIC9OplTqvSpQu89JLVEV2xotsIR0RERLzn7FmzB92//0KdOudHCC/gVDiJiIhI3jIMuPde+P57KF3a7EFXqpTVUeUJFU4iIiKSt55/Hj78EPz9zalVatWyOqI8o8JJRERE8s7KlTBunLn+2mtw443WxpPHVDiJiIhI3vjlF7jjDvNR3fDhMGyY1RHlORVOIiIicuUOHzZ70KWkQKdO8PLLVkfkFSqcRERE5MqkpcGtt8K+fWZ7po8/LhQ96LKjwklEREQ8Zxhw332wcaPZc+6TT8yedIWUCicRERHx3AsvwAcfmD3oPv7YHLOpEFPhJCIiIp755BN44glz/eWXoXNnS8PJDyqcREREJPd++w1uv918VPfAA2YvuiJAhZOIiIjkztGjZg+606fhhhvg1VfBZrM6qnyhwklERETcl9mDbs8eqFHDHBm8CE1qr8JJRERE3GMY5qCWGzZAeLjZxqlMGaujylcqnERERMQ9L70Ec+eCnx8sWgT16lkdUb5T4SQiIiKX99lnMGaMuf7SS9Ctm7XxWESFk4iIiFza9u0wcKD5qO7ee+Ghh6yOyDIqnERERCRnx45Bz55w6hS0bw+zZhWZHnTZUeEkIiIi2UtPhz59YPduqF4dli6FoCCro7KUCicRERHJyjDMQS3Xr4cSJcwedFddZXVUllPhJCIiIlm98gq88875HnT161sdkU9Q4SQiIiKuVq+G0aPN9RdfhO7drY3Hh1haOE2bNo2WLVtSokQJypcvT0xMDDt37rzkOfPmzcNms7ksISEh+RSxiIhIIffHHzBgADgccPfdMHKk1RH5FEsLp2+//Zbhw4fz3XffER8fj91up0uXLqSkpFzyvPDwcA4dOuRc9u7dm08Ri4iIFGLHj5s96JKT4frrYfbsIt2DLjsBVr75F1984fJ63rx5lC9fni1bttCuXbscz7PZbERGRno7PBERkaIjPR369oW//4boaPWgy4GlhdPFkpKSAChzmXlvTp8+TdWqVXE4HDRr1oypU6dy9dVXZ3tsWloaaWlpztfJyckA2O127HZ7HkV+XuY1vXHtwkx585xy5znlzjPKm+d8NneGgd/w4fivW4dRvDjnli2DUqXAR+L0dt5yc12bYRiGV6LIJYfDQa9evUhMTGTDhg05Hrdp0yb++usvGjVqRFJSEi+++CLr169n+/btVK5cOcvxEydOZNKkSVm2x8XFERYWlqefQUREpCCq/umnNHznHQybje/HjeNwy5ZWh5SvUlNTuf3220lKSiI8PPySx/pM4TRs2DBWr17Nhg0bsi2AcmK326lXrx4DBw5kypQpWfZnd8cpKiqKY8eOXTY5nrDb7cTHx9O5c2cCAwPz/PqFlfLmOeXOc8qdZ5Q3z/li7mxr1uDfqxc2h4OM557DMWqU1SFl4e28JScnU7ZsWbcKJ594VBcbG8unn37K+vXrc1U0AQQGBtK0aVN27dqV7f7g4GCCg4OzPc+bX1pvX7+wUt48p9x5TrnzjPLmOZ/J3Y4dcMcdZg+6u+7C/7HH8PfhxuDeylturmlprzrDMIiNjWX58uV8/fXXVKtWLdfXyMjI4LfffqNChQpeiFBERKSQOnHC7EGXlATXXacedG6y9I7T8OHDiYuLY+XKlZQoUYKEhAQASpYsSWhoKACDBg2iUqVKTJs2DYDJkydzzTXXULNmTRITE3nhhRfYu3cvQ4cOtexziIiIFCh2O/TrB7t2QdWqZg+6bJ7OSFaWFk6zZ88GoEOHDi7b586dy5AhQwDYt28ffn7nb4ydPHmSe++9l4SEBEqXLk3z5s3ZuHEj9TUUvIiIiHsefhi+/hqKFzfnoCtf3uqICgxLCyd32qWvW7fO5fXMmTOZOXOmlyISEREp5F5//fxjubg4aNjQ6ogKFM1VJyIiUlTEx5t3mwCee85s4yS5osJJRESkKPjzT7jtNsjIgEGDYMwYqyMqkFQ4iYiIFHYnT5p3lxIToU0bePNN9aDzkAonERGRwuzcOfNO059/QlQULF8OISFWR1VgqXASEREpzB55BL76CooVM3vQRURYHVGBpsJJRESksJo9G2bNMtc//BAaN7Y2nkJAhZOIiEhhtHYtjBhhrj/7LMTEWBpOYaHCSUREpLD56y9zZPCMDHMuurFjrY6o0FDhJCIiUpgkJpo96E6ehNat4Z131IMuD6lwEhERKSzOnYP+/WHnTqhcGVasUA+6PKbCSUREpLAYPRrWrIGwMFi1CiIjrY6o0FHhJCIiUhi89Ra8+qq5Pn8+NG1qbTyFlAonERGRgu6bb2D4cHN9yhS49VZr4ynEVDiJiIgUZLt2Qd++ZvumgQPhySetjqhQU+EkIiJSUCUlQa9ecOIEtGwJ776rHnRepsJJRESkIMrIgAED4I8/oFIlWLkSQkOtjqrQU+EkIiJSEI0ZA198YRZLq1ZBhQpWR1QkqHASEREpaN59F2bONNfffx+aNbM2niJEhZOIiEhBsn49DBtmrk+caE6tIvlGhZOIiEhBsXs39OkDdjvcdhuMH291REWOCicREZGCIDnZnIPu2DFo0QLmzlUPOguocBIREfF1GRlw++2wfbvZCHzFCnNaFcl3KpxERER83RNPwGefmRP2rlxpDj8gllDhJCIi4svmzYMXXzy/3rKlldEUeSqcREREfNWGDXDffeb6+PHQv7+18YgKJxEREZ+0Zw/07m32oOvbFyZMsDoiQYWTiIiI7zl16nwPumbNzEEu/fQn2xfotyAiIuJLMjLgjjtg2zaIjDQbg6sHnc9Q4SQiIuJLxo2DTz6B4GBz2IHKla2OSC6gwklERMRXfPABTJ9urr/3HrRubW08koUKJxEREV+wcSPce6+5/uST5oCX4nNUOImIiFht716zB116uvlz8mSrI5IcqHASERGx0unT0KsXHDkCjRubj+vUg85n6TcjIiJiFYcD/vMf+PVXiIiAVaugeHGro5JLUOEkIiJilaeeMnvOBQXB8uVQpYrVEcllqHASERGxwocfwrRp5vq770KbNtbGI26xtHCaNm0aLVu2pESJEpQvX56YmBh27tx52fMWL15M3bp1CQkJoWHDhnz++ef5EK2IiEjesH3/PQwdar544gm4805rAxK3WVo4ffvttwwfPpzvvvuO+Ph47HY7Xbp0ISUlJcdzNm7cyMCBA7nnnnv4+eefiYmJISYmhm3btuVj5CIiIp4JPXoU/759IS0NbrkFnn3W6pAkFwKsfPMvvvjC5fW8efMoX748W7ZsoV27dtme88orr9CtWzfGjBkDwJQpU4iPj2fWrFnMmTPH6zGLiIh4LCWFVlOnYjt8GBo2hPnz1YOugPGp31ZSUhIAZcqUyfGYTZs20alTJ5dtXbt2ZdOmTV6NTURE5Io4HPgPGUKp3bsxypc3p1UpUcLqqCSXLL3jdCGHw8HIkSO59tpradCgQY7HJSQkEBER4bItIiKChISEbI9PS0sjLS3N+To5ORkAu92O3W7Pg8hdZV7TG9cuzJQ3zyl3nlPuPKO8ecZv/Hj8V64kIyAA+0cf4V+xIiiHbvH2dy431/WZwmn48OFs27aNDRs25Ol1p02bxqRJk7JsX7NmDWFenG06Pj7ea9cuzJQ3zyl3nlPuPKO8ua/St9/SYuZMAH4ZPpz9p06BOjblmre+c6mpqW4f6xOFU2xsLJ9++inr16+n8mVmgY6MjOTw4cMu2w4fPkxkZGS2x48dO5ZRo0Y5XycnJxMVFUWXLl0IDw+/8uAvYrfbiY+Pp3PnzgQGBub59Qsr5c1zyp3nlDvPKG+5Y/vxR/zfeAMA+yOPsL99e+Uul7z9nct8GuUOSwsnwzAYMWIEy5cvZ926dVSrVu2y57Rp04a1a9cycuRI57b4+Hja5DD+RXBwMMHBwVm2BwYGevVL6+3rF1bKm+eUO88pd55R3tzw77/Qp4/Zg65nT5g6Fb78UrnzkLfylptrWlo4DR8+nLi4OFauXEmJEiWc7ZRKlixJaGgoAIMGDaJSpUpM+/9Bwh5++GHat2/PjBkz6NGjBwsXLmTz5s289dZbln0OERGRLFJSzOEGEhKgQQNYsAD8/a2OSq6Qpb3qZs+eTVJSEh06dKBChQrOZdGiRc5j9u3bx6FDh5yv27ZtS1xcHG+99RaNGzdmyZIlrFix4pINykVERPKVwwFDhsBPP0HZsupBV4hY/qjuctatW5dlW79+/ejXr58XIhIREckDkyfDkiUQGGjOQRcdbXVEkkd8ahwnERGRAu/jjyGzN/ebb8J111kbj+QpFU4iIiJ5ZfNmGDzYXB89Gu66y9p4JM+pcBIREckLBw6YjcHPnoWbboLnn7c6IvECFU4iIiJXKjUVYmLg4EGoXx8++kg96AopFU4iIiJXwjDg7rvNx3RXXWX2oPPCAMviG1Q4iYiIXIkpU2DRIggIgKVLoXp1qyMSL1LhJCIi4qnFi2HCBHN99mxo397aeMTrVDiJiIh4YsuW8z3oRo6EoUMtDUfyhwonERGR3Dp0yOxBd+YMdOsGL7xgdUSST1Q4iYiI5MaZM2bRdOAA1KsHCxea7ZukSFDhJCIi4q7MHnQ//ghlypg96EqWtDoqyUcqnERERNw1der5O0xLl0KNGlZHJPlMhZOIiIg7li2Dp54y119/HTp0sDQcsYYKJxERkcv5+Wf4z3/M9YcegvvuszYesYwKJxERkUtJSIBevcxpVbp0gRkzrI5ILKTCSUREJCdnz5pz0P37L9Spc36EcCmyVDiJiIhkxzDMQS2//x5KlzZ70JUqZXVUYjEVTiIiItl57jlYsAD8/WHJEqhVy+qIxAeocBIREbnYihUwbpy5/tprcMMNloYjvsOjwql69eocP348y/bExESqa1ZoEREpyH75Be6801x/8EEYNszaeMSneFQ47dmzh4yMjCzb09LSOHDgwBUHJSIiYonDh6FnT0hJgRtvhJdftjoi8TG56hqwatUq5/qXX35JyQuGmc/IyGDt2rVER0fnWXAiIiL5Ji0Nbr0V9u832zMtXgyBgVZHJT4mV4VTTEwMADabjcGDB7vsCwwMJDo6mhka30JERAoawzAHtdy40ew598knZk86kYvkqnByOBwAVKtWjR9//JGyZct6JSgREZF89cIL8MEHZg+6jz82x2wSyYZHo3jt3r07r+MQERGxxiefwBNPmOsvvwydO1sajvg2jwqnyZMnX3L/+PHjPQpGREQkX/32G9x+u/mo7oEHYPhwqyMSH+dR4bR8+XKX13a7nd27dxMQEECNGjVUOImIiO87csTsQXf6tDlO06uvgs1mdVTi4zwqnH7++ecs25KTkxkyZAi9e/e+4qBERES8Ki0N+vSBvXuhZk31oBO35dnI4eHh4UyaNImnn346ry4pIiKS9wzDHNRywwYoWdJs41SmjNVRSQGRp1OuJCUlkZSUlJeXFBERyVsvvQRz54Kfn9mDrm5dqyOSAsSjR3Wvvvqqy2vDMDh06BDz58+ne/fueRKYiIhInvvsMxgzxlyfORO6dLE2HilwPCqcZs6c6fLaz8+PcuXKMXjwYMaOHZsngYmIiOSp7dth4EDzUd2998KIEVZHJAWQxnESEZHC79gxswfdqVPQvj3MmqUedOKRK27jtH//fvbv358XsYiIiOS99HSzB93u3VC9OixdCkFBVkclBZRHhdO5c+d4+umnKVmyJNHR0URHR1OyZEmeeuop7HZ7XscoIiLiGcOABx+E9eshPNzsQXfVVVZHJQWYR4/qRowYwbJly5g+fTpt2rQBYNOmTUycOJHjx48ze/bsPA1SRETEIy+/DO++a/agW7gQ6te3OiIp4Dy64xQXF8e8efO4//77adSoEY0aNeL+++/n3XffJS4uzu3rrF+/np49e1KxYkVsNhsrVqy45PHr1q3DZrNlWRISEjz5GCIiUpitXg2PPmquv/giqNe35AGPCqfg4GCio6OzbK9WrRpBuXhunJKSQuPGjXn99ddz9f47d+7k0KFDzqV8+fK5Ol9ERAq5P/6AAQPA4YB77oGRI62OSAoJjx7VxcbGMmXKFObOnUtwcDAAaWlpPPvss8TGxrp9ne7du3s07lP58uUpVapUrs8TEZEi4PhxswddcjK0awdvvKEedJJnPJ6rbu3atVSuXJnGjRsD8Msvv5Cens6NN97Irbfe6jx22bJleRPpBZo0aUJaWhoNGjRg4sSJXHvttXn+HiIiUgClp0PfvvD331CtmnrQSZ7zqHAqVaoUffr0cdkWFRWVJwFdSoUKFZgzZw4tWrQgLS2Nd955hw4dOvD999/TrFmzbM9JS0sjLS3N+To5ORkAu93ulR6AmddU78LcUd48p9x5TrnzjM/mzTDwGz4c/3XrMIoX59yyZeZcdD4Up8/mzsd5O2+5ua7NMAzDK1Hkks1mY/ny5cTExOTqvPbt21OlShXmz5+f7f6JEycyadKkLNvj4uIICwvzJFQREfFB1T/9lIbvvINhs/H9uHEcbtnS6pCkgEhNTeX2228nKSmJ8PDwSx7r0R2nG264gWXLlmVpZ5ScnExMTAxff/21J5f1SKtWrdiwYUOO+8eOHcuoUaOcr5OTk4mKiqJLly6XTY4n7HY78fHxdO7cmcDAwDy/fmGlvHlOufOccucZX8ybbc0a/N97DwDHtGk0v+C/+77EF3NXEHg7b5lPo9zhUeG0bt060tPTs2w/e/Ys//3vfz25pMe2bt1KhQoVctwfHBzsbMB+ocDAQK9+ab19/cJKefOccuc55c4zPpO3HTvgjjvMHnR33YX/Y4/h7+ONwX0mdwWMt/KWm2vmqnD69ddfneu///67y/hJGRkZfPHFF1SqVMnt650+fZpdu3Y5X+/evZutW7dSpkwZqlSpwtixYzlw4AAffPABAC+//DLVqlXj6quv5uzZs7zzzjt8/fXXrFmzJjcfQ0RECosTJ8wedElJcN11MHu2etCJV+WqcGrSpIlz0Mkbbrghy/7Q0FBee+01t6+3efNmOnbs6Hyd+Uht8ODBzJs3j0OHDrFv3z7n/vT0dEaPHs2BAwcICwujUaNGfPXVVy7XEBGRIsJuh379YNcuqFrV7EGXzRMGkbyUq8Jp9+7dGIZB9erV+eGHHyhXrpxzX1BQEOXLl8ff39/t63Xo0IFLtU2fN2+ey+vHHnuMxx57LDchi4hIYfXww/D111CsGKxaBRoMWfJBrgqnqlWrAuBwOLwSjIiIiFtef/38Y7m4OGjUyOqIpIjwqHF4ZpujnAwaNMijYERERC4rPt682wQwbRr06mVtPFKkeFQ4PZz5hf1/drud1NRUgoKCCAsLU+EkIiLe8eefcNttkJEBgwaBmm9IPvNokt+TJ0+6LKdPn2bnzp1cd911fPTRR3kdo4iICJw8afagS0yENm3gzTfVg07ynUeFU3Zq1arFc889l+VulIiIyBWz2807TX/+CVWqwPLlEBJidVRSBOVZ4QQQEBDAwYMH8/KSIiIiMGoUfPXV+R50ERFWRyRFlEdtnFatWuXy2jAMDh06xKxZs7j22mvzJDAREREA5syBWbPM9Q8/hMaNrY1HijSPCqeLJ+K12WyUK1eOG264gRkzZuRFXCIiIuY4TbGx5vrUqZDLieBF8ppHhVPmOE5Hjx4FcBkIU0REJE/89Rf07Wv2oLvzTnjiCasjEsl9G6fExESGDx9O2bJliYyMJDIykrJlyxIbG0tiYqIXQhQRkSInMdHsQXfyJLRuDW+/rR504hNydcfpxIkTtGnThgMHDnDHHXdQr149wJzwd968eaxdu5aNGzdSunRprwQrIiJFwLlz0L8/7NwJlSvDihXqQSc+I1eF0+TJkwkKCuLvv/8m4qIeDZMnT6ZLly5MnjyZmTNn5mmQIiJShIweDWvWQFiY2YMuMtLqiESccvWobsWKFbz44otZiiaAyMhIpk+fzvLly/MsOBERKWLeegtefdVcnz8fmja1Nh6Ri+SqcDp06BBXX311jvsbNGhAQkLCFQclIiJF0DffwPDh5vqUKXDrrdbGI5KNXBVOZcuWZc+ePTnu3717N2XKlLnSmEREpKj5+2+zB925czBwIDz5pNURiWQrV4VT165defLJJ0lPT8+yLy0tjaeffppu3brlWXAiIlIEJCWZPehOnICWLeHdd9WDTnxWrhuHt2jRglq1ajF8+HDq1q2LYRj88ccfvPHGG6SlpTF//nxvxSoiIoVNRoZ5h+mPP6BSJVi5EkJDrY5KJEe5KpwqV67Mpk2bePDBBxk7diyGYQDmyOGdO3dm1qxZREVFeSVQEREphMaMgdWrzWJp1SqoUMHqiEQuKdcjh1erVo3Vq1dz8uRJ/vrrLwBq1qyptk0iIpI7774LmcPXvP8+NGtmbTwibvBoyhWA0qVL06pVq7yMRUREior162HYMHN94kTo18/ScETclespV0RERK7IP/+YQw3Y7XDbbTB+vNURibhNhZOIiOSf5GSzB93x49CiBcydqx50UqCocBIRkfyRkQG33w6//242Al+xwpxWRaQAUeEkIiL544kn4LPPzAl7V640hx8QKWBUOImIiPfNmwcvvnh+vWVLK6MR8ZgKJxER8a4NG+C++8z18eOhf39r4xG5AiqcRETEe3bvht69zR50ffrAhAlWRyRyRVQ4iYiId5w6Bb16wbFj0LSpOciln/7sSMGmb7CIiOS9zB5027ZBZKQ5nUqxYlZHJXLFVDiJiEjeGzcOPv0UgoPNYQcqV7Y6IpE8ocJJRETy1vvvw/Tp5vp770Hr1tbGI5KHVDiJiEje2bjxfA+6J580H9eJFCIqnEREJG/s3QsxMZCebvakmzzZ6ohE8pwKJxERuXKnT5s96I4ehSZNYP589aCTQknfahERuTIOB/znP/DrrxARYU6noh50UkipcBIRkSvz1FNmz7mgIFi+HKpUsToiEa9R4SQiIp778EOYNs1cf/ddaNPG2nhEvMzSwmn9+vX07NmTihUrYrPZWLFixWXPWbduHc2aNSM4OJiaNWsyb948r8cpIiJZ2b7/HoYONV888QTceae1AYnkA0sLp5SUFBo3bszrr7/u1vG7d++mR48edOzYka1btzJy5EiGDh3Kl19+6eVIRUTkQiFHj+Lfty+kpcEtt8Czz1odkki+CLDyzbt370737t3dPn7OnDlUq1aNGTNmAFCvXj02bNjAzJkz6dq1q7fCFBGRC6Wk0HrqVGyHD0OjRubjOvWgkyLC0sIptzZt2kSnTp1ctnXt2pWRI0fmeE5aWhppaWnO18nJyQDY7Xbsdnuex5h5TW9cuzBT3jyn3HlOufOAw4FtyBBK7d6NUa4c55YuNadVUQ7dou+cZ7ydt9xct0AVTgkJCURERLhsi4iIIDk5mTNnzhAaGprlnGnTpjFp0qQs29esWUNYWJjXYo2Pj/fatQsz5c1zyp3nlDv31V2wgDorV5IREMDGUaM4sX07bN9udVgFjr5znvFW3lJTU90+tkAVTp4YO3Yso0aNcr5OTk4mKiqKLl26EB4enufvZ7fbiY+Pp3PnzgQGBub59Qsr5c1zyp3nlLvcsS1cSMDixQD8Mnw4zR96SHnLJX3nPOPtvGU+jXJHgSqcIiMjOXz4sMu2w4cPEx4enu3dJoDg4GCCg4OzbA8MDPTql9bb1y+slDfPKXeeU+7c8MMPcO+9AGSMHs3+66+nofLmMX3nPOOtvOXmmgWqNV+bNm1Yu3aty7b4+HjaaNwQERHv+fdfs+dcWhr07InjmWesjkjEMpYWTqdPn2br1q1s3boVMIcb2Lp1K/v27QPMx2yDBv1fe/ceVWWd73H8s0EukqJRBKYoXprSClJIg3EmLW+dVg5TcewySWie9IiTUZlWipcpx8xLo4Y2k1pThmmj2OomhyO6zhF1vNDFojRLSkExExRngNj7/PGMGEe0zZbNb1/er7VY69kPz9589nchftbez+/ZI+uPHzt2rA4cOKBJkyapuLhYL730kt566y09+uijJuIDgO+rqrJKU1mZdP310htvSIGBplMBxhgtTjt37lTv3r3Vu3dvSVJmZqZ69+6tadOmSZJKS0vrS5Qkde3aVe+++67y8vIUHx+vefPm6S9/+QuXIgAAd7DbpQcflHbvliIjpQ0bpLZtTacCjDJ6jtOAAQPkcDjO+/3Grgo+YMAA7dmzx42pAACSpBkzpLVrpaAg6W9/k2JjTScCjPOqc5wAAC1k9Wpp5kxr++WXpf79zeYBPATFCQDQ0M6d1lt0kvTYY2e3AVCcAAA/ceiQdTL4P/8p3X67NGeO6USAR6E4AQAs//iHlJIiHT4sXXuttGoVK+iA/4fiBACQHA4pPd16m+6yy6wVdG74dAXA21GcAADSrFnWCeGtWklvvy1162Y6EeCRKE4A4O/WrJGysqzt7Gzp5pvN5gE8GMUJAPzZrl1SWpq1PXGi9NBDRuMAno7iBAD+qrTUWkH3j39Iw4ZJc+eaTgR4PIoTAPijMyvoDh2SevaUcnKs85sAXBDFCQD8jcMhjRol7dghRURYK+jatTOdCvAKFCcA8DfPPnv2Faa1a6UePUwnArwGxQkA/Mnf/iZNnWptL14sDRxoNg/gZShOAOAv9uyRHnjA2p4wQXr4YbN5AC9EcQIAf1BWJg0fLp0+LQ0ZIs2fbzoR4JUoTgDg6/75T+m3v5W++066+uqzVwgH0GQUJwDwZQ6HNGaMtG2bdOml0jvvSO3bm04FeC2KEwD4sjlzpNdflwIDrRV0V11lOhHg1ShOAOCrcnOlp56ythctkm65xWwewAdQnADAF330kXT//dZbdePHS+PGmU4E+ASKEwD4miNHrBV0VVXSrbdKCxaYTgT4DIoTAPiS6mrpzjulkhLrfKY1a6SgINOpAJ9BcQIAX+FwSP/xH9LWrdZnz73zjrWSDkCzoTgBgK+YO1d67TVrBd2aNdY1mwA0K4oTAPiCDRukyZOt7YULpcGDjcYBfBXFCQC83SefnF1BN3astYoOgFtQnADAmx09Kt1xh3TqlHWdpj/9SbLZTKcCfBbFCQC81ZkVdAcPSj16sIIOaAEUJwDwRg6HdVHL//3fsyvoIiJMpwJ8HsUJALzR/PnSihVSQID01lvSNdeYTgT4BYoTAHibd9+VnnjC2l6wQBoyxGwewI9QnADAm+zdK91779mLXU6YYDoR4FcoTgDgLY4ds1bQnTwp3XyztGgRK+iAFkZxAgBvUFMj3XWX9PXXUrdu0ttvS8HBplMBfofiBACezuGQ/vM/pS1bpPBwawXdZZeZTgX4JYoTAHi6hQulV16xVtDl5Ei9eplOBPgtjyhOS5YsUWxsrEJDQ9WvXz/t2LHjvMeuXLlSNputwVdoaGgLpgWAFvT++9Ljj1vbc+dKt91mNg/g54wXp9WrVyszM1NZWVnavXu34uPjNXToUB09evS89wkPD1dpaWn918GDB1swMQC0kM8+k+65R7LbpVGjpEcfNZ0I8HvGi9P8+fM1ZswYpaenq1evXlq6dKnCwsK0fPny897HZrMpOjq6/isqKqoFEwNAC/j+e2sFXWWl9KtfSdnZrKADPEArkz+8pqZGu3bt0pQpU+r3BQQEaNCgQSosLDzv/U6dOqUuXbrIbrerT58+eu6553Tttdc2emx1dbWqq6vrb1dWVkqSamtrVVtb20zP5Kwzj+mOx/ZlzM11zM51Hju7mhoF3nmnAg4ckCM2Vj/m5FilyUNyeuzcvACzc42759aUx7U5HA6HW1I44fDhw+rYsaO2bt2qpKSk+v2TJk3S5s2btX379nPuU1hYqH379ikuLk4VFRV64YUXtGXLFu3du1edOnU65/jp06drxowZ5+xftWqVwsLCmvcJAcDFcjgUn52t2I0b9WNoqLbMmaOTXbqYTgX4tNOnT+u+++5TRUWFwsPDL3is0VecXJGUlNSgZCUnJ6tnz55atmyZZs2adc7xU6ZMUWZmZv3tyspKxcTEaMiQIT87HFfU1tYqLy9PgwcPVhCfUu405uY6Zuc6T5xdwOLFCty4UQ6bTXrzTf3q9ttNRzqHJ87NWzA717h7bmfejXKG0eJ0+eWXKzAwUEeOHGmw/8iRI4qOjnbqMYKCgtS7d2/t37+/0e+HhIQoJCSk0fu585fW3Y/vq5ib65id6zxmdh9+WL+Czvb882qVkmI2z8/wmLl5IWbnGnfNrSmPafTk8ODgYCUkJCg/P79+n91uV35+foNXlS6krq5On3zyiTp06OCumADgfsXF0ogR1gq69HTpscdMJwLQCONv1WVmZiotLU2JiYnq27evFi5cqKqqKqWnp0uSRo4cqY4dO2r27NmSpJkzZ+qmm25Sjx49dOLECc2dO1cHDx7UQw89ZPJpAIDrjh+3VtBVVEj9+7OCDvBgxovTiBEjVF5ermnTpqmsrEw33HCDPvjgg/pLDJSUlCgg4OwLYz/88IPGjBmjsrIyXXrppUpISNDWrVvViyvpAvBGtbVSaqq0f7/UpYv1GXSNnF4AwDMYL06SlJGRoYyMjEa/V1BQ0OD2ggULtGDBghZIBQAt4JFHpP/+b6lNG+sz6K64wnQiABdg/AKYAOC3liw5+7bcG29I119vOhGAn0FxAgAT8vKsV5skafZsafhws3kAOIXiBAAt7csvpX//d6muTho5Upo0yXQiAE6iOAFAS/rhB2sF3YkTUlKStGwZK+gAL0JxAoCW8uOP1itNX34pde4srVsnhYaaTgWgCShOANBSHn1U+q//ki65RNqwQfrXZVcAeA+KEwC0hOxsafFia/v116X4eLN5ALiE4gQA7pafL02YYG0/95zk4Z9BB+D8KE4A4E779llXBq+rk373O2nyZNOJAFwEihMAuMuJE9YKuh9+kG66Sfrzn1lBB3g5ihMAuMOPP0ojRkhffCHFxLCCDvARFCcAcIfHHpM2bpTCwqTcXCk62nQiAM2A4gQAze3ll6U//cna/utfpd69zeYB0GwoTgDQnDZtksaPt7ZnzZLuvNNsHgDNiuIEAM1l/37p7rut85vuuUd6+mnTiQA0M4oTADSHigpp+HDp+HHpxhul5ctZQQf4IIoTAFysM68wff651LGjtH691Lq16VQA3IDiBAAXa9Ik6YMPrLKUmytdeaXpRADchOIEABfjlVekBQus7VdflRISzOYB4FYUJwBw1ZYt0rhx1vaMGdZHqwDwaRQnAHDFgQPWpQZqa60rhE+dajoRgBZAcQKApqqstD6D7vvvpcREVtABfoTiBABNUVcn3Xuv9NlnUocO1gq6sDDTqQC0EIoTADTFk09K771nfWBvbq51+QEAfoPiBADOWrFCmjfP2l650rrQJQC/QnECAGf8z/9IDz9sbU+bZp0QDsDvUJwA4Od88430299aK+juvlvKyjKdCIAhFCcAuJCTJ60VdMeOSX36WBe5DOBPJ+Cv+NcPAOdTVyfdf7/06adSdLR1Mjgr6AC/RnECgPN5+mnpnXekkBDrsgOdOplOBMAwihMANOa116Q5c6zt5culfv3M5gHgEShOAPD/FRZKY8ZY208/Ld13n9k8ADwGxQkAfurgQSklRaqpsVbSzZxpOhEAD0JxAoAzTp2Shg+Xjh6V4uOtt+tYQQfgJ/iLAACSZLdLDzwgffyxFBUlbdggtWljOhUAD0NxAgBJeuYZa+VccLC0bp3UubPpRAA8kEcUpyVLlig2NlahoaHq16+fduzYccHj16xZo2uuuUahoaG6/vrr9d5777VQUgC+yPb669Ls2daNV16RkpLMBgLgsYwXp9WrVyszM1NZWVnavXu34uPjNXToUB09erTR47du3ap7771Xo0eP1p49e5SSkqKUlBR9+umnLZwcgNc7ckS9X3xRrUaNsm5Pniz97ndmMwHwaMaL0/z58zVmzBilp6erV69eWrp0qcLCwrR8+fJGj3/xxRc1bNgwPfHEE+rZs6dmzZqlPn36aPHixS2cHIDX+vFHafFitbruOnXetMnaN3689OyzZnMB8HitTP7wmpoa7dq1S1OmTKnfFxAQoEGDBqmwsLDR+xQWFiozM7PBvqFDh2r9+vXujOqcsjIFvPSSrt63TwG7dkmBgaYTeY2Aujrm5iJm10QOh/XRKUVFskk60a2b2rz6qlr17286GQAvYLQ4HTt2THV1dYqKimqwPyoqSsXFxY3ep6ysrNHjy8rKGj2+urpa1dXV9bcrKyslSbW1taqtrb2Y+Of69lsFzZqla5r3Uf1CoMTcXMTsXONo316106drc0yMBvfpI0dz/z3wYWf+djb731A/wOxc4+65NeVxjRanljB79mzNmDHjnP0bN25UWDN/WGdoebl+MWxYsz4mgOZX3a6dvv63f1NNu3aSpLy8PMOJvBNzcx2zc4275nb69GmnjzVanC6//HIFBgbqyJEjDfYfOXJE0dHRjd4nOjq6ScdPmTKlwVt7lZWViomJ0ZAhQxQeHn6Rz+Bctffdp7y8PA0ePFhBQUHN/vi+qra2lrm5iNm5pruYnauYm+uYnWvcPbcz70Y5w2hxCg4OVkJCgvLz85WSkiJJstvtys/PV0ZGRqP3SUpKUn5+viZOnFi/Ly8vT0nnWT4cEhKikJCQc/YHBQW59ZfW3Y/vq5ib65id65ida5ib65ida9w1t6Y8pvG36jIzM5WWlqbExET17dtXCxcuVFVVldLT0yVJI0eOVMeOHTX7X9dYeeSRR3TzzTdr3rx5uv3225WTk6OdO3fq5ZdfNvk0AACAHzBenEaMGKHy8nJNmzZNZWVluuGGG/TBBx/UnwBeUlKigJ98VlRycrJWrVqlZ555Rk899ZSuuuoqrV+/Xtddd52ppwAAAPyE8eIkSRkZGed9a66goOCcfampqUpNTXVzKgAAgIaMXwATAADAW1CcAAAAnERxAgAAcBLFCQAAwEkUJwAAACdRnAAAAJxEcQIAAHCSR1zHqSU5HA5JTftcmqaora3V6dOnVVlZyeX0m4C5uY7ZuY7ZuYa5uY7ZucbdczvTCc50hAvxu+J08uRJSVJMTIzhJAAAwJOcPHlS7dq1u+AxNocz9cqH2O12HT58WG3btpXNZmv2x6+srFRMTIy+/fZbhYeHN/vj+yrm5jpm5zpm5xrm5jpm5xp3z83hcOjkyZO68sorG3zMW2P87hWngIAAderUye0/Jzw8nH8ULmBurmN2rmN2rmFurmN2rnHn3H7ulaYzODkcAADASRQnAAAAJ1GcmllISIiysrIUEhJiOopXYW6uY3auY3auYW6uY3au8aS5+d3J4QAAAK7iFScAAAAnUZwAAACcRHECAABwEsXJjYYPH67OnTsrNDRUHTp00AMPPKDDhw+bjuXxvvnmG40ePVpdu3ZV69at1b17d2VlZammpsZ0NI/37LPPKjk5WWFhYWrfvr3pOB5tyZIlio2NVWhoqPr166cdO3aYjuTxtmzZojvuuENXXnmlbDab1q9fbzqSV5g9e7ZuvPFGtW3bVldccYVSUlL0xRdfmI7lFbKzsxUXF1d//aakpCS9//77RjNRnNxo4MCBeuutt/TFF1/o7bff1ldffaW7777bdCyPV1xcLLvdrmXLlmnv3r1asGCBli5dqqeeesp0NI9XU1Oj1NRUjRs3znQUj7Z69WplZmYqKytLu3fvVnx8vIYOHaqjR4+ajubRqqqqFB8fryVLlpiO4lU2b96s8ePHa9u2bcrLy1Ntba2GDBmiqqoq09E8XqdOnfTHP/5Ru3bt0s6dO3XLLbfoN7/5jfbu3WssE6vqWtCGDRuUkpKi6upqPtyxiebOnavs7GwdOHDAdBSvsHLlSk2cOFEnTpwwHcUj9evXTzfeeKMWL14syfooppiYGE2YMEGTJ082nM472Gw2rVu3TikpKaajeJ3y8nJdccUV2rx5s37961+bjuN1IiIiNHfuXI0ePdrIz+cVpxZy/PhxvfHGG0pOTqY0uaCiokIRERGmY8AH1NTUaNeuXRo0aFD9voCAAA0aNEiFhYUGk8FfVFRUSBJ/05qorq5OOTk5qqqqUlJSkrEcFCc3e/LJJ3XJJZfosssuU0lJiXJzc01H8jr79+/XokWL9PDDD5uOAh9w7Ngx1dXVKSoqqsH+qKgolZWVGUoFf2G32zVx4kT98pe/1HXXXWc6jlf45JNP1KZNG4WEhGjs2LFat26devXqZSwPxamJJk+eLJvNdsGv4uLi+uOfeOIJ7dmzRxs3blRgYKBGjhwpf313tKmzk6RDhw5p2LBhSk1N1ZgxYwwlN8uVuQHwTOPHj9enn36qnJwc01G8xtVXX62ioiJt375d48aNU1pamj777DNjeTjHqYnKy8v1/fffX/CYbt26KTg4+Jz93333nWJiYrR161ajLzOa0tTZHT58WAMGDNBNN92klStXKiDAP3u+K79znON0fjU1NQoLC9PatWsbnJ+TlpamEydO8KqwkzjHqekyMjKUm5urLVu2qGvXrqbjeK1Bgwape/fuWrZsmZGf38rIT/VikZGRioyMdOm+drtdklRdXd2ckbxGU2Z36NAhDRw4UAkJCVqxYoXflibp4n7ncK7g4GAlJCQoPz+//j99u92u/Px8ZWRkmA0Hn+RwODRhwgStW7dOBQUFlKaLZLfbjf4/SnFyk+3bt+vvf/+7+vfvr0svvVRfffWVpk6dqu7du/vlq01NcejQIQ0YMEBdunTRCy+8oPLy8vrvRUdHG0zm+UpKSnT8+HGVlJSorq5ORUVFkqQePXqoTZs2ZsN5kMzMTKWlpSkxMVF9+/bVwoULVVVVpfT0dNPRPNqpU6e0f//++ttff/21ioqKFBERoc6dOxtM5tnGjx+vVatWKTc3V23btq0/l65du3Zq3bq14XSebcqUKbrtttvUuXNnnTx5UqtWrVJBQYE+/PBDc6EccIuPP/7YMXDgQEdERIQjJCTEERsb6xg7dqzju+++Mx3N461YscIhqdEvXFhaWlqjc9u0aZPpaB5n0aJFjs6dOzuCg4Mdffv2dWzbts10JI+3adOmRn+/0tLSTEfzaOf7e7ZixQrT0TzeqFGjHF26dHEEBwc7IiMjHbfeeqtj48aNRjNxjhMAAICT/PfEEQAAgCaiOAEAADiJ4gQAAOAkihMAAICTKE4AAABOojgBAAA4ieIEAADgJIoTAACAkyhOAAAATqI4AfBaDz74YP0H9baUlStXqn379i36MwF4DooTAACAkyhOAHzCgAED9Pvf/16TJk1SRESEoqOjNX369AbH2Gw2ZWdn67bbblPr1q3VrVs3rV27tv77BQUFstlsOnHiRP2+oqIi2Ww2ffPNNyooKFB6eroqKipks9lks9nO+RkAfBvFCYDPePXVV3XJJZdo+/btev755zVz5kzl5eU1OGbq1Km666679NFHH+n+++/XPffco88//9ypx09OTtbChQsVHh6u0tJSlZaW6vHHH3fHUwHgoShOAHxGXFycsrKydNVVV2nkyJFKTExUfn5+g2NSU1P10EMP6Re/+IVmzZqlxMRELVq0yKnHDw4OVrt27WSz2RQdHa3o6Gi1adPGHU8FgIeiOAHwGXFxcQ1ud+jQQUePHm2wLykp6Zzbzr7iBAAUJwA+IygoqMFtm80mu93u9P0DAqw/iQ6Ho35fbW1t84QD4BMoTgD8yrZt28653bNnT0lSZGSkJKm0tLT++0VFRQ2ODw4OVl1dnXtDAvBYFCcAfmXNmjVavny5vvzyS2VlZWnHjh3KyMiQJPXo0UMxMTGaPn269u3bp3fffVfz5s1rcP/Y2FidOnVK+fn5OnbsmE6fPm3iaQAwhOIEwK/MmDFDOTk5iouL02uvvaY333xTvXr1kmS91ffmm2+quLhYcXFxmjNnjv7whz80uH9ycrLGjh2rESNGKDIyUs8//7yJpwHAEJvjp2/mA4APs9lsWrduXYtfbRyA7+AVJwAAACdRnAAAAJzUynQAAGgpnJkA4GLxihMAAICTKE4AAABOojgBAAA4ieIEAADgJIoTAACAkyhOAAAATqI4AQAAOIniBAAA4CSKEwAAgJP+D+z1L4iv5xCgAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "smooth cureve can lead to better optimization properties during training\n",
        "\n",
        "relu not differentiable at 0 ,but gelu is\n",
        "\n",
        "gelu is not zero for negative value"
      ],
      "metadata": {
        "id": "JyvINBVeoUZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "role of feedforwad network- the self attetnion mechanism only see how much importance should be given to each word . and the dimensions are fix\n",
        "\n",
        "but when we add feedforward neural network more neurons are added\n",
        "\n",
        "so we had input dim of 768 and we are projecting the imput into 4 times larger dimension(4 neurons or feedforwad nn layer) it will help the transformer to explroe more in this 4 times bigger dimension"
      ],
      "metadata": {
        "id": "R6R6qYvbpu56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FeedForward n//n implemnt Part 3"
      ],
      "metadata": {
        "id": "9So8Rk_TsZ8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),  #expand dim to explore more\n",
        "        GELU(),    #activation\n",
        "        nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])    #contraction of dim to orginal dim of output\n",
        "    )\n",
        "\n",
        "  def forward(self,X):\n",
        "      return self.layers(X)"
      ],
      "metadata": {
        "id": "SUC91QSDoMfB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(GPT_config_124[\"emb_dim\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LxhgQW8tAND",
        "outputId": "f289d52a-db27-42c7-dc89-e3b8f0904b30"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test ff nn\n"
      ],
      "metadata": {
        "id": "d4VfmwAPtzHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedForward(GPT_config_124)\n",
        "X = torch.rand(2,4,768)\n",
        "out = ffn(X)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbRatahZtCto",
        "outputId": "d2fc9eab-a6c3-4efe-d5a5-9dbbfa052d1a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHORTCUT CONNECTIONS PART 4**"
      ],
      "metadata": {
        "id": "PZr-n9A712ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self, layer_sizes, use_shortcut):\n",
        "        super().__init__()\n",
        "        self.use_shortcut = use_shortcut\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[1]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU())\n",
        "        ])\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            layer_output = layer(X)\n",
        "\n",
        "            # Applying shortcut connection\n",
        "            if self.use_shortcut and X.shape == layer_output.shape:\n",
        "                X = X + layer_output\n",
        "            else:\n",
        "                X = layer_output\n",
        "\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "-9aJvDT7t2No"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [3,3,3,3,3,1]\n",
        "sample_input = torch.tensor([[1.,0,-1]])\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
        "    layer_sizes ,use_shortcut = False\n",
        ")"
      ],
      "metadata": {
        "id": "v2XdSEox4Du8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def print_gradient(model, X):\n",
        "    output = model(X)\n",
        "    target = torch.tensor([[0.0]])\n",
        "\n",
        "    # Calculate loss based on how close the target is\n",
        "    loss_fn = nn.MSELoss()\n",
        "    loss = loss_fn(output, target)\n",
        "\n",
        "    # Backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Print the gradients of the weights\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name and param.grad is not None:\n",
        "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n"
      ],
      "metadata": {
        "id": "L0iCg1Ep4zyv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_gradient(model_without_shortcut,sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKA7qL0H7I1J",
        "outputId": "3846e4d2-1d2d-445a-da91-109a62a283a7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 1.849435648182407e-05\n",
            "layers.1.0.weight has gradient mean of 3.1618601497029886e-05\n",
            "layers.2.0.weight has gradient mean of 0.00016230760957114398\n",
            "layers.3.0.weight has gradient mean of 0.0005445715505629778\n",
            "layers.4.0.weight has gradient mean of 0.0014962060377001762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see the layer 0\n",
        " is facing vanishsing gradient problem"
      ],
      "metadata": {
        "id": "hjOoj5Va7wHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
        "    layer_sizes, use_shortcut = True\n",
        ")\n",
        "print_gradient(model_with_shortcut,sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye9If5wR7ZD0",
        "outputId": "15055aba-40cf-40f7-c0d4-2157a0c9d247"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 2.8950865268707275\n",
            "layers.1.0.weight has gradient mean of 3.535364866256714\n",
            "layers.2.0.weight has gradient mean of 2.278616189956665\n",
            "layers.3.0.weight has gradient mean of 1.619220495223999\n",
            "layers.4.0.weight has gradient mean of 1.509148359298706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see we have added skipped connection not not facing vanishing gradient problem\n"
      ],
      "metadata": {
        "id": "66uxiq_N8ycG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dw6QDknV8wRO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE ENTIRE GPT2: PART 5"
      ],
      "metadata": {
        "id": "TdisuFNIBsTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "Y3LFFtUhBvFm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self,X):\n",
        "    mean = X.mean(dim = -1,keepdim = True)\n",
        "    var = X.var(dim = -1,keepdim =True,unbiased = False)\n",
        "    norm_X = (X-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*norm_X + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,X):\n",
        "    return 0.5*X*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(X+0.044715*torch.pow(X,3))))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),  #expand dim to explore more\n",
        "        GELU(),    #activation\n",
        "        nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])    #contraction of dim to orginal dim of output\n",
        "    )\n",
        "\n",
        "  def forward(self,X):\n",
        "      return self.layers(X)"
      ],
      "metadata": {
        "id": "fDZAt5zlCBt4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's code transformer block**\n",
        "\n",
        "st1:shortcut connection   \n",
        "\n",
        "st2: shprtcut connection for feed forward block\n",
        "\n",
        "st3: add orignal input back\n"
      ],
      "metadata": {
        "id": "398XnvCUCOXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "FFf0iJ3SCJcR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(2,4,768)\n",
        "\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "out = block(X)\n",
        "print(\"input shape:\",X.shape)\n",
        "print(\"Output shape\",out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfvaa8m3FC1f",
        "outputId": "d1592600-65f8-4ac3-aa7a-c3dd2ee58568"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([2, 4, 768])\n",
            "Output shape torch.Size([2, 4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our **Transformer** block is workin completely fine we can see it's preserving the dimensions"
      ],
      "metadata": {
        "id": "EWMt9p3WH-6n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IcVxAyBFUJK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement complete gpt2**"
      ],
      "metadata": {
        "id": "A_uS471qXqzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "UnxEIFfiXyRz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk-Ybw0zc8c1",
        "outputId": "0befc2d9-b7cc-4ac9-e87e-bfa39ddc5682"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgc1oYRVaV_F",
        "outputId": "966e523e-f288-4c11-9547-f4a751feff5e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.0072, -0.2137, -0.3467,  ..., -0.3240, -0.2338, -0.1070],\n",
            "         [ 0.7063, -0.7429, -0.6645,  ..., -0.6530, -0.1393, -0.2166],\n",
            "         [ 0.8226, -0.2982, -0.4547,  ...,  0.0913, -0.6949, -0.2091],\n",
            "         [-0.3933,  0.3074, -0.1343,  ...,  1.0464,  0.4620, -0.5298]],\n",
            "\n",
            "        [[ 0.1386, -0.4081, -0.1563,  ..., -0.0892, -0.0672, -0.0157],\n",
            "         [ 0.2353, -0.1177, -0.1307,  ...,  1.0729, -0.3517,  0.3905],\n",
            "         [ 0.7367,  0.3377, -0.4311,  ...,  0.8471,  0.2219, -0.2541],\n",
            "         [ 0.0166, -0.0721,  0.3451,  ...,  1.1352, -0.4069,  0.0309]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VYzhr7eatnX",
        "outputId": "43f966f8-dbb3-40c0-92db-87254f11f93c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163,009,536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "actual gpt2 had 124m parameters bt this is having more than 163m parameters it's becuse of weight tying let's understand it with code"
      ],
      "metadata": {
        "id": "1gdv7oHtiOje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p1gD1_ph2kD",
        "outputId": "54663bfa-bda0-453a-fc47-d3113a5522f1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The token embedding and output layers are very large due to number of rows for the 50257 in the tokenizer's vocab, Let's remove the output layer parameter count from gpt-2 model count accoring to weight tying"
      ],
      "metadata": {
        "id": "nn4sw7AyjIIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ9k2Y5pit_1",
        "outputId": "45a8e79d-afd1-48ca-fd85-c912115c05f1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters considering weight tying: 124,412,160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "weight tying reduces memory footprint and computational complexity and time of the model. But to get better training and result's it's better not to resue the parameters"
      ],
      "metadata": {
        "id": "je_KVmzyjpkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's calculate memory req of 163 million parameters in ut GptModel object"
      ],
      "metadata": {
        "id": "Z1GCv1FPkBah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params *4\n",
        "total_size_mb = total_size_bytes/(1024*1024)\n",
        "\n",
        "print(f\"Total size of model :,{total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfjJTNn9ja2T",
        "outputId": "a3292b27-b6aa-4bfd-f53c-58da2faa647d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of model :,621.83 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATE TEXT FROM OUTPUT TOKENS"
      ],
      "metadata": {
        "id": "7zfmor7VprDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Step 1: idx is a (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "Step 2: Crop current context if it exceeds the supported context size E.g., if LLM supports only 5 tokens, and the\n",
        "context size is 10 then only the last 5 tokens are used as context\n",
        "\n",
        "Step 3: Focus only on the last time step, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "\n",
        "Step 4: probas has shape (batch, vocab_size)\n",
        "\n",
        "Step 5: idx_next has shape (batch, 1)\n",
        "\n",
        "Step 6: Append sampled index to the running sequence, where idx has shape (batch, n_tokens+1)\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "sJa9GAVVp1ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx,max_new_tokens,context_size):\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "\n",
        "    #crop current context if it exceeds the supported context size\n",
        "    idx_cond = idx[:,-context_size:]\n",
        "\n",
        "    #get the predictions\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)   #batch ,n_tokens,vocab_size\n",
        "\n",
        "    #focus only on the last time step\n",
        "    logits = logits[:,-1,:]\n",
        "\n",
        "    #apply softmax to get probabilities\n",
        "    probas = torch.softmax(logits ,dim = -1)\n",
        "\n",
        "\n",
        "    #get the idx of the voab entry wit the highest probability value\n",
        "    idx_next = torch.argmax(probas,dim =-1,keepdim =True)\n",
        "\n",
        "    #append sampled index to runnning sequence\n",
        "    idx = torch.cat((idx,idx_next),dim = 1)\n",
        "\n",
        "  return idx"
      ],
      "metadata": {
        "id": "iJJn2fyWkjqA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test our generate text simple"
      ],
      "metadata": {
        "id": "2bLoI_TAvG5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\",encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encode_tensor.shape:\",encoded_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ1J4KvisOTC",
        "outputId": "5491a200-7797-4284-fc0c-b71f896f532c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: [15496, 11, 314, 716]\n",
            "encode_tensor.shape: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "out = generate_text_simple(\n",
        "    model = model,\n",
        "    idx =encoded_tensor,\n",
        "    max_new_tokens = 6,\n",
        "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output:\",out)\n",
        "print(\"Output length:\",len(out[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfK-nRvZvcBG",
        "outputId": "1ff3a1e8-fc44-4bca-c012-4ad5762435b5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[15496,    11,   314,   716, 27018,  7283, 46275, 41426, 33167, 33239]])\n",
            "Output length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see 6 new tokens have been added because of generate text function"
      ],
      "metadata": {
        "id": "lluqazHRwHq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use .decoder method of tokenzer ,we can convert IDS back into text"
      ],
      "metadata": {
        "id": "RdMO6hj0wODH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uevG3WtMv_Fj",
        "outputId": "65e5383e-df20-48af-c45b-6527fc023908"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am Feature IT snowballProtect youngstersMu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**result of model is not good because the weights are just random and it needs training now**"
      ],
      "metadata": {
        "id": "p7bJkJJVwlir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **using GPT to generate text**"
      ],
      "metadata": {
        "id": "D0_p583GzDZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4suENHwAweHb",
        "outputId": "48af00d8-7b02-4828-976e-591faff2ae1d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we reduce context length only to 256 while orginal gpt-2 had 1024 tokens"
      ],
      "metadata": {
        "id": "2B_hMM5X4hem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text,tokenizer):\n",
        "  encoded = tokenizer.encode(text,allowed_special = {\"<|endoftext|>\"})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids,tokenizer):\n",
        "  flat = token_ids.squeeze(0)\n",
        "  return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model = model,\n",
        "    idx  = text_to_token_ids(start_context,tokenizer),\n",
        "    max_new_tokens = 6,\n",
        "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text \\n\",token_ids_to_text(token_ids,tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_okqRCND4Vz5",
        "outputId": "60e952ff-5caa-447d-b332-1622a7c6c59b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text \n",
            " Every effort moves youaunder STUD How Pwr respawn ONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evertything working fine just need training becuase it's not giving good result"
      ],
      "metadata": {
        "id": "UJlFe9QR6ZyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For model training we need loss function"
      ],
      "metadata": {
        "id": "tFbXayfB6fsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[16833,3626,6100],    #[\"every effort moves\"]\n",
        "                       [40,1107,588]])       #[\"I really like\"]\n",
        "\n",
        "targets = torch.tensor([[3626,6100,345],    #[\"effort mvoes you\"]\n",
        "                        [1107,588,11311]])  #[\"really like choclate\"]\n"
      ],
      "metadata": {
        "id": "GM0OJI4i6Rr9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits ,dim = -1)    #probab of each token in vocab\n",
        "print(probas.shape)    #shape batch,num_token,vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UD7qp477BKO",
        "outputId": "bd1a0d1f-eb36-49d7-90b2-8ad89dbb3812"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L-16PlnKICHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = torch.argmax(probas,dim =-1,keepdim = True)\n",
        "print(\"Token IDs:\\n\",token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlqkbLXI7dgN",
        "outputId": "ec66f6f0-c76a-4bcd-8fc5-28083d5a587b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[[45502],\n",
            "         [35535],\n",
            "         [16263]],\n",
            "\n",
            "        [[ 3693],\n",
            "         [38945],\n",
            "         [49908]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ts0kTVqGIbeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEcode token ides** and it's not preoducing good results, so to train we need to find how much wrong answer it's giving or far far it is from correct predictions"
      ],
      "metadata": {
        "id": "P4IiS1GBIW27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Targets batch 1:,{token_ids_to_text(targets[0],tokenizer)}\")\n",
        "print(f\"Outputs batch 1:,{token_ids_to_text(token_ids[0].flatten(),tokenizer)}\")"
      ],
      "metadata": {
        "id": "akLLFoAYISsk",
        "outputId": "bb94520b-1c05-4ef4-98af-0da7e8f03d60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets batch 1:, effort moves you\n",
            "Outputs batch 1:, Garage Foley Obviously\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****let's calculate cross entorpy****\n",
        "\n",
        "s1: logits\n",
        "\n",
        "s2:probabilities(will use sfotmax+argmax)\n",
        "\n",
        "s3:Target probabilites\n",
        "\n",
        "\n",
        "s4:log probabilities\n",
        "\n",
        "s5:Avg log probabiliti\n",
        "\n",
        "\n",
        "s6:negative avg log probab"
      ],
      "metadata": {
        "id": "Xh9vi6bnJJOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
        "print(\"Text 1:\",target_probas_1)\n",
        "\n",
        "text_idx =1\n",
        "target_probas_2 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
        "\n",
        "print(\"Text 2:\",target_probas_2)"
      ],
      "metadata": {
        "id": "onYjTuZEI4Sz",
        "outputId": "edb00982-88d7-4604-861e-3b769006cfc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: tensor([1.5088e-05, 2.7362e-05, 3.6141e-05])\n",
            "Text 2: tensor([2.4204e-05, 8.1310e-06, 1.8705e-05])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #compute log of all tokens\n",
        "log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
        "print(log_probas)\n",
        "\n",
        "\n",
        "\n",
        "#ccompute avg. log probab\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)"
      ],
      "metadata": {
        "id": "a3Tkk-MHLc5y",
        "outputId": "cf487043-8524-4365-9958-7faff11da578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-11.1016, -10.5063, -10.2281, -10.6290, -11.7198, -10.8867])\n",
            "tensor(-10.8453)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_avg_log_probas = avg_log_probas*1\n",
        "print(neg_avg_log_probas)"
      ],
      "metadata": {
        "id": "DhfMcA18MkAm",
        "outputId": "26c22a70-baee-4b1b-fd55-640288807cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-10.8453)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "succesfully implmeneted this,now we'll use pytorch cross entorypy"
      ],
      "metadata": {
        "id": "G7h_ukp_NHqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits_flat = logits.flatten(0,1)\n",
        "target_flat = targets.flatten()\n",
        "\n",
        "print(\"Falttened logits:\",logits_flat.shape)\n",
        "print(\"Flattened logits:\",target_flat.shape)"
      ],
      "metadata": {
        "id": "UIQLI11fM7VS",
        "outputId": "b950cf96-590e-4937-b22b-5285c6d200f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Falttened logits: torch.Size([6, 50257])\n",
            "Flattened logits: torch.Size([6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.functional.cross_entropy(logits_flat,target_flat)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "nYZq8FOlNgw8",
        "outputId": "8b800e3a-6415-4f21-b0e2-1cac97687491",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.8453)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Perplexity** little modifcation of cross entory used in llms perplexity=exponent(cross_entorypy_loss)  \n",
        "\n",
        "so if our loss is 48725 means  we had total 50527 toekns now our llm have to pick token randomly from 48725 which is very bad if it's 2 then it's really good our model is accurate"
      ],
      "metadata": {
        "id": "frUDRUP4N46r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = torch.exp(loss)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "id": "edk7ehd9NqJk",
        "outputId": "661a9720-931c-410b-80a7-2cee2aa2cae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(51290.8789)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We'll use smaller dataset because it'll take time to train"
      ],
      "metadata": {
        "id": "IF8z7g4naaWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "  with urllib.request.urlopen(url) as response:\n",
        "    text_data = response.read().decode(\"utf-8\")\n",
        "\n",
        "  with open(file_path,\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text_data)\n",
        "else:\n",
        "  with open(file_path,\"r\",encoding=\"utf-8\") as f:\n",
        "    text_data = f.read()\n"
      ],
      "metadata": {
        "id": "A7PIlXcrO5Hh"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([text_data[:99]])"
      ],
      "metadata": {
        "id": "WSZJVqn2bOMe",
        "outputId": "e3d1c511-3e16-48e4-8ada-de83063e4144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html\\n  lang=\"en\"\\n  \\n  data-color-mode=\"auto\" data-light-theme=\"light\" data-d']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = \"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
        "\n",
        "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
        "\n",
        "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
        "\n",
        "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n",
        "\n",
        "Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the brush into his hand again--what a vocation for a wife! But Mrs. Gisburn appeared to have disdained it--and I felt it might be interesting to find out why.\n",
        "\n",
        "The desultory life of the Riviera lends itself to such purely academic speculations; and having, on my way to Monte Carlo, caught a glimpse of Jack's balustraded terraces between the pines, I had myself borne thither the next day.\n",
        "\n",
        "I found the couple at tea beneath their palm-trees; and Mrs. Gisburn's welcome was so genial that, in the ensuing weeks, I claimed it frequently. It was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her so. For Jack, all his life, had been surrounded by interesting women: they had fostered his art, it had been reared in the hot-house of their adulation. And it was therefore instructive to note what effect the \"deadening atmosphere of mediocrity\" (I quote Miss Croft) was having on him.\n",
        "\n",
        "I have mentioned that Mrs. Gisburn was rich; and it was immediately perceptible that her husband was extracting from this circumstance a delicate but substantial satisfaction. It is, as a rule, the people who scorn money who get most out of it; and Jack's elegant disdain of his wife's big balance enabled him, with an appearance of perfect good-breeding, to transmute it into objects of art and luxury. To the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that bespoke the amplest resources.\n",
        "\n",
        "\"Money's only excuse is to put beauty into circulation,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn, beaming on him, added for my enlightenment: \"Jack is so morbidly sensitive to every form of beauty.\"\n",
        "\n",
        "Poor Jack! It had always been his fate to have women say such things of him: the fact should be set down in extenuation. What struck me now was that, for the first time, he resented the tone. I had seen him, so often, basking under similar tributes--was it the conjugal note that robbed them of their savour? No--for, oddly enough, it became apparent that he was fond of Mrs. Gisburn--fond enough not to see her absurdity. It was his own absurdity he seemed to be wincing under--his own attitude as an object for garlands and incense.\n",
        "\n",
        "\"My dear, since I've chucked painting people don't say that stuff about me--they say it about Victor Grindle,\" was his only protest, as he rose from the table and strolled out onto the sunlit terrace.\n",
        "\n",
        "I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger artist was said to have formed himself at my friend's feet, and I wondered if a tinge of jealousy underlay the latter's mysterious abdication. But no--for it was not till after that event that the _rose Dubarry_ drawing-rooms had begun to display their \"Grindles.\"\n",
        "\n",
        "I turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in the dining-room.\n",
        "\n",
        "\"Why _has_ he chucked painting?\" I asked abruptly.\n",
        "\n",
        "She raised her eyebrows with a hint of good-humoured surprise.\n",
        "\n",
        "\"Oh, he doesn't _have_ to now, you know; and I want him to enjoy himself,\" she said quite simply.\n",
        "\n",
        "I looked about the spacious white-panelled room, with its _famille-verte_ vases repeating the tones of the pale damask curtains, and its eighteenth-century pastels in delicate faded frames.\n",
        "\n",
        "\"Has he chucked his pictures too? I haven't seen a single one in the house.\"\n",
        "\n",
        "A slight shade of constraint crossed Mrs. Gisburn's open countenance. \"It's his ridiculous modesty, you know. He says they're not fit to have about; he's sent them all away except one--my portrait--and that I have to keep upstairs.\"\n",
        "\n",
        "His ridiculous modesty--Jack's modesty about his pictures? My curiosity was growing like the bean-stalk. I said persuasively to my hostess: \"I must really see your portrait, you know.\"\n",
        "\n",
        "She glanced out almost timorously at the terrace where her husband, lounging in a hooded chair, had lit a cigar and drawn the Russian deerhound's head between his knees.\n",
        "\n",
        "\"Well, come while he's not looking,\" she said, with a laugh that tried to hide her nervousness; and I followed her between the marble Emperors of the hall, and up the wide stairs with terra-cotta nymphs poised among flowers at each landing.\n",
        "\n",
        "In the dimmest corner of her boudoir, amid a profusion of delicate and distinguished objects, hung one of the familiar oval canvases, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's past!\n",
        "\n",
        "Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink azaleas, pushed an arm-chair away, and said: \"If you stand here you can just manage to see it. I had it over the mantel-piece, but he wouldn't let it stay.\"\n",
        "\n",
        "Yes--I could just manage to see it--the first portrait of Jack's I had ever had to strain my eyes over! Usually they had the place of honour--say the central panel in a pale yellow or _rose Dubarry_ drawing-room, or a monumental easel placed so that it took the light through curtains of old Venetian point. The more modest place became the picture better; yet, as my eyes grew accustomed to the half-light, all the characteristic qualities came out--all the hesitations disguised as audacities, the tricks of prestidigitation by which, with such consummate skill, he managed to divert attention from the real business of the picture to some pretty irrelevance of detail. Mrs. Gisburn, presenting a neutral surface to work on--forming, as it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was one of Jack's \"strongest,\" as his admirers would have put it--it represented, on his part, a swelling of muscles, a congesting of veins, a balancing, straddling and straining, that reminded one of the circus-clown's ironic efforts to lift a feather. It met, in short, at every point the demand of lovely woman to be painted \"strongly\" because she was tired of being painted \"sweetly\"--and yet not to lose an atom of the sweetness.\n",
        "\n",
        "\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride. \"The last but one,\" she corrected herself--\"but the other doesn't count, because he destroyed it.\"\n",
        "\n",
        "\"Destroyed it?\" I was about to follow up this clue when I heard a footstep and saw Jack himself on the threshold.\n",
        "\n",
        "As he stood there, his hands in the pockets of his velveteen coat, the thin brown waves of hair pushed back from his white forehead, his lean sunburnt cheeks furrowed by a smile that lifted the tips of a self-confident moustache, I felt to what a degree he had the same quality as his pictures--the quality of looking cleverer than he was.\n",
        "\n",
        "His wife glanced at him deprecatingly, but his eyes travelled past her to the portrait.\n",
        "\n",
        "\"Mr. Rickham wanted to see it,\" she began, as if excusing herself. He shrugged his shoulders, still smiling.\n",
        "\n",
        "\"Oh, Rickham found me out long ago,\" he said lightly; then, passing his arm through mine: \"Come and see the rest of the house.\"\n",
        "\n",
        "He showed it to me with a kind of naive suburban pride: the bath-rooms, the speaking-tubes, the dress-closets, the trouser-presses--all the complex simplifications of the millionaire's domestic economy. And whenever my wonder paid the expected tribute he said, throwing out his chest a little: \"Yes, I really don't see how people manage to live without that.\"\n",
        "\n",
        "Well--it was just the end one might have foreseen for him. Only he was, through it all and in spite of it all--as he had been through, and in spite of, his pictures--so handsome, so charming, so disarming, that one longed to cry out: \"Be dissatisfied with your leisure!\" as once one had longed to say: \"Be dissatisfied with your work!\"\n",
        "\n",
        "But, with the cry on my lips, my diagnosis suffered an unexpected check.\n",
        "\n",
        "\"This is my own lair,\" he said, leading me into a dark plain room at the end of the florid vista. It was square and brown and leathery: no \"effects\"; no bric-a-brac, none of the air of posing for reproduction in a picture weekly--above all, no least sign of ever having been used as a studio.\n",
        "\n",
        "The fact brought home to me the absolute finality of Jack's break with his old life.\n",
        "\n",
        "\"Don't you ever dabble with paint any more?\" I asked, still looking about for a trace of such activity.\n",
        "\n",
        "\"Never,\" he said briefly.\n",
        "\n",
        "\"Or water-colour--or etching?\"\n",
        "\n",
        "His confident eyes grew dim, and his cheeks paled a little under their handsome sunburn.\n",
        "\n",
        "\"Never think of it, my dear fellow--any more than if I'd never touched a brush.\"\n",
        "\n",
        "And his tone told me in a flash that he never thought of anything else.\n",
        "\n",
        "I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above the mantel-piece--the only object breaking the plain oak panelling of the room.\n",
        "\n",
        "\"Oh, by Jove!\" I said.\n",
        "\n",
        "It was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.\n",
        "\n",
        "\"By Jove--a Stroud!\" I cried.\n",
        "\n",
        "He was silent; but I felt him close behind me, breathing a little quickly.\n",
        "\n",
        "\"What a wonder! Made with a dozen lines--but on everlasting foundations. You lucky chap, where did you get it?\"\n",
        "\n",
        "He answered slowly: \"Mrs. Stroud gave it to me.\"\n",
        "\n",
        "\"Ah--I didn't know you even knew the Strouds. He was such an inflexible hermit.\"\n",
        "\n",
        "\"I didn't--till after. . . . She sent for me to paint him when he was dead.\"\n",
        "\n",
        "\"When he was dead? You?\"\n",
        "\n",
        "I must have let a little too much amazement escape through my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor Stroud! She thought it the surest way of proclaiming his greatness--of forcing it on a purblind public. And at the moment I was _the_ fashionable painter.\"\n",
        "\n",
        "\"Ah, poor Stroud--as you say. Was _that_ his history?\"\n",
        "\n",
        "\"That was his history. She believed in him, gloried in him--or thought she did. But she couldn't bear not to have all the drawing-rooms with her. She couldn't bear the fact that, on varnishing days, one could always get near enough to see his pictures. Poor woman! She's just a fragment groping for other fragments. Stroud is the only whole I ever knew.\"\n",
        "\n",
        "\"You ever knew? But you just said--\"\n",
        "\n",
        "Gisburn had a curious smile in his eyes.\n",
        "\n",
        "\"Oh, I knew him, and he knew me--only it happened after he was dead.\"\n",
        "\n",
        "I dropped my voice instinctively. \"When she sent for you?\"\n",
        "\n",
        "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
        "\n",
        "He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I couldn't look at that thing--couldn't face it. But I forced myself to put it here; and now it's cured me--cured me. That's the reason why I don't dabble any more, my dear Rickham; or rather Stroud himself is the reason.\"\n",
        "\n",
        "For the first time my idle curiosity about my companion turned into a serious desire to understand him better.\n",
        "\n",
        "\"I wish you'd tell me how it happened,\" I said.\n",
        "\n",
        "He stood looking up at the sketch, and twirling between his fingers a cigarette he had forgotten to light. Suddenly he turned toward me.\n",
        "\n",
        "\"I'd rather like to tell you--because I've always suspected you of loathing my work.\"\n",
        "\n",
        "I made a deprecating gesture, which he negatived with a good-humoured shrug.\n",
        "\n",
        "\"Oh, I didn't care a straw when I believed in myself--and now it's an added tie between us!\"\n",
        "\n",
        "He laughed slightly, without bitterness, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"\n",
        "\n",
        "He placed them at my elbow and continued to wander up and down the room, stopping now and then beneath the picture.\n",
        "\n",
        "\"How it happened? I can tell you in five minutes--and it didn't take much longer to happen. . . . I can remember now how surprised and pleased I was when I got Mrs. Stroud's note. Of course, deep down, I had always _felt_ there was no one like him--only I had gone with the stream, echoed the usual platitudes about him, till I half got to think he was a failure, one of the kind that are left behind. By Jove, and he _was_ left behind--because he had come to stay! The rest of us had to let ourselves be swept along or go under, but he was high above the current--on everlasting foundations, as you say.\n",
        "\n",
        "\"Well, I went off to the house in my most egregious mood--rather moved, Lord forgive me, at the pathos of poor Stroud's career of failure being crowned by the glory of my painting him! Of course I meant to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely, my dear Rickham! I was posing to myself like one of my own sitters.\n",
        "\n",
        "\"Then I was taken up and left alone with him. I had sent all my traps in advance, and I had only to set up the easel and get to work. He had been dead only twenty-four hours, and he died suddenly, of heart disease, so that there had been no preliminary work of destruction--his face was clear and untouched. I had met him once or twice, years before, and thought him insignificant and dingy. Now I saw that he was superb.\n",
        "\n",
        "\"I was glad at first, with a merely aesthetic satisfaction: glad to have my hand on such a 'subject.' Then his strange life-likeness began to affect me queerly--as I blocked the head in I felt as if he were watching me do it. The sensation was followed by the thought: if he _were_ watching me, what would he say to my way of working? My strokes began to go a little wild--I felt nervous and uncertain.\n",
        "\n",
        "\"Once, when I looked up, I seemed to see a smile behind his close grayish beard--as if he had the secret, and were amusing himself by holding it back from me. That exasperated me still more. The secret? Why, I had a secret worth twenty of his! I dashed at the canvas furiously, and tried some of my bravura tricks. But they failed me, they crumbled. I saw that he wasn't watching the showy bits--I couldn't distract his attention; he just kept his eyes on the hard passages between. Those were the ones I had always shirked, or covered up with some lying paint. And how he saw through my lies!\n",
        "\n",
        "\"I looked up again, and caught sight of that sketch of the donkey hanging on the wall near his bed. His wife told me afterward it was the last thing he had done--just a note taken with a shaking hand, when he was down in Devonshire recovering from a previous heart attack. Just a note! But it tells his whole history. There are years of patient scornful persistence in every line. A man who had swum with the current could never have learned that mighty up-stream stroke. . . .\n",
        "\n",
        "\"I turned back to my work, and went on groping and muddling; then I looked at the donkey again. I saw that, when Stroud laid in the first stroke, he knew just what the end would be. He had possessed his subject, absorbed it, recreated it. When had I done that with any of my things? They hadn't been born of me--I had just adopted them. . . .\n",
        "\n",
        "\"Hang it, Rickham, with that face watching me I couldn't do another stroke. The plain truth was, I didn't know where to put it--_I had never known_. Only, with my sitters and my public, a showy splash of colour covered up the fact--I just threw paint into their faces. . . . Well, paint was the one medium those dead eyes could see through--see straight to the tottering foundations underneath. Don't you know how, in talking a foreign language, even fluently, one says half the time not what one wants to but what one can? Well--that was the way I painted; and as he lay there and watched me, the thing they called my 'technique' collapsed like a house of cards. He didn't sneer, you understand, poor Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: 'Are you sure you know where you're coming out?'\n",
        "\n",
        "\"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn't--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn't have given to have Stroud alive before me, and to hear him say: 'It's not too late--I'll show you how'?\n",
        "\n",
        "\"It _was_ too late--it would have been, even if he'd been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn't tell her _that_--it would have been Greek to her. I simply said I couldn't paint him, that I was too moved. She rather liked the idea--she's so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she did so want him 'done' by some one showy! At first I was afraid she wouldn't let me off--and at my wits' end I suggested Grindle. Yes, it was I who started Grindle: I told Mrs. Stroud he was the 'coming' man, and she told somebody else, and so it got to be true. . . . And he painted Stroud without wincing; and she hung the picture among her husband's things. . . .\"\n",
        "\n",
        "He flung himself down in the arm-chair near mine, laid back his head, and clasping his arms beneath it, looked up at the picture above the chimney-piece.\n",
        "\n",
        "\"I like to fancy that Stroud himself would have given it to me, if he'd been able to say what he thought that day.\"\n",
        "\n",
        "And, in answer to a question I put half-mechanically--\"Begin again?\" he flashed out. \"When the one thing that brings me anywhere near him is that I knew enough to leave off?\"\n",
        "\n",
        "He stood up and laid his hand on my shoulder with a laugh. \"Only the irony of it is that I _am_ still painting--since Grindle's doing it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\"\""
      ],
      "metadata": {
        "id": "y-kX-yd9bXmM"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data[:99]"
      ],
      "metadata": {
        "id": "aRCLDx5NcEAM",
        "outputId": "5f4980ad-1ee9-4037-8fae-4cb6e50d17c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters :\",total_characters)\n",
        "print(\"Tokens :\",total_tokens)"
      ],
      "metadata": {
        "id": "r3qbEZ35cQXs",
        "outputId": "fde4768e-9b4d-43b3-9140-4b522d7e6e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters : 20478\n",
            "Tokens : 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dataset is small but enough for learning"
      ],
      "metadata": {
        "id": "wfPVwlsrdvHX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CUOeW_KMduC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text,tokenizer):\n",
        "  encoded = tokenizer.encode(text,allowed_special = {\"<|endoftext|>\"})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "  return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids,tokenizer):\n",
        "  flat = token_ids.squeeze(0)\n",
        "  return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(start_context,tokenizer),\n",
        "    max_new_tokens = 6,\n",
        "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text \\n\",token_ids_to_text(token_ids,tokenizer))\n",
        "\n"
      ],
      "metadata": {
        "id": "ub2xfFuOcqEe",
        "outputId": "9a17a0ce-ed9c-4539-e602-4c5612e7a19b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text \n",
            " Every effort moves youaunder STUD How Pwr respawn ONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "70UB4flQdc8G"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7DsJTVQwfeB2"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "kbAaXV7clM_p"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "\n",
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")"
      ],
      "metadata": {
        "id": "nqQrTPDlpIZ2"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Data into training and validation"
      ],
      "metadata": {
        "id": "a_tcZKZImwcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n"
      ],
      "metadata": {
        "id": "waZIPHE3mqgs",
        "outputId": "08d0d754-23b2-4e9e-bc0d-eeae9de5fd3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "9\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT architectuer we coded earlier**"
      ],
      "metadata": {
        "id": "LgeJbOl1qwLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();  # Disable dropout during inference"
      ],
      "metadata": {
        "id": "Os8tuwqjm952"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "YQKTN9lrrEq0"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "id": "oZpCWy0JrH6-",
        "outputId": "e9d07e35-256d-40ce-c22d-f2f2496a447a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.987583690219456\n",
            "Validation loss: 10.98169231414795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Loop FOr LLm**"
      ],
      "metadata": {
        "id": "htlGyVDhvzYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "-im5s3pDrKs8"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "DChSEz66zTjo"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "-VnYPakvzW91"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note:\n",
        "# Uncomment the following code to calculate the execution time\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "Wea3BE57zacV",
        "outputId": "c0cd48ee-3877-406a-919a-28a8396c9671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.923\n",
            "Ep 1 (Step 000005): Train loss 7.996, Val loss 8.334\n",
            "Every effort moves you,.                                                \n",
            "Ep 2 (Step 000010): Train loss 6.759, Val loss 7.046\n",
            "Ep 2 (Step 000015): Train loss 6.120, Val loss 6.571\n",
            "Every effort moves you, and,, and,, and,,,, and,,,,,,,,,,,,,,,, and,,,,,,,,,,,,,,,,,,,,\n",
            "Ep 3 (Step 000020): Train loss 5.583, Val loss 6.470\n",
            "Ep 3 (Step 000025): Train loss 5.559, Val loss 6.468\n",
            "Every effort moves you, and to to have to to have to to theisburn, and, and, and, and, and, and, and, and the to the to theis, and, and, and, and, and, and, and,\n",
            "Ep 4 (Step 000030): Train loss 5.228, Val loss 6.383\n",
            "Ep 4 (Step 000035): Train loss 4.846, Val loss 6.317\n",
            "Every effort moves you know not to have to have to have was not to have to have not to have to have was not to have to have was not to the picture to have was not to have my to the picture to the picture and he was not to have to\n",
            "Ep 5 (Step 000040): Train loss 4.735, Val loss 6.326\n",
            "Every effort moves you know not to have to have to have to see, I had been.                                   \n",
            "Ep 6 (Step 000045): Train loss 4.297, Val loss 6.174\n",
            "Ep 6 (Step 000050): Train loss 3.753, Val loss 6.188\n",
            "Every effort moves you know; and he had been, and I had been, and he was not and he was, and he was, and he had been, and he had been, and I had been, and he had been, and I had been, and\n",
            "Ep 7 (Step 000055): Train loss 3.366, Val loss 6.135\n",
            "Ep 7 (Step 000060): Train loss 3.261, Val loss 6.095\n",
            "Every effort moves you know it was not that, and he was not to have to me in a little: \"Yes, and to have to have to see.       \"--and it, and up and down, and he was his\n",
            "Ep 8 (Step 000065): Train loss 2.571, Val loss 6.137\n",
            "Ep 8 (Step 000070): Train loss 2.408, Val loss 6.088\n",
            "Every effort moves you know the picture.                                              \n",
            "Ep 9 (Step 000075): Train loss 2.040, Val loss 6.152\n",
            "Ep 9 (Step 000080): Train loss 1.638, Val loss 6.196\n",
            "Every effort moves you know,\" was not that my hostess was not that lifted the fact of a self-confident moustache, I had been to the end of his pictures that he had been; and I had the donkey. \"strongest,\" she was\n",
            "Ep 10 (Step 000085): Train loss 1.264, Val loss 6.192\n",
            "Every effort moves you know,\" was not that my hostess was \"interesting\": on that point I had been through--I looked up, I had been to the display of his pictures with my unexpected discovery; and as I had been the man of the hour. The\n",
            "Training completed in 22.87 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see model is overfitted because we have train it for multiples epochs on very less data so it was obvious,\n",
        "\n",
        "\n",
        "now model is producing grammetically correct outputs"
      ],
      "metadata": {
        "id": "8bHt4kPg_Pp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "SA_DrYWSzcr4",
        "outputId": "15fd5f3c-1b0e-479c-dcee-32b9984ab3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DVVIAtM__c_T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bF9OUCyq1xXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DECODING STRATEGIES TO CONTROL RANDOMNESS"
      ],
      "metadata": {
        "id": "NTpOjLjsJAzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "ckzqmPc-JBHC",
        "outputId": "acf1f610-8365-4926-8272-a9de0ba4e158",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you know; and up-stream stroke. Gisburn--as, in a self-confident moustache, I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher temperature = more randomness and creativity.\n",
        "\n",
        "Lower temperature = more coherence and predictability.\n",
        "\n",
        "Temperature = 1.0 = default, balanced behavior.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v9dXINjOJKvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}"
      ],
      "metadata": {
        "id": "dh1nruP5JG73"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_logits = torch.tensor(\n",
        "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")"
      ],
      "metadata": {
        "id": "JB1ghzKsJQp2"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "As discussed in the previous chapter, inside the generate_text_simple, we convert the\n",
        "logits into probabilities via the softmax function and obtain the token ID corresponding the\n",
        "generated token via the argmax function, which we can then map back into text via the\n",
        "inverse vocabulary:\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "GV153CphJYzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probas = torch.softmax(next_token_logits, dim=0)\n",
        "next_token_id = torch.argmax(probas).item()\n",
        "print(inverse_vocab[next_token_id])"
      ],
      "metadata": {
        "id": "rRwmT4ltJS3t",
        "outputId": "aa0652f6-5ea0-4ced-ca6f-4c18d842978f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "To implement a probabilistic sampling process, we can now replace the argmax with the\n",
        "multinomial function in PyTorch:\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "-KSmXrI4Jeno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
        "print(inverse_vocab[next_token_id])"
      ],
      "metadata": {
        "id": "CmQO0HGaJbeD",
        "outputId": "ce9654b9-c8e8-4c2f-e41d-4cd5f751e4b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "The printed output is \"forward\" just like before. What happened? The multinomial\n",
        "function samples the next token proportional to its probability score.\n",
        "\n",
        "In other words,\n",
        "\"forward\" is still the most likely token and will be selected by multinomial most of the\n",
        "time but not all the time.\n",
        "\n",
        "To illustrate this, let's implement a function that repeats this\n",
        "sampling 1000 times:\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "6mF72In_JlhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sampled_tokens(probas):\n",
        "    torch.manual_seed(123) # Manual seed for reproducibility\n",
        "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
        "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
        "    for i, freq in enumerate(sampled_ids):\n",
        "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
        "\n",
        "print_sampled_tokens(probas)"
      ],
      "metadata": {
        "id": "Cgw6mSyxJh0l",
        "outputId": "7965e668-c3d7-41e5-8eb0-58d9294826e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71 x closer\n",
            "2 x every\n",
            "0 x effort\n",
            "544 x forward\n",
            "2 x inches\n",
            "1 x moves\n",
            "0 x pizza\n",
            "376 x toward\n",
            "4 x you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "As we can see based on the output, the word \"forward\" is sampled most of the time (582\n",
        "out of 1000 times), but other tokens such as \"closer\", \"inches\", and \"toward\" will also\n",
        "be sampled some of the time.\n",
        "\n",
        "This means that if we replaced the argmax function with the\n",
        "multinomial function inside the generate_and_print_sample function, the LLM would\n",
        "sometimes generate texts such as \"every effort moves you toward\", \"every effort\n",
        "moves you inches\", and \"every effort moves you closer\" instead of \"every effort\n",
        "moves you forward\".\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "EpBnonDwJsBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "We can further control the distribution and selection process via a concept called\n",
        "temperature scaling, where temperature scaling is just a fancy description for dividing the\n",
        "logits by a number greater than 0:\n",
        "\n",
        "</div>**"
      ],
      "metadata": {
        "id": "mifrbjmbJvx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temperature(logits, temperature):\n",
        "    scaled_logits = logits / temperature\n",
        "    return torch.softmax(scaled_logits, dim=0)\n",
        "\n",
        "# Temperature values\n",
        "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
        "\n",
        "# Calculate scaled probabilities\n",
        "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
      ],
      "metadata": {
        "id": "ZPEfpJeCJoVi"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "x = torch.arange(len(vocab))\n",
        "bar_width = 0.15\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 3))\n",
        "for i, T in enumerate(temperatures):\n",
        "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
        "\n",
        "ax.set_ylabel('Probability')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"temperature-plot.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FUGSAmoLJzOP",
        "outputId": "d7a44190-337a-481c-808a-2c91582abea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABM5klEQVR4nO3deVxU1f8/8Newg2wimyAKiiYUO0q4oUWCGmqkGWooIt8scYFwjUUgwDQR/YRiKu5rRlqaJvIRcc0dMxEDREhBcSVA1jm/P/xxP44DyH7v4Pv5eMzjw5y5d+Y185l8zz333HNEjDEGQgghhAiSHN8BCCGEEFI/KtSEEEKIgFGhJoQQQgSMCjUhhBAiYFSoCSGEEAGjQk0IIYQIGBVqQgghRMCoUBNCCCECpsB3gPYmFotx7949aGhoQCQS8R2HEELIG4gxhn///RdGRkaQk2v4mPmNK9T37t2DiYkJ3zEIIYQQ5Ofno1u3bg1u88YVag0NDQAvPhxNTU2e0xBCCHkTFRcXw8TEhKtJDXnjCnVtd7empiYVakIIIbxqzClYGkxGCCGECBivhTotLQ0eHh4wMjKCSCTC/v37X7tPamoq7O3toaysDHNzc2zevLnNcxJCCCF84bVQl5aWwsbGBvHx8Y3a/vbt2xg1ahSGDRuGq1evYu7cuZg+fTp+//33Nk5KCCGE8IPXc9QjRozAiBEjGr19QkICzMzMsGLFCgCAhYUFTp06hZUrV8LNza2tYhJC2plYLEZlZSXfMQhpNkVFRcjLy7fKc8nUYLKzZ8/C1dVVos3NzQ1z586td5+KigpUVFRw94uLi9sqHiGkFVRWVuL27dsQi8V8RyGkRbS1tWFoaNjiOTtkqlAXFhbCwMBAos3AwADFxcV4/vw5VFVVpfaJiYlBeHh4e0UkhLQAYwwFBQWQl5eHiYnJayeCIESIGGMoKyvDgwcPAABdu3Zt0fPJVKFujkWLFiEwMJC7X3vtGiFEeKqrq1FWVgYjIyOoqanxHYeQZqs9cHzw4AH09fVb1A0uU4Xa0NAQ9+/fl2i7f/8+NDU16zyaBgBlZWUoKyu3RzxCGm+JVgOPPWu/HAJTU1MDAFBSUuI5CSEtV/tjs6qqqkWFWqb6lZydnZGSkiLRlpycDGdnZ54SEULaAs3DTzqC1voe81qoS0pKcPXqVVy9ehXAi8uvrl69iry8PAAvuq29vb257WfMmIGcnBzMnz8fN2/exJo1a7B3714EBATwEZ8QQghpc7wW6osXL8LOzg52dnYAgMDAQNjZ2SE0NBQAUFBQwBVtADAzM8OhQ4eQnJwMGxsbrFixAhs2bKBLswghhHRYvJ6jHjp0KBhj9T5e16xjQ4cOxZUrV9owFSFEaEwXHmrX18tdOqrR276uezMsLAxLlixpYSJhMTU1xdy5cxu8NFboZs+ejdOnT+P69euwsLDgenaFSKYGkxFCiNAUFBRwf+/ZswehoaHIzMzk2tTV1fmI1WSMMdTU1EBBof3KQmVlJa8DB6dNm4Y//vgD165d4y1DY8jUYDJCCBEaQ0ND7qalpQWRSCTRtnv3blhYWEBFRQV9+/bFmjVruH1zc3MhEomwd+9eDB48GKqqqujXrx9u3bqFCxcuwNHREerq6hgxYgSKioq4/aZOnYqxY8ciPDwcenp60NTUxIwZMyRmcxOLxYiJiYGZmRlUVVVhY2ODffv2cY+npqZCJBLh8OHDcHBwgLKyMk6dOoXs7GyMGTMGBgYGUFdXR79+/XDs2DFuv6FDh+LOnTsICAiASCTiehSWLFkCW1tbic8mLi4OpqamUrmjoqJgZGSEt956C8CLZYc/+eQTaGtrQ0dHB2PGjEFubm5r/N9Tr9WrV2PmzJno2bNnm75Oa6BCTQghbWTHjh0IDQ1FVFQUMjIyEB0djZCQEGzZskViu7CwMAQHB+Py5ctQUFDAxIkTMX/+fKxatQonT55EVlYWN3anVkpKCjIyMpCamopdu3YhKSlJYnKnmJgYbN26FQkJCfjrr78QEBCAyZMn48SJExLPs3DhQixduhQZGRmwtrZGSUkJRo4ciZSUFFy5cgXu7u7w8PDgxgslJSWhW7duiIiIQEFBgUSPQmOkpKQgMzMTycnJOHjwIKqqquDm5gYNDQ2cPHkSp0+fhrq6Otzd3RucRlZdXb3B24wZM5qUS8io65sQQtpIWFgYVqxYAU9PTwAvBsTeuHED69atw5QpU7jtgoKCuEGxc+bMgZeXF1JSUjBw4EAAgK+vr9SYHSUlJSQmJkJNTQ1vv/02IiIiMG/ePERGRqKqqgrR0dE4duwYd/lqz549cerUKaxbtw4uLi7c80REROCDDz7g7uvo6MDGxoa7HxkZiZ9//hm//PIL/P39oaOjA3l5eWhoaMDQ0LDJn0mnTp2wYcMGrst7+/btEIvF2LBhA3d0vmnTJmhrayM1NRXDhw+v83led05ZU1OzydmEigo1IYS0gdLSUmRnZ8PX1xd+fn5ce3V1NbS0JCe8sba25v6unSbZyspKoq12OspaNjY2ErO3OTs7o6SkBPn5+SgpKUFZWZlEAQZenBOuvcqmlqOjo8T9kpISLFmyBIcOHUJBQQGqq6vx/PlziStwWsLKykrivHR6ejqysrKgoaEhsV15eTmys7PrfR5zc/NWySMLqFATQkgbKCkpAQCsX78eTk5OEo+9OkuVoqIi93ftUeWrbU1ZpKT2tQ8dOgRjY2OJx16dqbFTp04S94OCgpCcnIzvvvsO5ubmUFVVxbhx4167mpmcnJzUVTxVVVVS2736eiUlJXBwcMCOHTukttXT06v39V43SG/y5MlISEhocBtZQYWaEELagIGBAYyMjJCTk4NJkya1+vOnp6dLLEZ07tw5qKurw8TEBDo6OlBWVkZeXp5EN3djnD59GlOnTsVHH30E4EUhfXVgl5KSEjfday09PT0UFhaCMcb92GjMJU/29vbYs2cP9PX1m9RdTV3fhBBCWiw8PByzZ8+GlpYW3N3dUVFRgYsXL+LJkycSiwU1R2VlJXx9fREcHIzc3FyEhYXB398fcnJy0NDQQFBQEAICAiAWizFo0CA8e/YMp0+fhqampsT58Vf17t0bSUlJ8PDwgEgkQkhIiNTRvKmpKdLS0vDpp59CWVkZurq6GDp0KIqKirBs2TKMGzcOR44cweHDh19bMCdNmoTly5djzJgxiIiIQLdu3XDnzh0kJSVh/vz56NatW537tbTrOysrCyUlJSgsLMTz58+5wm9paSm4ueZp1DchhLSR6dOnY8OGDdi0aROsrKzg4uKCzZs3w8zMrMXP/f7776N3794YMmQIJkyYgNGjR0tMrBIZGYmQkBDExMTAwsIC7u7uOHTo0GtfOzY2Fp07d8aAAQPg4eEBNzc32NvbS2wTERGB3Nxc9OrVi+uetrCwwJo1axAfHw8bGxucP38eQUFBr30fampqSEtLQ/fu3eHp6QkLCwv4+vqivLy8TY+Kp0+fDjs7O6xbtw63bt3iZsm8d+9em71mc4lYQ1ODdUDFxcXQ0tLCs2fPOlTXCJExtHpWncrLy3H79m2YmZlBRUWF7ziCNXXqVDx9+hT79+/nOwppQEPf56bUIjqiJoQQQgSMCjUhhBAiYDSYjBBCZExdCxaRjouOqAkhhBABo0JNCCGECBgVakIIIUTAqFATQgghAkaFmhBCCBEwKtSEEEKIgFGhJoSQFhCJRA3eXp7Ws6MwNTVFXFwc3zFaJC8vD6NGjYKamhr09fUxb948VFdXN7hPVFQUBgwYADU1NWhra7dPUNB11IQQWdDQlKtt8nqNn8a1oKCA+3vPnj0IDQ1FZmYm1/a65RiFgjGGmpoaKCi0X1morKzkZQGMmpoajBo1CoaGhjhz5gwKCgrg7e0NRUVFREdH17tfZWUlxo8fD2dnZ2zcuLHd8tIRNSGEtIChoSF309LSgkgkkmjbvXs3LCwsoKKigr59+2LNmjXcvrm5uRCJRNi7dy8GDx4MVVVV9OvXD7du3cKFCxfg6OgIdXV1jBgxAkVFRdx+U6dOxdixYxEeHg49PT1oampixowZEmtGi8VixMTEwMzMDKqqqrCxscG+ffu4x1NTUyESiXD48GE4ODhAWVkZp06dQnZ2NsaMGQMDAwOoq6ujX79+OHbsGLff0KFDcefOHQQEBHC9BgCwZMkS2NraSnw2cXFxMDU1lcodFRUFIyMjvPXWWwCA/Px8fPLJJ9DW1oaOjg7GjBkjtbRmazp69Chu3LiB7du3w9bWFiNGjEBkZCTi4+MbXHc7PDwcAQEBsLKyarNsdaFCTQghbWTHjh0IDQ1FVFQUMjIyEB0djZCQEGzZskViu7CwMAQHB+Py5ctQUFDAxIkTMX/+fKxatQonT55EVlYWQkNDJfZJSUlBRkYGUlNTsWvXLiQlJSE8PJx7PCYmBlu3bkVCQgL++usvBAQEYPLkyThx4oTE8yxcuBBLly5FRkYGrK2tUVJSgpEjRyIlJQVXrlyBu7s7PDw8kJeXBwBISkpCt27dEBERgYKCAokehcZISUlBZmYmkpOTcfDgQVRVVcHNzQ0aGho4efIkTp8+DXV1dbi7uzdYNNXV1Ru8zZgxo959z549CysrKxgYGHBtbm5uKC4uxl9//dWk99MeqOubEELaSFhYGFasWAFPT08AgJmZGW7cuIF169ZJrAkdFBQENzc3AMCcOXPg5eWFlJQUDBw4EADg6+srNW2okpISEhMToaamhrfffhsRERGYN28eIiMjUVVVhejoaBw7dgzOzs4AgJ49e+LUqVNYt24dXFxcuOeJiIjABx98wN3X0dGBjY0Ndz8yMhI///wzfvnlF/j7+0NHRwfy8vLQ0NCAoaFhkz+TTp06YcOGDVyX9/bt2yEWi7Fhwwbu6HzTpk3Q1tZGamoqhg8fXufz1K4fXZ+GVqQqLCyUKNIAuPuFhYWNfSvthgo1IYS0gdLSUmRnZ8PX1xd+fn5ce3V1NbS0JM+5W1tbc3/XFoyXu1cNDAzw4MEDiX1sbGygpqbG3Xd2dkZJSQny8/NRUlKCsrIyiQIMvDjHamdnJ9Hm6Ogocb+kpARLlizBoUOHUFBQgOrqajx//pw7om4pKysrifPS6enpyMrKgoaGhsR25eXlyM7Orvd5zM3NWyWPLKBCTQghbaCkpAQAsH79ejg5OUk8Ji8vL3FfUVGR+7v2qPLVNrFY3OTXPnToEIyNjSUeU1ZWlrjfqVMniftBQUFITk7Gd999B3Nzc6iqqmLcuHENdkMDgJycHBhjEm1VVVVS2736eiUlJXBwcMCOHTukttXT06v39V43SG/y5MlISEio8zFDQ0OcP39eou3+/fvcY0JDhZoQQtqAgYEBjIyMkJOTg0mTJrX686enp+P58+dQVVUFAJw7dw7q6uowMTGBjo4OlJWVkZeXJ9HN3RinT5/G1KlT8dFHHwF4UUhfHdilpKSEmpoaiTY9PT0UFhaCMcb92Hhd9zQA2NvbY8+ePdDX12+wu/pVLen6dnZ2RlRUFB48eAB9fX0AQHJyMjQ1NWFpadnoDO2FCjUhhLSR8PBwzJ49G1paWnB3d0dFRQUuXryIJ0+eIDAwsEXPXVlZCV9fXwQHByM3NxdhYWHw9/eHnJwcNDQ0EBQUhICAAIjFYgwaNAjPnj3D6dOnoampKXF+/FW9e/dGUlISPDw8IBKJEBISInU0b2pqirS0NHz66adQVlaGrq4uhg4diqKiIixbtgzjxo3DkSNHcPjw4dcW30mTJmH58uUYM2YMIiIi0K1bN9y5cwdJSUmYP38+unXrVud+Len6Hj58OCwtLfHZZ59h2bJlKCwsRHBwMGbOnMn1OJw/fx7e3t5ISUnheiXy8vLw+PFj5OXloaamhvuxYG5u3qaX4fE+6js+Ph6mpqZQUVGBk5OTVHfEq+Li4vDWW29BVVUVJiYmCAgIQHl5eTulJYSQxps+fTo2bNiATZs2wcrKCi4uLti8eTPMzMxa/Nzvv/8+evfujSFDhmDChAkYPXq0xOQqkZGRCAkJQUxMDCwsLODu7o5Dhw699rVjY2PRuXNnDBgwAB4eHnBzc4O9vb3ENhEREcjNzUWvXr247mkLCwusWbMG8fHxsLGxwfnz5xEUFPTa96Gmpoa0tDR0794dnp6esLCwgK+vL8rLy5t0hN0U8vLyOHjwIOTl5eHs7IzJkyfD29sbERER3DZlZWXIzMyU6L4PDQ2FnZ0dwsLCUFJSAjs7O9jZ2eHixYttkrOWiL16UqEd7dmzB97e3khISICTkxPi4uLw448/IjMzk+uOeNnOnTsxbdo0JCYmYsCAAbh16xamTp2KTz/9FLGxsY16zeLiYmhpaeHZs2dt9iUg5LUamsCjCZNtdDTl5eW4ffs2zMzMoKKiwnccwZo6dSqePn2K/fv38x2FNKCh73NTahGvR9SxsbHw8/ODj48PLC0tkZCQADU1NSQmJta5/ZkzZzBw4EBMnDgRpqamGD58OLy8vF57FE4IIYTIKt4KdWVlJS5dugRXV9f/hZGTg6urK86ePVvnPgMGDMClS5e4wpyTk4PffvsNI0eObJfMhBBCSHvjbTDZw4cPUVNTU+dF5zdv3qxzn4kTJ+Lhw4cYNGgQGGOorq7GjBkzsHjx4npfp6KiAhUVFdz94uLi1nkDhBDCk1cnPyEdG++DyZoiNTUV0dHRWLNmDS5fvoykpCQcOnQIkZGR9e4TExMDLS0t7mZiYtKOiQkhhJCW4e2IWldXF/Ly8txF5rXu379f7wXnISEh+OyzzzB9+nQAL2a4KS0txf/93//h66+/hpyc9O+ORYsWSVwGUVxcTMWaEEKIzODtiFpJSQkODg5ISUnh2sRiMVJSUri5aV9VVlYmVYxrZ/ipb/C6srIyNDU1JW6EEEKIrOB1wpPAwEBMmTIFjo6O6N+/P+Li4lBaWgofHx8AgLe3N4yNjRETEwMA8PDwQGxsLOzs7ODk5ISsrCyEhITAw8NDako+QgghpCPgtVBPmDABRUVFCA0NRWFhIWxtbXHkyBFugFleXp7EEXRwcDBEIhGCg4Nx9+5d6OnpwcPDA1FRUXy9BUIIIaRN8TrhCR9owhMiCDThSZ1owhPSkXSICU8IIYQQ0jAq1IQQ0gIikajB28vzb3cUpqamiIuL4ztGi9T1/9Xu3bv5jlUnWj2LECJ4Vlus2vX1/pzyZ6O3LSgo4P7es2cPQkNDkZmZybW15apKrYkxhpqaGigotF9ZqKyshJKSUru93qs2bdoEd3d37r62tjZvWRpCR9SEENIChoaG3E1LSwsikUiibffu3bCwsICKigr69u2LNWvWcPvm5uZCJBJh7969GDx4MFRVVdGvXz/cunULFy5cgKOjI9TV1TFixAgUFRVx+02dOhVjx45FeHg49PT0oKmpiRkzZqCyspLbRiwWIyYmBmZmZlBVVYWNjQ327dvHPZ6amgqRSITDhw/DwcEBysrKOHXqFLKzszFmzBgYGBhAXV0d/fr1w7Fjx7j9hg4dijt37iAgIIA7EgWAJUuWwNbWVuKziYuLg6mpqVTuqKgoGBkZ4a233gIA5Ofn45NPPoG2tjZ0dHQwZswYqTWw24K2trbE/1dCHRdBhZoQQtrIjh07EBoaiqioKGRkZCA6OhohISHYsmWLxHZhYWEIDg7G5cuXoaCggIkTJ2L+/PlYtWoVTp48iaysLISGhkrsk5KSgoyMDKSmpmLXrl1ISkpCeHg493hMTAy2bt2KhIQE/PXXXwgICMDkyZNx4sQJiedZuHAhli5dioyMDFhbW6OkpAQjR45ESkoKrly5And3d3h4eCAvLw8AkJSUhG7duiEiIgIFBQUSPQqNkZKSgszMTCQnJ+PgwYOoqqqCm5sbNDQ0cPLkSZw+fRrq6upwd3eX+OHxKnV19QZvM2bMeG2WmTNnQldXF/3790diYmK983Hwjbq+CSGkjYSFhWHFihXw9PQEAJiZmeHGjRtYt24dpkyZwm0XFBQENzc3AMCcOXPg5eWFlJQUDBw4EADg6+srNb+3kpISEhMToaamhrfffhsRERGYN28eIiMjUVVVhejoaBw7doybQKpnz544deoU1q1bBxcXF+55IiIi8MEHH3D3dXR0YGNjw92PjIzEzz//jF9++QX+/v7Q0dGBvLw8NDQ06p1FsiGdOnXChg0buC7v7du3QywWY8OGDdzR+aZNm6CtrY3U1FQMHz68zue5evVqg6/zupHUEREReO+996CmpoajR4/iyy+/RElJCWbPnt3k99TWqFATQkgbKC0tRXZ2Nnx9feHn58e1V1dXQ0tL8vI8a2tr7u/aeSSsrKwk2h48eCCxj42NDdTU1Lj7zs7OKCkpQX5+PkpKSlBWViZRgIEX54Tt7Owk2hwdHSXul5SUYMmSJTh06BAKCgpQXV2N58+fc0fULWVlZSVxXjo9PR1ZWVnQ0NCQ2K68vBzZ2dn1Po+5uXmLcoSEhHB/29nZobS0FMuXL6dCTQghb4qSkhIAwPr16+Hk5CTx2KszKSoqKnJ/1x5VvtomFoub/NqHDh2CsbGxxGPKysoS9zt16iRxPygoCMnJyfjuu+9gbm4OVVVVjBs3rsFuaODFMsWvdh1XVVVJbffq65WUlMDBwQE7duyQ2lZPT6/e13vdIL3JkycjISGhwW1e5uTkhMjISFRUVEh9RnyjQk0IIW3AwMAARkZGyMnJwaRJk1r9+dPT0/H8+XOoqqoCAM6dOwd1dXWYmJhAR0cHysrKyMvLk+jmbozTp09j6tSp+OijjwC8KKSvDuxSUlJCTU2NRJuenh4KCwvBGON+bLyuexoA7O3tsWfPHujr6zdpEqqWdn3X9XydO3cWXJEGqFATQkibCQ8Px+zZs6GlpQV3d3dUVFTg4sWLePLkicSqfs1RWVkJX19fBAcHIzc3F2FhYfD394ecnBw0NDQQFBSEgIAAiMViDBo0CM+ePcPp06ehqakpcX78Vb1790ZSUhI8PDwgEokQEhIidTRvamqKtLQ0fPrpp1BWVoauri6GDh2KoqIiLFu2DOPGjcORI0dw+PDh1xbMSZMmYfny5RgzZgwiIiLQrVs33LlzB0lJSZg/fz66detW534t6fr+9ddfcf/+fbz77rtQUVFBcnIyoqOjERQU1OznbEs06psQQtrI9OnTsWHDBmzatAlWVlZwcXHB5s2bYWZm1uLnfv/999G7d28MGTIEEyZMwOjRoyUmV4mMjERISAhiYmJgYWEBd3d3HDp06LWvHRsbi86dO2PAgAHw8PCAm5sb7O3tJbaJiIhAbm4uevXqxXVPW1hYYM2aNYiPj4eNjQ3Onz/fqMKnpqaGtLQ0dO/eHZ6enrCwsICvry/Ky8vbbJpnRUVFxMfHw9nZGba2tli3bh1iY2MRFhbWJq/XUjTXNyF8oLm+60RzfTfO1KlT8fTpU+zfv5/vKKQBNNc3IYQQ8gagQk0IIYQIGA0mI4QQGfPq5CekY2vWEfXx48dbOwchhBBC6tCsQu3u7o5evXrhm2++QX5+fmtnIoQQQsj/16xCfffuXfj7+2Pfvn3o2bMn3NzcsHfv3tfOXEMIIY3xhl2MQjqo1voeN6tQ6+rqIiAgAFevXsUff/yBPn364Msvv4SRkRFmz56N9PT0VglHCHmz1E6tST/6SUdQVlYGQHI62OZo8WAye3t7GBoaokuXLli6dCkSExOxZs0aODs7IyEhAW+//XZLX4IQ8oZQUFCAmpoaioqKoKioCDk5ujCFyB7GGMrKyvDgwQNoa2tLze3eVM0u1FVVVThw4AASExORnJwMR0dHfP/99/Dy8kJRURGCg4Mxfvx43Lhxo0UBCSFvDpFIhK5du+L27du4c+cO33EIaRFtbe1mLQX6qmYV6lmzZmHXrl1gjOGzzz7DsmXL8M4773CPd+rUCd999x2MjIxaHJAQ8mZRUlJC7969qfubyDRFRcUWH0nXalahvnHjBv7zn//A09Oz3pVGdHV16TIuQkizyMnJ0RSihPx/zToBFBYWhvHjx0sV6erqaqSlpQF4ca6pqcurEUIIIURSswr1sGHD8PjxY6n2Z8+eYdiwYS0ORQghhJAXmlWoX14Y/GWPHj1Cp06dWhyKEEIIIS806Ry1p6cngBcjM6dOnSrR9V1TU4Nr165hwIABrZuQEEIIeYM1qVBrab1YQ5cxBg0NDaiqqnKPKSkp4d1334Wfn1/rJiSEEELeYE0q1Js2bQIAmJqaIigoiLq5CSGEkDbW7FHfrVWk4+PjYWpqChUVFTg5OeH8+fMNbv/06VPMnDkTXbt2hbKyMvr06YPffvutVbIQQgghQtPoI2p7e3ukpKSgc+fOsLOzq3MwWa3Lly836jn37NmDwMBAJCQkwMnJCXFxcXBzc0NmZib09fWltq+srMQHH3wAfX197Nu3D8bGxrhz5w60tbUb+zYIIYQQmdLoQj1mzBhu8NjYsWNb5cVjY2Ph5+cHHx8fAEBCQgIOHTqExMRELFy4UGr7xMREPH78GGfOnOEmOTc1NW2VLIQQQogQiRhP68lVVlZCTU0N+/btkyj8U6ZMwdOnT3HgwAGpfUaOHAkdHR2oqanhwIED0NPTw8SJE7FgwYJ6p2qrqKhARUUFd7+4uBgmJiZ49uwZNDU1W/19EdIoS7QaeOxZ++UghPCiuLgYWlpajapFvC1N8/DhQ9TU1MDAwECi3cDAAIWFhXXuk5OTg3379qGmpga//fYbQkJCsGLFCnzzzTf1vk5MTAy0tLS4m4mJSau+D0IIIaQtNbrru3Pnzg2el35ZXbOWtQaxWAx9fX388MMPkJeXh4ODA+7evYvly5cjLCyszn0WLVqEwMBA7n7tETUhhBAiCxpdqOPi4lr1hXV1dSEvL4/79+9LtN+/f7/eZcG6du0qtSKJhYUFCgsLUVlZCSUlJal9lJWV6104hBBCCBG6RhfqKVOmtOoLKykpwcHBASkpKdw5arFYjJSUFPj7+9e5z8CBA7Fz506IxWJuQflbt26ha9eudRZpQgghRNY1+hx1cXGxxN8N3RorMDAQ69evx5YtW5CRkYEvvvgCpaWl3Chwb29vLFq0iNv+iy++wOPHjzFnzhzcunULhw4dQnR0NGbOnNno1ySEEEJkSZPOURcUFEBfXx/a2tp1nq+uXayjpqamUc85YcIEFBUVITQ0FIWFhbC1tcWRI0e4AWZ5eXnckTMAmJiY4Pfff0dAQACsra1hbGyMOXPmYMGCBY19G4QQQohMafTlWSdOnMDAgQOhoKCAEydONLitkNehbsqQeEJawnThoXofy1WZWP+OdHkWIR1eU2pRo4+oXy6+Qi7EhBBCSEfSpEU5XvbkyRNs3LgRGRkZAABLS0v4+PhAR0en1cIRQgghb7pmTXiSlpYGU1NTrF69Gk+ePMGTJ0+wevVqmJmZIS0trbUzEkIIIW+sZh1Rz5w5ExMmTMDatWu5a5pramrw5ZdfYubMmfjzzz9bNSQhhBDypmrWEXVWVha++uoriYlH5OXlERgYiKysrFYLRwghhLzpmlWo7e3tuXPTL8vIyICNjU2LQxFCCCHkhUZ3fV+7do37e/bs2ZgzZw6ysrLw7rvvAgDOnTuH+Ph4LF26tPVTEkIIIW+oRl9HLScnB5FIhNdt3pQJT/hA11GT9kLXURNC6tMm11Hfvn27xcEIIYQQ0jSNLtQ9evRoyxyEEEIIqUOzJzwBgBs3biAvLw+VlZUS7aNHj25RKEIIIYS80KxCnZOTg48++gh//vmnxHnr2oU6hHyOmhBCCJElzbo8a86cOTAzM8ODBw+gpqaGv/76C2lpaXB0dERqamorRySEEELeXM06oj579iz++9//QldXF3JycpCTk8OgQYMQExOD2bNn48qVK62dkxBCCHkjNeuIuqamBhoaGgAAXV1d3Lt3D8CLAWeZmZmtl44QQgh5wzXriPqdd95Beno6zMzM4OTkhGXLlkFJSQk//PADevbs2doZCSGEkDdWswp1cHAwSktLAQARERH48MMPMXjwYHTp0gV79uxp1YCEEELIm6xZhdrNzY3729zcHDdv3sTjx4/RuXNnbuQ3IYQQQlquRddRA0B+fj4AwMTEpMVhCCGEECKpWYPJqqurERISAi0tLZiamsLU1BRaWloIDg5GVVVVa2ckhBBC3ljNOqKeNWsWkpKSsGzZMjg7OwN4ccnWkiVL8OjRI6xdu7ZVQxJCCCFvqmYV6p07d2L37t0YMWIE12ZtbQ0TExN4eXlRoSaEEEJaSbO6vpWVlWFqairVbmZmBiUlpZZmIoQQQsj/16xC7e/vj8jISFRUVHBtFRUViIqKgr+/f6uFI4QQQt50je769vT0lLh/7NgxdOvWDTY2NgCA9PR0VFZW4v3332/dhIQQQsgbrNGFWktLS+L+xx9/LHGfLs8ihBBCWl+jC/WmTZvaMgchhBBC6tCiCU+Kioq4RTjeeust6OnptUooQgghhLzQrMFkpaWlmDZtGrp27YohQ4ZgyJAhMDIygq+vL8rKylo7IyGEEPLGalahDgwMxIkTJ/Drr7/i6dOnePr0KQ4cOIATJ07gq6++avLzxcfHw9TUFCoqKnBycsL58+cbtd/u3bshEokwduzYJr8mIYQQIguaVah/+uknbNy4ESNGjICmpiY0NTUxcuRIrF+/Hvv27WvSc+3ZsweBgYEICwvD5cuXYWNjAzc3Nzx48KDB/XJzcxEUFITBgwc35y0QQgghMqFZhbqsrAwGBgZS7fr6+k3u+o6NjYWfnx98fHxgaWmJhIQEqKmpITExsd59ampqMGnSJISHh9P614QQQjq0ZhVqZ2dnhIWFoby8nGt7/vw5wsPDubm/G6OyshKXLl2Cq6vr/wLJycHV1RVnz56td7+IiAjo6+vD19f3ta9RUVGB4uJiiRshhBAiK5o16jsuLg7u7u5SE56oqKjg999/b/TzPHz4EDU1NVJH5wYGBrh582ad+5w6dQobN27E1atXG/UaMTExCA8Pb3QmQgghREiaVaitrKzw999/Y8eOHVxB9fLywqRJk6CqqtqqAV/277//4rPPPsP69euhq6vbqH0WLVqEwMBA7n5xcTFNzkIIIURmNLlQV1VVoW/fvjh48CD8/Pxa9OK6urqQl5fH/fv3Jdrv378PQ0NDqe2zs7ORm5sLDw8Prk0sFgMAFBQUkJmZiV69eknso6ysDGVl5RblJIQQQvjS5HPUioqKEuemW0JJSQkODg5ISUnh2sRiMVJSUuo81923b1/8+eefuHr1KncbPXo0hg0bhqtXr9KRMiGEkA6nWV3fM2fOxLfffosNGzZAQaFFk5shMDAQU6ZMgaOjI/r374+4uDiUlpbCx8cHAODt7Q1jY2PExMRARUUF77zzjsT+2traACDVTgghhHQEzaqyFy5cQEpKCo4ePQorKyt06tRJ4vGkpKRGP9eECRNQVFSE0NBQFBYWwtbWFkeOHOEGmOXl5UFOrlmD0wkhhBCZ16xCra2tLbV6Vkv4+/vXu451ampqg/tu3ry51XIQQgghQtOkQi0Wi7F8+XLcunULlZWVeO+997BkyZI2HelNCCGEvMma1KccFRWFxYsXQ11dHcbGxli9ejVmzpzZVtkIIYSQN16Tjqi3bt2KNWvW4PPPPwcAHDt2DKNGjcKGDRvoPDIhhHRwpgsP1dmeu3RUOyd5szSpuubl5WHkyJHcfVdXV4hEIty7d6/VgxFCCCGkiYW6uroaKioqEm2Kioqoqqpq1VCEEEIIeaFJXd+MMUydOlVipq/y8nLMmDFD4hKtplyeRQghhJD6NalQT5kyRapt8uTJrRaGEEIIIZKaVKg3bdrUVjkIIYQQUgcaqk0IIYQIGBVqQgghRMCoUBNCCCECRoWaEEIIETAq1IQQQoiAUaEmhBBCBIwKNSGEECJgVKgJIYQQAaNCTQghhAgYFWpCCCFEwKhQE0IIIQJGhZoQQggRMCrUhBBCiIBRoSaEEEIEjAo1IYQQImBUqAkhhBABo0JNCCGECJgC3wEIIZKstljV+9ifU/5sxySEECGgI2pCCCFEwKhQE0IIIQImiEIdHx8PU1NTqKiowMnJCefPn6932/Xr12Pw4MHo3LkzOnfuDFdX1wa3J4QQQmQZ7+eo9+zZg8DAQCQkJMDJyQlxcXFwc3NDZmYm9PX1pbZPTU2Fl5cXBgwYABUVFXz77bcYPnw4/vrrLxgbG/PwDgghhNSHxly0HO9H1LGxsfDz84OPjw8sLS2RkJAANTU1JCYm1rn9jh078OWXX8LW1hZ9+/bFhg0bIBaLkZKS0s7JCSGEkLbHa6GurKzEpUuX4OrqyrXJycnB1dUVZ8+ebdRzlJWVoaqqCjo6Om0VkxBCCOENr13fDx8+RE1NDQwMDCTaDQwMcPPmzUY9x4IFC2BkZCRR7F9WUVGBiooK7n5xcXHzAxNCCCHtjPeu75ZYunQpdu/ejZ9//hkqKip1bhMTEwMtLS3uZmJi0s4pCSGEkObjtVDr6upCXl4e9+/fl2i/f/8+DA0NG9z3u+++w9KlS3H06FFYW1vXu92iRYvw7Nkz7pafn98q2QkhhJD2wGuhVlJSgoODg8RAsNqBYc7OzvXut2zZMkRGRuLIkSNwdHRs8DWUlZWhqakpcSOEEEJkBe+XZwUGBmLKlClwdHRE//79ERcXh9LSUvj4+AAAvL29YWxsjJiYGADAt99+i9DQUOzcuROmpqYoLCwEAKirq0NdXZ2390EIIYS0Bd4L9YQJE1BUVITQ0FAUFhbC1tYWR44c4QaY5eXlQU7ufwf+a9euRWVlJcaNGyfxPGFhYViyZEl7RieEEELaHO+FGgD8/f3h7+9f52OpqakS93Nzc9s+ECGEECIQMj3qmxBCCOnoqFATQgghAkaFmhBCCBEwQZyjfhPRRPWEEEIag46oCSGEEAGjQk0IIYQIGBVqQgghRMCoUBNCCCECRoWaEEIIETAq1IQQQoiAUaEmhBBCBIwKNSGEECJgVKgJIYQQAaNCTQghhAgYFWpCCCFEwKhQE0IIIQJGi3IQQlqMFpkhHYnQvs90RE0IIYQIGBVqQgghRMCo65s0mtC6gwgh5E1AR9SEEEKIgFGhJoQQQgSMur5byHThoXofy106qh2TEEII6YjoiJoQQggRMCrUhBBCiIBR1zfp0GikOqmPLH43ZDEzaTk6oiaEEEIEjAo1IYQQImBUqAkhhBABE0Shjo+Ph6mpKVRUVODk5ITz5883uP2PP/6Ivn37QkVFBVZWVvjtt9/aKSkhhBDSvngv1Hv27EFgYCDCwsJw+fJl2NjYwM3NDQ8ePKhz+zNnzsDLywu+vr64cuUKxo4di7Fjx+L69evtnJwQQghpe7wX6tjYWPj5+cHHxweWlpZISEiAmpoaEhMT69x+1apVcHd3x7x582BhYYHIyEjY29vj+++/b+fkhBBCSNvj9fKsyspKXLp0CYsWLeLa5OTk4OrqirNnz9a5z9mzZxEYGCjR5ubmhv3797dlVEIIIfVZolX/Y2bd2y9HB8VroX748CFqampgYGAg0W5gYICbN2/WuU9hYWGd2xcWFta5fUVFBSoqKrj7z549AwAUFxe3JDpHXFFW72MNvUbN85pm7dca3gn7vd7Hroe71fsYn5mbi8/MDX43RKzex/j+nOv7ftB3g398Z67vO03f56arfR7G6v/sOIxHd+/eZQDYmTNnJNrnzZvH+vfvX+c+ioqKbOfOnRJt8fHxTF9fv87tw8LCGAC60Y1udKMb3QR3y8/Pf22t5PWIWldXF/Ly8rh//75E+/3792FoaFjnPoaGhk3aftGiRRJd5WKxGI8fP0aXLl0gEola+A4kFRcXw8TEBPn5+dDU1GzV524rlLl9UOb2QZnbB2VuOcYY/v33XxgZGb12W14LtZKSEhwcHJCSkoKxY8cCeFFIU1JS4O/vX+c+zs7OSElJwdy5c7m25ORkODs717m9srIylJWVJdq0tbVbI369NDU1BfFFaArK3D4oc/ugzO2DMreMlpZWo7bjfa7vwMBATJkyBY6Ojujfvz/i4uJQWloKHx8fAIC3tzeMjY0RExMDAJgzZw5cXFywYsUKjBo1Crt378bFixfxww8/8Pk2CCGEkDbBe6GeMGECioqKEBoaisLCQtja2uLIkSPcgLG8vDzIyf3vKrIBAwZg586dCA4OxuLFi9G7d2/s378f77zzDl9vgRBCCGkzvBdqAPD396+3qzs1NVWqbfz48Rg/fnwbp2o6ZWVlhIWFSXW1Cxllbh+UuX1Q5vZBmduXiLHGjA0nhBBCCB94n5mMEEIIIfWjQk0IIYQIGBVqQgghRMCoUBNCCCECRoW6maqrq7F161apWdIIIYSQ1kSjvltATU0NGRkZ6NGjB99RGm3KlCnw9fXFkCFD+I7SJD179sSFCxfQpUsXifanT5/C3t4eOTk5PCX7n19++aXR244ePboNk7zZampq8Oeff6JHjx7o3Lkz33FkVlMWnxDKTF+vSktLa/BxWfl3UBDXUcuq/v374+rVqzJVqJ89ewZXV1f06NEDPj4+mDJlCoyNjfmO9Vq5ubmoqZFe0aaiogJ3797lIZG02mlwa4lEIomVcV6eW76u9yIEW7Zsga6uLkaNGgUAmD9/Pn744QdYWlpi165dgvyuz507F1ZWVvD19UVNTQ1cXFxw5swZqKmp4eDBgxg6dCjfEWWStrZ2o9dDEOr3ua7/72Xhv8NXUaFugS+//BKBgYHIz8+Hg4MDOnXqJPG4tbU1T8nqt3//fhQVFWHbtm3YsmULwsLC4OrqCl9fX4wZMwaKiop8R5Tw8lHq77//LjE3bk1NDVJSUmBqaspDMmlisZj7+9ixY1iwYAGio6O5eejPnj2L4OBgREdH8xXxtaKjo7F27VoAL/LGx8dj5cqVOHjwIAICApCUlMRzQmn79u3D5MmTAQC//vorbt++jZs3b2Lbtm34+uuvcfr0aZ4T1m3fvn3Yu3cv8vLyUFlZKfHY5cuXeUr1P8ePH+f+zs3NxcKFCzF16lSJ7/OWLVu46Z2F6MmTJxL3q6qqcOXKFYSEhCAqKoqnVM3w2vW1SL1EIpHUTU5OjvtfWXDp0iXm7+/PVFRUmK6uLps7dy67desW37E4dX3GtTclJSXWp08f9uuvv/IdU8rbb7/NTp48KdWelpbG+vbty0OixlFVVWV37txhjDE2f/589tlnnzHGGLt+/TrT1dXlM1q9lJWVuaUC/fz82Jw5cxhjjOXk5DANDQ0ek9Vv1apVTF1dnfn7+zMlJSX2+eefM1dXV6alpcUWL17Mdzwp7733ntTywowxtmPHDubi4tL+gVooNTWV2dvb8x2j0WgwWQvcvn1b6paTk8P9r9AVFBQgOTkZycnJkJeXx8iRI/Hnn3/C0tISK1eu5DsegBdHqWKxGD169EBRURF3XywWo6KiApmZmfjwww/5jiklOzu7zlXatLS0kJub2+55GktdXR2PHj0CABw9ehQffPABAEBFRQXPnz/nM1q9DAwMcOPGDdTU1ODIkSNc5rKyMsjLy/Ocrm5r1qzBDz/8gP/85z9QUlLC/PnzkZycjNmzZ+PZs2d8x5Ny9uxZODo6SrU7Ojri/PnzPCRqGQMDA2RmZvIdo/H4/qVA2ldlZSXbt28fGzVqFFNUVGQODg5s7dq17NmzZ9w2SUlJTFtbm8eUkiorK9l7770nqCP91xk8eDD74IMPWGFhIddWWFjIhg8fzoYMGcJjsoZNnDiR2dvbM19fX6ampsYePnzIGGPswIED7O233+Y5Xd3CwsKYlpYW69u3L+vevTsrLy9njDG2ceNG9u677/Kcrm6qqqosNzeXMcaYnp4eu3r1KmOMsVu3bjEdHR0+o9WpT58+bN68eVLt8+bNY3369OEhUeOkp6dL3K5evcoOHz7MXFxc2MCBA/mO12h0jrqFtm3bhoSEBNy+fRtnz55Fjx49EBcXBzMzM4wZM4bveFK6du0KsVgMLy8vnD9/Hra2tlLbDBs2rM3X7G4KRUVFXLt2je8YTbJx40Z4enqie/fuMDExAQDk5+dzq70JVXx8PIKDg5Gfn4+ffvqJG2V/6dIleHl58ZyubkuWLME777yD/Px8jB8/nlt0QV5eHgsXLuQ5Xd0MDQ3x+PFj9OjRA927d8e5c+dgY2OD27dvSwxAFIqVK1fi448/xuHDh+Hk5AQAOH/+PP7++2/89NNPPKern62trdSgTgB49913kZiYyFOqpqPLs1pg7dq1CA0Nxdy5cxEVFYXr16+jZ8+e2Lx5M7Zs2SIxGEMotm3bhvHjx0NFRYXvKE0SEBAAZWVlLF26lO8ojcYYQ3JyMm7evAkAsLCwgKura6NH0pKmKy8vl4nv9vTp02FiYoKwsDDEx8dj3rx5GDhwIC5evAhPT09s3LiR74hS/vnnH6xduxYZGRkAXnyfZ8yYwf0QFaI7d+5I3JeTk4Oenp5MfEdeRoW6BSwtLREdHY2xY8dCQ0MD6enp6NmzJ65fv46hQ4fi4cOHfEeUUFVVBVVVVVy9elXm1u+eNWsWtm7dit69e9c5wj42NpanZNJk+XMGgJMnT2LdunXIycnBjz/+CGNjY2zbtg1mZmYYNGgQ3/Gk1NTUIDo6GgkJCbh//z5u3bqFnj17IiQkBKampvD19eU7opTacRYKCi86NXfv3o0zZ86gd+/e+Pzzz6GkpMRzwv+pqqqCu7s7EhIS0Lt3b77jvJFoMFkL3L59G3Z2dlLtysrKKC0t5SFRwxQVFdG9e3eZuXbwZdevX4e9vT00NDRw69YtXLlyhbtdvXqV73gSZPlz/umnn+Dm5gZVVVVcvnwZFRUVAF5cfy/Uy8qioqKwefNmLFu2TKLAvfPOO9iwYQOPyeonJyfHFWkA+PTTT7F69WrMmjVLUEUakM1TTy87ceIEPDw8YG5uDnNzc4wePRonT57kO1bT8Hh+XOZZWFiw/fv3M8YYU1dXZ9nZ2YwxxlavXs3s7Oz4jFavDRs2sJEjR7JHjx7xHaVDk9XP2dbWlm3ZsoUxJvmdvnz5MjMwMOAzWr169erFjh07xhiTzJyRkSGoQZEvMzMzY1OnTuUGvtUqKipiZmZmPKWq39y5c9mCBQv4jtFk27ZtYwoKCuyTTz5hq1atYqtWrWKffPIJU1RUZDt27OA7XqPRYLIWCAwMxMyZM1FeXg7GGM6fP49du3YhJiZGsL/kv//+e2RlZcHIyAg9evSQ6kIWwkQLr/PPP/8AALp168ZzkvrJ6uecmZlZ57SKWlpaePr0afsHaoS7d+/C3Nxcql0sFqOqqoqHRK+Xm5sLBQUFDB48GL/88gsMDQ0BvOjGf/W8qhBUV1cjMTERx44dE/ypp5dFRUVh2bJlCAgI4Npmz56N2NhYREZGYuLEiTymazwq1C0wffp0qKqqIjg4GGVlZZg4cSKMjIywatUqfPrpp3zHq9Or01zKCrFYjG+++QYrVqxASUkJAEBDQwNfffUVvv76a8jJCessjqx+zoaGhsjKypKa7e3UqVPo2bMnP6Few9LSEidPnpSa3nTfvn11npoSApFIhCNHjiAoKAgODg7Yv38/+vXrx3esetWeegKAW7duSTwm5MGROTk58PDwkGofPXo0Fi9ezEOiZuL7kL6jKC0tZffv3+c7Roe1cOFCpqenx9asWcNdExkfH8/09PQEOZOTrIqOjmaWlpbs3LlzTENDg508eZJt376d6enpsdWrV/Mdr0779+9nWlpabOnSpUxNTY0tX76cTZ8+nSkpKbGjR4/yHa9OIpGI+/di4cKFTFVVlW3bto0VFhbKzKyGsqBXr14sISFBqn3t2rXM3Nych0TNQ4W6BcrKylhpaSl3Pzc3l61cuZL9/vvvPKZ6vSdPnrD169ezhQsXcudQL126xP755x+ek9Wva9eu7MCBA1Lt+/fvZ0ZGRjwk6pjEYjH75ptvWKdOnbipWlVUVFhwcDDf0RqUlpbGXF1dmZ6eHlNVVWUDBw4U9H+HcnJyEj/st23bxlRUVJiPjw8V6la0Zs0apqSkxGbMmMG2bt3Ktm7dyj7//HOmrKxcZwEXKro8qwWGDx8OT09PzJgxA0+fPsVbb70FJSUlPHz4ELGxsfjiiy/4jijl2rVrcHV15aayzMzMRM+ePREcHIy8vDxs3bqV74h1UlFRwbVr19CnTx+J9szMTNja2gpuesuamhqsXLmy3kUXHj9+zFOyxqmsrERWVhZKSkpgaWkJdXV1viN1KHJycigsLIS+vj7XdvbsWXz00UcoKioS5BUDFy9erPf7LMTFWmr9/PPPWLFihcT13/PmzRPkhFT14vuXgizr0qULu379OmOMsfXr1zNra2tWU1PD9u7dK9iFF95//31uKsCXR8iePn2a9ejRg8dkDevfvz+bNWuWVLu/vz9zcnLiIVHDQkJCWNeuXdl3333HVFRUWGRkJPP19WVdunRhq1at4jteh+Lr68uOHz/Od4xWUVhYyFJTU/mOIWXXrl1MUVGRffjhh0xJSYl9+OGHrE+fPkxLS4tNnTqV73j18vb2ZidOnOA7RotRoW6Bl1caGj9+PFuyZAljjLG8vDymqqrKZ7R6aWpqsqysLMaYZKHOzc1lysrKfEZrUGpqKuvUqROzsLBg06ZNY9OmTWMWFhZMXV2dpaWl8R1PSs+ePdnBgwcZYy8+59rPfNWqVczLy4vPaA0qKSlhwcHBzNnZmfXq1YuZmZlJ3IRo9OjRTFlZmXXr1o0FBQWxK1eu8B3ptcLDw1lKSopUe0lJCQsPD+chUcOsrKzY999/zxj7378bYrGY+fn5sdDQUJ7T1W/MmDFMUVGRmZubs6ioKHb37l2+IzULFeoWsLKyYqtWrWJ5eXlMU1OTnTlzhjHG2MWLFwV7zamenh67fPkyY0yyUB89epR169aNz2ivdffuXbZ48WLm6enJPD092ddffy3Y//DU1NS4H3GGhobs0qVLjDHGsrOzmaamJp/RGvTpp5+yrl27svnz57OVK1eyuLg4iZtQPX78mK1bt465uLgwOTk5ZmlpyaKiotjt27f5jlan2mVaV6xYIdEu1MFkampq3Gepo6PDrl27xhhj7MaNG8zQ0JDHZK/34MEDtmLFCmZtbc0UFBSYu7s727t3L6usrOQ7WqNRoW6BH3/8kSkqKjI5OTnm6urKtUdHRzN3d3cek9XP19eXjR07llVWVjJ1dXWWk5PD7ty5w+zs7Lh1fIXio48+4lb12rJli9TkEELWp08fdu7cOcYYYwMHDmQxMTGMMcZ2797N9PT0+IzWIC0tLXbq1Cm+Y7RIfn4+W7ZsGevbty+Tl5fnO06dRCIR2717N+vSpQubOnUqq6ioYIwJt1AbGxtzxdnKyopbm/rMmTOC/uH5qkuXLjF/f3+moqLCdHV12dy5c2ViVT4q1C1UUFDALl++zGpqari2P/74g2VkZPCYqn5Pnz5lrq6uTFtbm8nLyzMTExOmqKjIhgwZwkpKSviOJ0FRUZHdu3ePMSY9SlboFixYwKKiohhjL4qzgoICMzc3Z0pKSoKe4cnU1JTduHGD7xjNVllZyX7++Wf28ccfMxUVFcFeEVB7eVZWVhazsLBgzs7O7P79+4It1F5eXtzRf0REBNPT02PTp09nPXr0YB999BHP6Rrn3r17bOnSpeytt95inTp1Yt7e3uz9999nCgoKLDY2lu94DaJR361EFmbLetmpU6dw7do1lJSUwN7eHq6urnxHkmJtbQ17e3sMGzYMPj4+WL16NTQ1Nevc1tvbu53TNc25c+e4RRfqmoBBKLZv344DBw5gy5YtUFNT4ztOox0/fhw7d+7ETz/9BLFYDE9PT0yaNAnvvfeeICfkkJeXR0FBAfT19VFcXIxPPvkEf/31FxISEjB69GjBjfp+/PgxysvLYWRkBLFYjGXLlnHf5+DgYHTu3JnviHWqqqrCL7/8gk2bNuHo0aOwtrbG9OnTMXHiRO7fkp9//hnTpk3DkydPeE5bPyrULSBrs2UBL9ZEFvKydC87ffo0vvrqK2RnZ+Px48fQ0NCo8x9dkUgk+MudhMzOzk7ic83KygJjDKamplBUVJTYVohTnxobG+Px48dwd3fHpEmT4OHhwa1JLVSvXp4lFosxd+5crF27FmKxWHCFWlbp6upCLBbDy8sLfn5+sLW1ldrm6dOnsLOzw+3bt9s/YCPRFKIt8PXXX2Pjxo1YunQpBg4cCODFkeqSJUtQXl6OqKgonhNKMzU1xaBBgzB58mSMGzdOsL+EAWDgwIE4d+4cgBf/sN26dUviulMh6969O4YOHQoXFxcMHToUvXr14jtSvWR1utNaS5Yswfjx46Gtrc13lEbbtGkTtLS0uPtycnJYvXo17OzskJaWxmOyunl7e2PYsGEYMmSIoL/Lr1q5ciXGjx/f4PrT2tragi7SAB1Rt4iRkRHXVfWyAwcO4Msvv8Tdu3d5Sla/K1euYOfOndi9ezeKiorg7u6OyZMnC/IoxNPTE5s3b4ampia2bNmCTz75BKqqqnzHapTt27cjLS0NqampyMrKgrGxMVxcXLjCTev6tg1ZOwUlK6ZPn460tDSJ73LtD1H6Lrc9KtQtIGuzZb2MMYbU1FSp83qJiYl8R+MoKSnhzp076Nq1q8Q5PVlTUFCAEydO4ODBg9izZ4+guzYvXLgAsVgMJycnifY//vgD8vLycHR05ClZ/WTlFNTq1avxf//3f1BRUcHq1avr3U4kEmHWrFntmKzx7t69i7S0NJw4cQInTpzArVu30LVrV+4HEmkbVKhbwMnJCU5OTlL/0c2aNQsXLlzgum2F7vLly/D19cW1a9cEVUBkfTBZWVkZTp06hdTUVBw/fhxXrlyBhYUFhg4dipUrV/Idr079+/fH/PnzMW7cOIn2pKQkfPvtt/jjjz94Sla/RYsWYePGjQgPD5c6BeXn5yeYU1BmZma4ePEiunTpAjMzs3q3E4lEyMnJacdkjVf7nT5+/DhSU1Nx+fJlWFpa4sqVK3xH69CoULfAiRMnMGrUKHTv3h3Ozs4AXszXm5+fj99++w2DBw/mOWH9/vnnH+zcuRM7d+7E9evX4ezsjEmTJmHGjBl8R+OcOXMGgYGBMjmYbMCAARKF2cXFBUOGDBH0mAAAUFdXx7Vr16SWtLx9+zasra3x77//8pSsfrJ4Cupltf8EC3F0eq3FixcjNTWV+07Xdn3Lwne6I6BC3UL37t1DfHw8bt68CeDFhO9ffvkljIyMeE5Wt3Xr1mHnzp04deoULCwsMGnSJEycOFFqLV+hqWsRAyHT0dGBnJwchg8fjqFDh2Lo0KFSp0iEqEuXLjh48CD3w7PWmTNnMGrUKEFewiKrp6A2btyIlStX4u+//wYA9O7dG3PnzsX06dN5TiZNTk4Oenp6CAgIgKenp0x8lzsSKtRvGBMTE3h5eWHSpEmwsbHhO06j3blzB3l5eVi3bh1ycnLw448/wtjYGNu2bYOZmRkGDRrEd0QJjDH8+eefSE1NxYkTJ5CWlgYlJSW4uLhg2LBh8PPz4ztinby8vFBQUIADBw5wo5KfPn2KsWPHQl9fH3v37uU5oTRZPAUVGhqK2NhYzJo1S6I37vvvv0dAQAAiIiJ4TigpPT0dJ06cQGpqKk6ePMl9l2XpR6gso0LdRNeuXWv0ttbW1m2YpHkYYzh16pTMFLxaP/30Ez777DNMmjQJ27Ztw40bN9CzZ098//33+O233/Dbb7/xHbFejDFcunQJ33//PXbs2CHowWR3797FkCFD8OjRI9jZ2QEArl69CgMDAyQnJwvyGvz6TkHl5eXh8OHDgjwFpaenh9WrV8PLy0uifdeuXZg1axYePnzIU7LGSU9Px8qVKwX/fe4o6DrqJrK1tYVIJMLrft+IRCJBfnmTkpK4gnf58mVUVFQAAJ49e4bo6GjBFrxvvvkGCQkJ8Pb2xu7du7n2gQMH4ptvvuExWd0uX76M1NRUpKam4tSpU/j3339hZWWFWbNmwcXFhe949TI2Nsa1a9ewY8cOpKenQ1VVFT4+PvDy8pKa/EQoXFxckJmZibVr13JrDnt6egr6FFRVVVWdI+gdHBxQXV3NQ6KGMcZw5coVie90cXExrK2tBf197ijoiLqJ7ty50+hthXje187ODgEBAfD29oaGhgbS09PRs2dPXLlyBSNGjEBhYSHfEeukpqaGGzduwNTUVCJ3Tk4OLC0tUV5ezndECQoKCrCzs+OunR4yZIjEBBekdZWXl+PatWt48OABxGKxxGOvDjITglmzZkFRURGxsbES7UFBQXj+/Dni4+N5Sla3zp07o6SkBDY2NlyX9+DBg2VqkhlZRkfUTfRy8Y2JiYGBgQGmTZsmsU1iYiKKioqwYMGC9o73WpmZmRgyZIhUu5aWFp4+fdr+gRrJ0NAQWVlZMDU1lWg/deqU1AhlvtXU1CApKQmDBw+WyRGxf//9N44fP15n0QsNDeUpVf2OHDkCb29vPHr0SKqnS6g9W8CLwWRHjx7Fu+++C+DFtep5eXnw9vZGYGAgt92rxZwP27dvx+DBg+u9PJK0LSrULVA7gvpVb7/9Nj799FNBFmpZKngv8/Pzw5w5c5CYmAiRSIR79+7h7NmzCAoKQkhICN/xJMjLy+OTTz5BRkaGzBXq9evX44svvoCuri4MDQ0lLhkSiUSCLNSzZs3C+PHjERoaCgMDA77jNMr169dhb28PAMjOzgbwYl5qXV1dXL9+ndtOKJdsjRo1ivubZn/jQbus0dVBKSsrs5ycHKn27OxspqyszEOi14uOjmaWlpbs3LlzTENDg508eZJt376d6enpsdWrV/Mdr15isZh98803rFOnTkwkEjGRSMRUVFRYcHAw39Hq5ODgwI4dO8Z3jCbr3r07W7p0Kd8xmkRDQ4NlZWXxHaNDq6mpYeHh4UxTU5PJyckxOTk5pqWlxSIiIiSW+CVtgwp1C5ibm7Nt27ZJtW/dupWZmZnxkOj1ZK3gvaqiooL99ddf7I8//mD//vsv33HqdfjwYWZra8t+/fVXdu/ePfbs2TOJm1BpaGiw7OxsvmM0iY+PD9uwYQPfMTq0hQsXMj09PbZmzRqWnp7O0tPTWXx8PNPT02OLFy/mO16HR4PJWmDZsmVYtmwZli9fjvfeew8AkJKSgvnz5+Orr77CokWLeE5Yv8rKSmRlZaGkpASWlpZQV1fnO1KH8vL80i93XzLGBH3e1NfXF/369RPUDHWvU1ZWhvHjx0NPTw9WVlZSo9Nnz57NU7KOQ9Znf5N1dI66BebNm4dHjx7hyy+/RGVlJYAXsyQtWLBA0EUaeLHghaWlJd8xOqzjx4/zHaFZzM3NERISgnPnzslM0du1axeOHj0KFRUVpKamSp1XF2JmWfP48WP07dtXqr1v376Cm763I6Ij6lZQUlKCjIwMqKqqonfv3oJbLpKQxpLFxSIMDQ0xe/ZsLFy4UDArZXU0sjj7W0dChZqQNvL06VNs3LiRm4Tj7bffxrRp0+h66lamo6ODCxcuoFevXnxH6bBkeQGijoAKNSFt4OLFi3Bzc4Oqqir69+8P4MVaz8+fP8fRo0e5S3OEIDAwEJGRkejUqZPE9buvEolEWLFiRTsma5yAgADo6elh8eLFfEfpsPLy8qCgoFDnAkTV1dXo3r07zwk7NirUhLSBwYMHw9zcHOvXr4eCwouhINXV1Zg+fTpycnKQlpbGc8L/GTZsGH7++Wdoa2tj2LBh9W4nEonw3//+tx2TNc7s2bOxdetW2NjYwNraWuq8uhAmDJF18vLyKCgokFq97tGjR9DX1xfs4MiOggo1IW1AVVUVV65ckRqAc+PGDTg6OqKsrIynZB2PLP64kDX1LTN7584dWFpaorS0lKdkbwYa9U1IG9DU1EReXp5Uoc7Pz4eGhgZPqTomWR1hLwtqT4XUzkqnpqbGPVZTU4M//vgDtra2PKV7c1ChJqQNTJgwAb6+vvjuu+8wYMAAAMDp06cxb948qaUNCRGqK1euAPjf+upKSkrcY0pKSrCxsUFQUBBf8d4Y1PVNSCu5du0a3nnnHcjJyaGyshLz5s1DQkICt2yhoqIivvjiCyxdupQu4SMyxcfHB6tWraJFOXhChZqQVvLygJuePXviwoULUFVV5RZd6NWrl0TXISGENAZ1fRPSSrS1tXH79m3o6+sjNzcXYrEYampqsLKy4jsaIUSGUaEmpJV8/PHHcHFxQdeuXSESieDo6Ah5efk6txXiDF+EEGGiQk1IK/nhhx/g6emJrKwszJ49G35+fjTCmxDSYnSOmpA24OPjg9WrV1OhJoS0GBVqQgghRMBoqRlCCCFEwKhQE0IIIQJGhZoQQggRMCrUhBBCiIBRoSaEEEIEjAo1IYQQImBUqAkhhBABo0JNCCGECNj/AziNpZr5Sbj4AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Applying very small temperatures, such as 0.1, will\n",
        "result in sharper distributions such that the behavior of the multinomial function selects\n",
        "the most likely token (here: \"forward\") almost 100% of the time, approaching the\n",
        "behavior of the argmax function.\n",
        "\n",
        "Vice versa, a temperature of 5 results in a more uniform\n",
        "distribution where other tokens are selected more often.\n",
        "\n",
        "This can add more variety to the\n",
        "generated texts but also more often results in nonsensical text.\n",
        "\n",
        "For example, using the\n",
        "temperature of 5 results in texts such as \"every effort moves you pizza\" about 4% of\n",
        "the time.\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "7lV7lmrTJ6lS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Left top-k and tp p sampling and fine tunning etc"
      ],
      "metadata": {
        "id": "0-nIrfNAJ9DA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eR93uT5BJ1cd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}